#+TITLE: Practical aspects of Deep Learning

* Table of Contents :TOC_3_gh:
- [[#optimization-alogrithms][Optimization alogrithms]]
  - [[#mini-batch-gradient-descent][Mini-batch gradient descent]]
  - [[#understanding-mini-batch-gradient-descent][Understanding mini-batch gradient descent]]
  - [[#exponentially-weighted-averages][Exponentially weighted averages]]
  - [[#understanding-exponentially-weighted-averages][Understanding exponentially weighted averages]]
  - [[#bias-correction-in-exponentially-weighted-averages][Bias correction in exponentially weighted averages]]
  - [[#gradient-descent-with-momentum][Gradient descent with momentum]]
  - [[#rmsprop][RMSprop]]
  - [[#adam-optimization-algorithm][Adam optimization algorithm]]
  - [[#learning-rate-decay][Learning rate decay]]
  - [[#the-problem-of-local-optima][The problem of local optima]]
- [[#programming-assignment][Programming assignment]]
  - [[#optimization][Optimization]]

* Optimization alogrithms
** Mini-batch gradient descent
[[file:img/screenshot_2017-10-18_07-50-21.png]]

[[file:img/screenshot_2017-10-18_07-56-43.png]]

** Understanding mini-batch gradient descent
[[file:img/screenshot_2017-10-18_08-00-20.png]]

[[file:img/screenshot_2017-10-18_08-23-00.png]]

[[file:img/screenshot_2017-10-18_08-25-46.png]]
** Exponentially weighted averages
[[file:img/screenshot_2017-10-21_17-25-56.png]]

[[file:img/screenshot_2017-10-21_17-25-29.png]]

- ~0.9~  :: red
- ~0.98~ :: green, morer smooth, shifted right because it now averages over a larger window.
- ~0.5~  :: yellow

** Understanding exponentially weighted averages
[[file:img/screenshot_2017-10-21_17-33-22.png]]

[[file:img/screenshot_2017-10-21_17-35-50.png]]

** Bias correction in exponentially weighted averages
[[file:img/screenshot_2017-10-21_17-40-11.png]]

- During initial phases, the values are very smaller than the actual values.(the coefficients of ~0.0196~ and ~0.02~ are both small)
- With ~Vt~ / ~(1 - B^t)~, only initial phases are affected, because as ~t~ becomes large, ~B^t~ goes ~0~.

** Gradient descent with momentum
[[file:img/screenshot_2017-10-21_17-52-09.png]]

[[file:img/screenshot_2017-10-21_17-55-06.png]]

- Some practitioners omit ~(1 - B)~.
  But it would change the sementic scale of ~Vdw~, ~Vdb~, which make the learning rate ~a~ have different meaning.
  So he recommends to keep ~(1 - B)~.
** RMSprop
[[file:img/screenshot_2017-10-22_00-27-14.png]]

** Adam optimization algorithm
[[file:img/screenshot_2017-10-22_00-27-46.png]]

[[file:img/screenshot_2017-10-22_00-28-03.png]]

** Learning rate decay
[[file:img/screenshot_2017-10-22_00-28-33.png]]

[[file:img/screenshot_2017-10-22_00-28-51.png]]

[[file:img/screenshot_2017-10-22_00-29-11.png]]

#+BEGIN_QUOTE
For me, I would say that learning rate decay is usually lower down on the list of things I try.
Setting alpha, just a fixed value of alpha, and getting that to be well tuned, has a huge impact.
Learning rate decay does help.
Sometimes it can really help speed up training, but it is a little bit lower down my list in terms of the things I would try.
#+END_QUOTE

** The problem of local optima
[[file:img/screenshot_2017-10-22_00-30-00.png]]

[[file:img/screenshot_2017-10-22_00-29-43.png]]

Local optima is unlikely to happen because there are lots of dimensions in most of practical problems.
The probability of the actual local optima for all the related dimensions are extremly low.
* Programming assignment
** Optimization
[[file:img/screenshot_2017-10-22_16-01-57.png]]

[[file:img/screenshot_2017-10-22_16-02-36.png]]

[[file:img/screenshot_2017-10-22_16-03-02.png]]

[[file:img/screenshot_2017-10-22_16-03-25.png]]

[[file:img/screenshot_2017-10-22_16-04-24.png]]

[[file:img/screenshot_2017-10-22_16-05-48.png]]

#+BEGIN_SRC python
  # Step 1: Shuffle (X, Y)
  permutation = list(np.random.permutation(m))
  shuffled_X = X[:, permutation]
  shuffled_Y = Y[:, permutation].reshape((1,m))
#+END_SRC

[[file:img/screenshot_2017-10-22_16-20-37.png]]

[[file:img/screenshot_2017-10-22_16-24-34.png]]

[[file:img/screenshot_2017-10-22_16-29-29.png]]

[[file:img/screenshot_2017-10-22_16-30-46.png]]

- Mini-batch gradient descent with momentum
[[file:img/screenshot_2017-10-22_16-47-14.png]]

- Mini-batch with Adam mode
[[file:img/screenshot_2017-10-22_16-48-42.png]]

[[file:img/screenshot_2017-10-22_16-49-21.png]]

- [[https://arxiv.org/pdf/1412.6980.pdf][Adam paper]]
