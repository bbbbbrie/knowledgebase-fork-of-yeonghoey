#+TITLE: Natural Language Processing & Word Embeddings

* Table of Contents :TOC_3_gh:
- [[#introduction-to-word-embeddings][Introduction to Word Embeddings]]
  - [[#word-representation][Word Representation]]
  - [[#using-word-embeddings][Using word embeddings]]
  - [[#properties-of-word-embeddings][Properties of word embeddings]]
  - [[#embedding-matrix][Embedding matrix]]
- [[#learning-word-embeddings-word2vec--glove][Learning Word Embeddings: Word2vec & GloVe]]
  - [[#learning-word-embeddings][Learning word embeddings]]
  - [[#word2vec][Word2Vec]]
  - [[#negative-sampling][Negative Sampling]]
  - [[#glove-word-vectors][GloVe word vectors]]
- [[#applications-using-word-embeddings][Applications using Word Embeddings]]
  - [[#sentiment-classification][Sentiment Classification]]
  - [[#debiasing-word-embeddings][Debiasing word embeddings]]
- [[#practice-questions][Practice questions]]
  - [[#quiz-natural-language-porcessing--word-embeddings][Quiz: Natural Language Porcessing & Word Embeddings]]
- [[#programming-assignments][Programming assignments]]
  - [[#operations-on-word-vectors---debiasing][Operations on word vectors - Debiasing]]
  - [[#emojify][Emojify]]

* Introduction to Word Embeddings
** Word Representation
[[file:_img/screenshot_2018-02-11_22-32-17.png]]

[[file:_img/screenshot_2018-02-11_22-37-17.png]]

[[file:_img/screenshot_2018-02-11_22-40-11.png]]

** Using word embeddings
[[file:_img/screenshot_2018-02-11_22-44-37.png]]

[[file:_img/screenshot_2018-02-11_22-49-04.png]]

[[file:_img/screenshot_2018-02-11_22-51-45.png]]
- The word embeddings of nlp funciton in a similar way as face encoding of face recognition.
- While word embeddings have a fixed number of inputs(vocabulary),
  face encoding differs in that it can have an infinite number of inputs(face images).

** Properties of word embeddings
[[file:_img/screenshot_2018-02-11_23-11-50.png]]

[[file:_img/screenshot_2018-02-11_23-16-24.png]]

[[file:_img/screenshot_2018-02-11_23-19-10.png]]

** Embedding matrix
[[file:_img/screenshot_2018-02-11_23-25-37.png]]

* Learning Word Embeddings: Word2vec & GloVe
** Learning word embeddings
[[file:_img/screenshot_2018-02-12_10-33-46.png]]

[[file:_img/screenshot_2018-02-12_10-37-12.png]]

** Word2Vec
[[file:_img/screenshot_2018-02-12_10-40-40.png]]

[[file:_img/screenshot_2018-02-12_10-45-31.png]]

- ~theta t~ is the parameter associated with the output ~t~.

[[file:_img/screenshot_2018-02-12_10-50-30.png]]

- If you sample the context uniformly at radom, words like ~the~, ~of~, ~a~ , ~and~ will dominate the context.
- So there is some other heuristic method to sample the context.

** Negative Sampling
[[file:_img/screenshot_2018-02-12_11-06-52.png]]

[[file:_img/screenshot_2018-02-12_11-13-43.png]]

- Each target word(10k) has their binary classification model.
- Each model takes word embeddings as inputs, in previous examples, 300 features.
- Each model predicts probability of the word in charge.

[[file:_img/screenshot_2018-02-12_15-38-08.png]]

- ~f(wi)^3/4~ is heuristic value, which is in between ~frequency distribution~ and ~uniform distribution~.

** GloVe word vectors
[[file:_img/screenshot_2018-02-12_15-44-28.png]]

[[file:_img/screenshot_2018-02-12_15-53-17.png]]

[[file:_img/screenshot_2018-02-12_15-56-58.png]]

- Each feature can't be completely orgthogonal.
- But it still makes sense when it comes to word similarity.

* Applications using Word Embeddings
** Sentiment Classification
[[file:_img/screenshot_2018-02-12_16-04-45.png]]

[[file:_img/screenshot_2018-02-12_16-08-03.png]]

[[file:_img/screenshot_2018-02-12_16-10-09.png]]

** Debiasing word embeddings
- the term ~bias~ in this lecture is NOT the ~bias~ as a parameter, but the bias in learned feature.

[[file:_img/screenshot_2018-02-12_16-16-08.png]]

[[file:_img/screenshot_2018-02-12_16-23-06.png]]

* Practice questions
** Quiz: Natural Language Porcessing & Word Embeddings

[[file:_img/screenshot_2018-02-12_16-26-15.png]]

[[file:_img/screenshot_2018-02-12_16-42-34.png]]

[[file:_img/screenshot_2018-02-12_16-43-09.png]]

* Programming assignments
** Operations on word vectors - Debiasing
[[file:_img/screenshot_2018-02-12_23-23-31.png]]

[[file:_img/screenshot_2018-02-12_23-24-13.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: cosine_similarity

  def cosine_similarity(u, v):
      """
      Cosine similarity reflects the degree of similariy between u and v

      Arguments:
          u -- a word vector of shape (n,)
          v -- a word vector of shape (n,)

      Returns:
          cosine_similarity -- the cosine similarity between u and v defined by the formula above.
      """

      distance = 0.0

      ### START CODE HERE ###
      # Compute the dot product between u and v (≈1 line)
      dot = None
      # Compute the L2 norm of u (≈1 line)
      norm_u = None

      # Compute the L2 norm of v (≈1 line)
      norm_v = None
      # Compute the cosine similarity defined by formula (1) (≈1 line)
      cosine_similarity = None
      ### END CODE HERE ###

      return cosine_similarity
#+END_SRC

[[file:_img/screenshot_2018-02-12_23-28-58.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: complete_analogy

  def complete_analogy(word_a, word_b, word_c, word_to_vec_map):
      """
      Performs the word analogy task as explained above: a is to b as c is to ____.

      Arguments:
      word_a -- a word, string
      word_b -- a word, string
      word_c -- a word, string
      word_to_vec_map -- dictionary that maps words to their corresponding vectors.

      Returns:
      best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity
      """

      # convert words to lower case
      word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()

      ### START CODE HERE ###
      # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)
      e_a, e_b, e_c = None
      ### END CODE HERE ###

      words = word_to_vec_map.keys()
      max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number
      best_word = None                   # Initialize best_word with None, it will help keep track of the word to output

      # loop over the whole word vector set
      for w in words:
          # to avoid best_word being one of the input words, pass on them.
          if w in [word_a, word_b, word_c] :
              continue

          ### START CODE HERE ###
          # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)
          cosine_sim = None

          # If the cosine_sim is more than the max_cosine_sim seen so far,
              # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)
          if None > None:
              max_cosine_sim = None
              best_word = None
          ### END CODE HERE ###

      return best_word
#+END_SRC

[[file:_img/screenshot_2018-02-12_23-33-08.png]]

[[file:_img/screenshot_2018-02-13_00-37-02.png]]

[[file:_img/screenshot_2018-02-13_00-39-35.png]]

[[file:_img/screenshot_2018-02-13_00-39-53.png]]

#+BEGIN_SRC python
  def neutralize(word, g, word_to_vec_map):
      """
      Removes the bias of "word" by projecting it on the space orthogonal to the bias axis.
      This function ensures that gender neutral words are zero in the gender subspace.

      Arguments:
          word -- string indicating the word to debias
          g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)
          word_to_vec_map -- dictionary mapping words to their corresponding vectors.

      Returns:
          e_debiased -- neutralized word vector representation of the input "word"
      """

      ### START CODE HERE ###
      # Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)
      e = None

      # Compute e_biascomponent using the formula give above. (≈ 1 line)
      e_biascomponent = None

      # Neutralize e by substracting e_biascomponent from it
      # e_debiased should be equal to its orthogonal projection. (≈ 1 line)
      e_debiased = None
      ### END CODE HERE ###

      return e_debiased
#+END_SRC

- ~g~ part of ~e^bias_component~ is L2 norm squared.

[[file:_img/screenshot_2018-02-13_00-50-41.png]]

- There is a typo in the denominators of equation (9), (10). Fix it as follows:
[[file:_img/screenshot_2018-02-13_01-15-48.png]]


#+BEGIN_SRC python
  def equalize(pair, bias_axis, word_to_vec_map):
      """
      Debias gender specific words by following the equalize method described in the figure above.

      Arguments:
      pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor")
      bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender
      word_to_vec_map -- dictionary mapping words to their corresponding vectors

      Returns
      e_1 -- word vector corresponding to the first word
      e_2 -- word vector corresponding to the second word
      """

      ### START CODE HERE ###
      # Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)
      w1, w2 = None
      e_w1, e_w2 = None

      # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)
      mu = None

      # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)
      mu_B = None
      mu_orth = None

      # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)
      e_w1B = None
      e_w2B = None

      # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)
      corrected_e_w1B = None
      corrected_e_w2B = None

      # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)
      e1 = None
      e2 = None

      ### END CODE HERE ###

      return e1, e2
#+END_SRC

[[file:_img/screenshot_2018-02-13_01-09-43.png]]

** Emojify
[[file:img/screenshot_2018-02-12_23-36-06.png]]

[[file:img/screenshot_2018-02-12_23-39-59.png]]

[[file:img/screenshot_2018-02-12_23-40-44.png]]

[[file:img/screenshot_2018-02-12_23-43-12.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: sentence_to_avg

  def sentence_to_avg(sentence, word_to_vec_map):
      """
      Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word
      and averages its value into a single vector encoding the meaning of the sentence.

      Arguments:
      sentence -- string, one training example from X
      word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation

      Returns:
      avg -- average vector encoding information about the sentence, numpy-array of shape (50,)
      """

      ### START CODE HERE ###
      # Step 1: Split sentence into list of lower case words (≈ 1 line)
      words = None

      # Initialize the average word vector, should have the same shape as your word vectors.
      avg = None

      # Step 2: average the word vectors. You can loop over the words in the list "words".
      for w in None:
          avg += None
      avg = None

      ### END CODE HERE ###

      return avg
#+END_SRC

[[file:img/screenshot_2018-02-12_23-49-55.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: model

  def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):
      """
      Model to train word vector representations in numpy.

      Arguments:
      X -- input data, numpy array of sentences as strings, of shape (m, 1)
      Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)
      word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
      learning_rate -- learning_rate for the stochastic gradient descent algorithm
      num_iterations -- number of iterations

      Returns:
      pred -- vector of predictions, numpy-array of shape (m, 1)
      W -- weight matrix of the softmax layer, of shape (n_y, n_h)
      b -- bias of the softmax layer, of shape (n_y,)
      """

      np.random.seed(1)

      # Define number of training examples
      m = Y.shape[0]                          # number of training examples
      n_y = 5                                 # number of classes
      n_h = 50                                # dimensions of the GloVe vectors

      # Initialize parameters using Xavier initialization
      W = np.random.randn(n_y, n_h) / np.sqrt(n_h)
      b = np.zeros((n_y,))

      # Convert Y to Y_onehot with n_y classes
      Y_oh = convert_to_one_hot(Y, C = n_y)

      # Optimization loop
      for t in range(num_iterations):                       # Loop over the number of iterations
          for i in range(m):                                # Loop over the training examples

              ### START CODE HERE ### (≈ 4 lines of code)
              # Average the word vectors of the words from the i'th training example
              avg = None

              # Forward propagate the avg through the softmax layer
              z = None
              a = None

              # Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)
              cost = None
              ### END CODE HERE ###

              # Compute gradients
              dz = a - Y_oh[i]
              dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))
              db = dz

              # Update parameters with Stochastic Gradient Descent
              W = W - learning_rate * dW
              b = b - learning_rate * db

          if t % 100 == 0:
              print("Epoch: " + str(t) + " --- cost = " + str(cost))
              pred = predict(X, Y, W, b, word_to_vec_map)

      return pred, W, b
#+END_SRC

[[file:img/screenshot_2018-02-12_23-59-58.png]]

[[file:img/screenshot_2018-02-13_00-00-13.png]]

[[file:img/screenshot_2018-02-13_00-00-42.png]]

[[file:img/screenshot_2018-02-13_00-01-02.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: sentences_to_indices

  def sentences_to_indices(X, word_to_index, max_len):
      """
      Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.
      The output shape should be such that it can be given to `Embedding()` (described in Figure 4).

      Arguments:
      X -- array of sentences (strings), of shape (m, 1)
      word_to_index -- a dictionary containing the each word mapped to its index
      max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this.

      Returns:
      X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)
      """

      m = X.shape[0]                                   # number of training examples

      ### START CODE HERE ###
      # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)
      X_indices = None

      for i in range(m):                               # loop over training examples

          # Convert the ith training sentence in lower case and split is into words. You should get a list of words.
          sentence_words =None

          # Initialize j to 0
          j = None

          # Loop over the words of sentence_words
          for w in None:
              # Set the (i,j)th entry of X_indices to the index of the correct word.
              X_indices[i, j] = None
              # Increment j to j + 1
              j = None

      ### END CODE HERE ###

      return X_indices
#+END_SRC

[[file:img/screenshot_2018-02-13_00-07-02.png]]

- https://keras.io/layers/embeddings/

#+BEGIN_SRC python
  # GRADED FUNCTION: pretrained_embedding_layer

  def pretrained_embedding_layer(word_to_vec_map, word_to_index):
      """
      Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.

      Arguments:
      word_to_vec_map -- dictionary mapping words to their GloVe vector representation.
      word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

      Returns:
      embedding_layer -- pretrained layer Keras instance
      """

      vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)
      emb_dim = word_to_vec_map["cucumber"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)

      ### START CODE HERE ###
      # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
      emb_matrix = None

      # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary
      for word, index in word_to_index.items():
          emb_matrix[index, :] = None

      # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.
      embedding_layer = None
      ### END CODE HERE ###

      # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".
      embedding_layer.build((None,))

      # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
      embedding_layer.set_weights([emb_matrix])

      return embedding_layer
#+END_SRC

[[file:img/screenshot_2018-02-13_00-10-26.png]]

- https://keras.io/layers/core/#input
- https://keras.io/layers/core/#dropout
- https://keras.io/layers/core/#dense
- https://keras.io/activations/
- https://keras.io/layers/recurrent/#lstm

#+BEGIN_SRC python
  # GRADED FUNCTION: Emojify_V2

  def Emojify_V2(input_shape, word_to_vec_map, word_to_index):
      """
      Function creating the Emojify-v2 model's graph.

      Arguments:
      input_shape -- shape of the input, usually (max_len,)
      word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
      word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

      Returns:
      model -- a model instance in Keras
      """

      ### START CODE HERE ###
      # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
      sentence_indices = None

      # Create the embedding layer pretrained with GloVe Vectors (≈1 line)
      embedding_layer = None

      # Propagate sentence_indices through your embedding layer, you get back the embeddings
      embeddings = None

      # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
      # Be careful, the returned output should be a batch of sequences.
      X = None
      # Add dropout with a probability of 0.5
      X = None
      # Propagate X trough another LSTM layer with 128-dimensional hidden state
      # Be careful, the returned output should be a single hidden state, not a batch of sequences.
      X = None
      # Add dropout with a probability of 0.5
      X = None
      # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
      X = None
      # Add a softmax activation
      X = None

      # Create Model instance which converts sentence_indices into X.
      model = None

      ### END CODE HERE ###

      return model
#+END_SRC

[[file:img/screenshot_2018-02-13_00-27-23.png]]

[[file:img/screenshot_2018-02-13_00-29-34.png]]

[[file:img/screenshot_2018-02-13_00-30-08.png]]

[[file:img/screenshot_2018-02-13_00-30-56.png]]
