<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120656861-1"></script>
  <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());
   gtag('config', 'UA-120656861-1');
  </script>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="theme-color" content="#ffffff">

  <title>Practical aspects of Deep Learning</title>

  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>

  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <link rel="stylesheet" href="/_css/content.css" />


  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

  <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Condensed|Roboto+Mono" rel="stylesheet">
</head>
<body>
<nav class="menu">
  <ol class="breadcrumb">
    <li><a href="/" >Home</a></li>
    <li><a href="/coursera/" >coursera</a></li>
    <li><a href="/coursera/deeplearning-ai/" >deeplearning-ai</a></li>
    <li><a href="/coursera/deeplearning-ai/deep-neural-network/" >deep-neural-network</a></li>
    <li>week2</li>
  </ol>
</nav>

<article>
<header>
<h1 class="title">Practical aspects of Deep Learning</h1>
</header>

<nav class="toc">
<h2>Table of Contents</h2>
<ul>
<li><a href="#optimization-alogrithms">Optimization alogrithms</a><ul>
<li><a href="#mini-batch-gradient-descent">Mini-batch gradient descent</a></li>
<li><a href="#understanding-mini-batch-gradient-descent">Understanding mini-batch gradient descent</a></li>
<li><a href="#exponentially-weighted-averages">Exponentially weighted averages</a></li>
<li><a href="#understanding-exponentially-weighted-averages">Understanding exponentially weighted averages</a></li>
<li><a href="#bias-correction-in-exponentially-weighted-averages">Bias correction in exponentially weighted averages</a></li>
<li><a href="#gradient-descent-with-momentum">Gradient descent with momentum</a></li>
<li><a href="#rmsprop">RMSprop</a></li>
<li><a href="#adam-optimization-algorithm">Adam optimization algorithm</a></li>
<li><a href="#learning-rate-decay">Learning rate decay</a></li>
<li><a href="#the-problem-of-local-optima">The problem of local optima</a></li>
</ul></li>
<li><a href="#programming-assignment">Programming assignment</a><ul>
<li><a href="#optimization">Optimization</a></li>
</ul></li>
</ul>
</nav>

<h1 id="optimization-alogrithms">Optimization alogrithms</h1>
<h2 id="mini-batch-gradient-descent">Mini-batch gradient descent</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-18_07-50-21.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-18_07-56-43.png" /></p>
<h2 id="understanding-mini-batch-gradient-descent">Understanding mini-batch gradient descent</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-18_08-00-20.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-18_08-23-00.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-18_08-25-46.png" /></p>
<h2 id="exponentially-weighted-averages">Exponentially weighted averages</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-25-56.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-25-29.png" /></p>
<dl>
<dt><code>0.9</code></dt>
<dd>red
</dd>
<dt><code>0.98</code></dt>
<dd>green, morer smooth, shifted right because it now averages over a larger window.
</dd>
<dt><code>0.5</code></dt>
<dd>yellow
</dd>
</dl>
<h2 id="understanding-exponentially-weighted-averages">Understanding exponentially weighted averages</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-33-22.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-35-50.png" /></p>
<h2 id="bias-correction-in-exponentially-weighted-averages">Bias correction in exponentially weighted averages</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-40-11.png" /></p>
<ul>
<li>During initial phases, the values are very smaller than the actual values.(the coefficients of <code>0.0196</code> and <code>0.02</code> are both small)</li>
<li>With <code>Vt</code> / <code>(1 - B^t)</code>, only initial phases are affected, because as <code>t</code> becomes large, <code>B^t</code> goes <code>0</code>.</li>
</ul>
<h2 id="gradient-descent-with-momentum">Gradient descent with momentum</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-52-09.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-21_17-55-06.png" /></p>
<ul>
<li>Some practitioners omit <code>(1 - B)</code>. But it would change the sementic scale of <code>Vdw</code>, <code>Vdb</code>, which make the learning rate <code>a</code> have different meaning. So he recommends to keep <code>(1 - B)</code>.</li>
</ul>
<h2 id="rmsprop">RMSprop</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-27-14.png" /></p>
<h2 id="adam-optimization-algorithm">Adam optimization algorithm</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-27-46.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-28-03.png" /></p>
<h2 id="learning-rate-decay">Learning rate decay</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-28-33.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-28-51.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-29-11.png" /></p>
<blockquote>
<p>For me, I would say that learning rate decay is usually lower down on the list of things I try. Setting alpha, just a fixed value of alpha, and getting that to be well tuned, has a huge impact. Learning rate decay does help. Sometimes it can really help speed up training, but it is a little bit lower down my list in terms of the things I would try.</p>
</blockquote>
<h2 id="the-problem-of-local-optima">The problem of local optima</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-30-00.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_00-29-43.png" /></p>
<p>Local optima is unlikely to happen because there are lots of dimensions in most of practical problems. The probability of the actual local optima for all the related dimensions are extremly low.</p>
<h1 id="programming-assignment">Programming assignment</h1>
<h2 id="optimization">Optimization</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-01-57.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-02-36.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-03-02.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-03-25.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-04-24.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-05-48.png" /></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># Step 1: Shuffle (X, Y)</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">permutation <span class="op">=</span> <span class="bu">list</span>(np.random.permutation(m))</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">shuffled_X <span class="op">=</span> X[:, permutation]</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">shuffled_Y <span class="op">=</span> Y[:, permutation].reshape((<span class="dv">1</span>,m))</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-20-37.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-24-34.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-29-29.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-30-46.png" /></p>
<ul>
<li>Mini-batch gradient descent with momentum</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-47-14.png" /></p>
<ul>
<li>Mini-batch with Adam mode</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-48-42.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week2/_img/screenshot_2017-10-22_16-49-21.png" /></p>
<ul>
<li><a href="https://arxiv.org/pdf/1412.6980.pdf">Adam paper</a></li>
</ul>
</article>

</body>
</html>
