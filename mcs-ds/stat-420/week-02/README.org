#+TITLE: STAT 420: Week 2

* Table of Contents :TOC_2_gh:
- [[#simple-linear-regression][Simple Linear Regression]]
  - [[#models][Models]]
  - [[#fitting-a-line][Fitting a Line]]
  - [[#simple-linear-regression-model][Simple Linear Regression Model]]
- [[#the-normal-distribution][The Normal Distribution]]
  - [[#normal-distribution][Normal Distribution]]
  - [[#normal-distribution-in-r][Normal Distribution in R]]
- [[#slr-details][SLR Details]]
  - [[#estimation][Estimation]]
  - [[#variance-decomposition][Variance Decomposition]]
  - [[#maximum-likelihood][Maximum Likelihood]]
  - [[#the-lm-function][The lm() Function]]
- [[#slr-in-r][SLR in R]]
  - [[#simulation][Simulation]]
  - [[#simulating-slr-in-r][Simulating SLR in R]]

* Simple Linear Regression
- http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html

** Models
[[file:_img/screenshot_2018-05-21_16-43-40.png]]

- Predictor and Response

[[file:_img/screenshot_2018-05-21_16-44-57.png]]

- Overfit and Underfit

[[file:_img/screenshot_2018-05-21_16-48-15.png]]

#+BEGIN_QUOTE
All models are wrong; some models are useful. â€” George E. P. Box
#+END_QUOTE

** Fitting a Line
[[file:_img/screenshot_2018-05-21_16-54-45.png]]
1. Minimize the maximum error
2. Minimize the abs value of errors
3. Minimize the squared errors. This is easier to calculate beta than others.

[[file:_img/screenshot_2018-05-21_16-56-42.png]]

- Note that ~f(b0, b1)~ is convex. ([[https://math.stackexchange.com/questions/483339/proof-of-convexity-of-linear-least-squares][note1]], [[https://en.wikipedia.org/wiki/Derivative_test#Second_derivative_test_(single_variable)][note2]])

[[file:_img/screenshot_2018-05-21_16-56-49.png]]

#+BEGIN_SRC latex :file _img/solve-least-squares1.png :results raw :exports results :buffer no
  \begin{aligned}
  \hat{\beta}_1 &= \frac{\sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}}{\sum_{i = 1}^{n} x_i^2 - \frac{(\sum_{i = 1}^{n} x_i)^2}{n}} = \frac{S_{xy}}{S_{xx}}\\
  \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/solve-least-squares1.png]]

#+BEGIN_SRC latex :file _img/solve-least-squares2.png :results raw :exports results :buffer no
  \begin{aligned}
  S_{xy} &= \sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}  = \sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})\\
  S_{xx} &= \sum_{i = 1}^{n} x_i^2 - \frac{(\sum_{i = 1}^{n} x_i)^2}{n}  = \sum_{i = 1}^{n}(x_i - \bar{x})^2\\
  S_{yy} &= \sum_{i = 1}^{n} y_i^2 - \frac{(\sum_{i = 1}^{n} y_i)^2}{n}  = \sum_{i = 1}^{n}(y_i - \bar{y})^2
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/solve-least-squares2.png]]

This is because:
#+BEGIN_SRC latex :file _img/solve-least-squares3.png :results raw :exports results :buffer no
  \begin{aligned}
  \sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})
  &= \sum x_i y_i - \bar{x}\sum y_i - \bar{y}\sum x_i + \sum \bar{x}\bar{y}\\
  &= \sum x_i y_i - n \bar{x}\bar{y} - n \bar{x}\bar{y} + n \bar{x}\bar{y}\\
  &= \sum x_i y_i - n \bar{x}\bar{y}\\
  &= \sum x_i y_i - \frac{(\sum x) (\sum y)}{n}
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/solve-least-squares3.png]]

So,
#+BEGIN_SRC latex :file _img/solve-least-squares4.png :results raw :exports results :buffer no
  \begin{aligned}
  \hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^{n}(x_i - \bar{x})^2}
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/solve-least-squares4.png]]


#+LATEX_HEADER: \usepackage{amsmath}
#+BEGIN_SRC latex :file _img/interpretation.png :results raw :exports results :buffer no
  \begin{aligned}
  & \beta_0       &&  \text{the \textbf{mean} of $y$ when predictor is zero} \\
  & \hat{\beta_0} &&  \text{the \textbf{estimated mean} of $y$ when predictor is zero} \\
  & \beta_1       &&  \text{the \textbf{mean} of $y$ increases by this value} \\
  & \hat{\beta_1} &&  \text{the \textbf{estimated mean} increases by this value} \\
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/interpretation.png]]


[[file:_img/screenshot_2018-05-21_17-18-23.png]]

- Interpolation and Extrapolation
- The further we get away from the dataset, the less confident we are in our fitted model.
- -----
- http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#least-squares-approach

** Simple Linear Regression Model
[[file:_img/screenshot_2018-05-21_17-26-37.png]]

[[file:_img/screenshot_2018-05-21_17-27-45.png]]

[[file:_img/screenshot_2018-05-21_17-29-40.png]]

- Linear
- Independent
- Normal 
- Equal Variance
* The Normal Distribution
** Normal Distribution
[[file:_img/screenshot_2018-05-21_17-33-14.png]]

[[file:_img/screenshot_2018-05-21_17-35-50.png]]

[[file:_img/screenshot_2018-05-21_17-38-24.png]]

** Normal Distribution in R
- ~dnrom~ :: ~d~ stands for *density* (PDF)
- ~pnorm~ :: ~p~ stands for *probability* (CDF)

#+BEGIN_SRC R :results output :exports both
  diff(pnorm(c(100, 115), mean = 100, sd = 15))
#+END_SRC

#+RESULTS:
: [1] 0.3413447

#+BEGIN_SRC R :results output :exports both
  pnorm(130, mean = 100, sd = 15, lower.tail = FALSE)
#+END_SRC

#+RESULTS:
: [1] 0.02275013

#+BEGIN_SRC R :results output :exports both
  qnorm(0.95, mean = 100, sd = 15)
#+END_SRC

#+RESULTS:
: [1] 124.6728

#+BEGIN_SRC R :results output :exports both
  pnorm(c(0.5, 1, 0), mean = c(-1, 0, 1), sd = c(2, 1, 0.5))
  # Equivalant to above
  pnorm(0.5, mean = -1, sd = 2)
  pnorm(1, mean = 0, sd = 1)
  pnorm(0, mean = 1, sd = 0.5)
#+END_SRC

#+RESULTS:
: [1] 0.77337265 0.84134475 0.02275013
: [1] 0.7733726
: [1] 0.8413447
: [1] 0.02275013
* SLR Details
** Estimation
[[file:_img/screenshot_2018-05-21_19-27-20.png]]

[[file:_img/screenshot_2018-05-21_19-28-43.png]]

[[file:_img/screenshot_2018-05-21_19-31-30.png]]

- As the general sample variance is divided by ~(n - 1)~, the estimated variance should be divided by ~(n - 2)~, since there are two variables(~b0~, ~b1~)

** Variance Decomposition
[[file:_img/screenshot_2018-05-21_19-38-33.png]]

#+BEGIN_SRC latex :file _img/variance-decomposition.png :results raw :exports results :buffer no
  \begin{aligned}
  y_i - \bar{y} &= (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y}).\\
  \sum_{i=1}^{n}(y_i - \bar{y})^2 &= \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2.
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/variance-decomposition.png]]

This is true. This can be proven by using the fact that sum of the residuals are zero ([[https://autarkaw.org/2017/07/06/sum-of-the-residuals-for-the-linear-regression-model-is-zero/][proof]])

[[file:_img/screenshot_2018-05-24_20-50-34.png]]
([[https://www.xycoon.com/decom1.htm][link1]]) ([[https://www.xycoon.com/decom2.htm][link2]])

[[file:_img/screenshot_2018-05-21_19-39-29.png]]

[[file:_img/screenshot_2018-05-21_19-40-15.png]]

For the ~cars~ example, we calculate ~R^2 = 0.65~,
We then say that ~65%~ of the observed variability in stopping distance is explained by the linear relationship with speed.

[[file:_img/screenshot_2018-05-21_19-46-39.png]]

-----
- https://en.wikipedia.org/wiki/Coefficient_of_determination

** Maximum Likelihood
[[file:_img/screenshot_2018-05-24_21-00-09.png]]

[[file:_img/screenshot_2018-05-21_19-50-54.png]]

#+BEGIN_SRC latex :file _img/mle.png :results raw :exports results :buffer no
  \begin{aligned}
  \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i) &= 0\\
  \sum_{i = 1}^{n}(x_i)(y_i - \beta_0 - \beta_1 x_i) &= 0\\
  -\frac{n}{2 \sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 &= 0
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/mle.png]]

The first two equations above are the same as least squares.

#+BEGIN_SRC latex :file _img/least-squares-mle.png :results raw :exports results :buffer no
  \begin{aligned}
  s_e^2 &= \frac{1}{n - 2} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 = \frac{1}{n - 2} \sum_{i = 1}^{n}e_i^2 & \text{Least Squares}\\
  \hat{\sigma}^2 &= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i = 1}^{n}e_i^2 & \text{MLE}
  \end{aligned}
#+END_SRC

#+RESULTS:
[[file:_img/least-squares-mle.png]]


[[file:_img/screenshot_2018-05-21_19-51-14.png]]

[[file:_img/screenshot_2018-05-21_19-52-32.png]]
-----
- http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#maximum-likelihood-estimation-mle-approach

** The lm() Function
- ~lm()~ stands for ~linear model~.

#+BEGIN_SRC R
  stop_dist_model = lm(dist ~ speed, data = cars)

  plot(dist ~ speed, data = cars,
       xlab = "Speed (in Miles Per Hour)",
       ylab = "Stopping Distance (in Feet)",
       main = "Stopping Distance vs Speed",
       pch  = 20,
       cex  = 2,
       col  = "grey")
  abline(stop_dist_model, lwd = 3, col = "darkorange")

  coef(stop_dist_model)
  resid(stop_dist_model)
  fitted(stop_dist_model)
  summary(stop_dist_model)

  predict(stop_dist_model, newdata = data.frame(speed = 8))
  predict(stop_dist_model, newdata = data.frame(speed = c(8, 21, 50)))
  predict(stop_dist_model)
#+END_SRC
-----
- http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#the-lm-function

* SLR in R
** Simulation
[[file:_img/screenshot_2018-05-21_20-17-43.png]]

[[file:_img/screenshot_2018-05-21_20-21-07.png]]

** Simulating SLR in R
#+BEGIN_SRC R :results output :exports both
  num_obs = 21
  (x_vals = seq(from = 0, to = 10, length.out = num_obs))
#+END_SRC

#+RESULTS:
:  [1]  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0
: [16]  7.5  8.0  8.5  9.0  9.5 10.0

#+BEGIN_SRC R :results output :exports both
  num_obs = 21
  sigma = 3
  set.seed(1)
  (epsilon = rnorm(n = num_obs, mean = 0, sd = sigma))
#+END_SRC

#+RESULTS:
:  [1] -1.87936143  0.55092997 -2.50688584  4.78584241  0.98852332 -2.46140515
:  [7]  1.46228716  2.21497412  1.72734405 -0.91616516  4.53534351  1.16952971
: [13] -1.86372174 -6.64409966  3.37479275 -0.13480083 -0.04857079  2.83150863
: [19]  2.46366359  1.78170396  2.75693211

#+BEGIN_SRC R :file _img/sim_slr.png :results graphics :exports both
  num_obs = 21
  beta_0  = 5
  beta_1  = -2
  sigma   = 3

  x_vals = seq(from = 0, to = 10, length.out = num_obs)

  sim_slr = function(x, beta_0 = 10, beta_1 = 5, sigma = 1) {
    n = length(x)
    epsilon = rnorm(n, mean = 0, sd = sigma)
    y = beta_0 + beta_1 * x + epsilon
    data.frame(predictor = x, response = y)
  }

  set.seed(1)
  sim_data = sim_slr(x = x_vals, beta_0 = beta_0, beta_1 = beta_1, sigma = sigma)
  sim_fit = lm(response ~ predictor, data = sim_data)

  plot(response ~ predictor, data = sim_data,
       xlab = "Simulated Predictor Variable",
       ylab = "Simulated Response Variable",
       main = "Simulated Regression Data",
       pch  = 20,
       cex  = 2,
       col  = "grey")
  abline(sim_fit, lwd = 3, lty = 1, col = "darkorange")
  abline(beta_0, beta_1, lwd = 3, lty = 2, col = "dodgerblue")
  legend("topright", c("Estimate", "Truth"), lty = c(1, 2), lwd = 2,
         col = c("darkorange", "dodgerblue"))
#+END_SRC

#+RESULTS:
[[file:_img/sim_slr.png]]

-----
- http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr
