#+TITLE: Sequence models & Attention mechanism

* Table of Contents :TOC_3_gh:
- [[#various-sequence-to-sequence-architectures][Various sequence to sequence architectures]]
  - [[#basic-models][Basic Models]]
  - [[#picking-the-most-likely-sentence][Picking the most likely sentence]]
  - [[#beam-search][Beam Search]]
  - [[#refinements-to-beam-search][Refinements to Beam Search]]
  - [[#error-analysis-in-beam-search][Error analysis in beam search]]
  - [[#bleu-score-optional][Bleu Score (optional)]]
  - [[#attention-model-intuition][Attention Model Intuition]]
  - [[#attention-model][Attention Model]]
- [[#speech-recognition---audio-data][Speech recognition - Audio data]]
  - [[#speech-recognition][Speech recognition]]
  - [[#trigger-word-detection][Trigger Word Detection]]
- [[#conclusion][Conclusion]]
  - [[#conclusion-and-thank-you][Conclusion and thank you]]
- [[#practice-questions][Practice questions]]
  - [[#quiz-sequence-models--attention-mechanism][Quiz: Sequence models & Attention mechanism]]
- [[#programming-assignments][Programming assignments]]
  - [[#neural-machine-translation][Neural Machine Translation]]
  - [[#trigger-word-detection-1][Trigger word detection]]

* Various sequence to sequence architectures
** Basic Models
[[file:img/screenshot_2018-02-13_10-12-12.png]]

[[file:img/screenshot_2018-02-13_10-14-44.png]]

** Picking the most likely sentence
[[file:img/screenshot_2018-02-13_10-18-37.png]]

[[file:img/screenshot_2018-02-13_10-20-21.png]]

Unlike the language model previously introduced, in machine translation,
pick most likely ~y~ instead of sampling for consistent results.

[[file:img/screenshot_2018-02-13_10-26-51.png]]

Don't use greedy approach, otherwise ~Jane is going~, which is less succinct in the above example, will always be chosen.

** Beam Search
[[file:img/screenshot_2018-02-13_10-33-49.png]]

[[file:img/screenshot_2018-02-13_10-41-03.png]]

In the second step, pick the top 3 again among 30,000 possibilities.
Because Beam width is 3, every step needs 3 models.

[[file:img/screenshot_2018-02-13_10-44-05.png]]

** Refinements to Beam Search
[[file:img/screenshot_2018-02-14_08-47-01.png]]

The actual probability is too small to practically calculate.
So take log to the probabilities

[[file:img/screenshot_2018-02-14_08-50-10.png]]

** Error analysis in beam search
[[file:img/screenshot_2018-02-14_08-56-15.png]]

[[file:img/screenshot_2018-02-14_08-59-11.png]]

[[file:img/screenshot_2018-02-14_09-01-28.png]]

** Bleu Score (optional)
[[file:img/screenshot_2018-02-14_09-08-08.png]]

[[file:img/screenshot_2018-02-14_09-11-13.png]]

[[file:img/screenshot_2018-02-14_09-15-12.png]]

[[file:img/screenshot_2018-02-14_09-17-49.png]]
** Attention Model Intuition
[[file:img/screenshot_2018-02-14_09-25-35.png]]

[[file:img/screenshot_2018-02-14_09-37-13.png]]

** Attention Model
[[file:img/screenshot_2018-02-14_09-43-51.png]]

[[file:img/screenshot_2018-02-14_09-50-54.png]]

- Essentially, ~a^<t, t'>~ is a softmax function

[[file:img/screenshot_2018-02-14_09-52-26.png]]

* Speech recognition - Audio data
** Speech recognition
[[file:img/screenshot_2018-02-14_19-59-57.png]]

[[file:img/screenshot_2018-02-14_20-32-30.png]]

[[file:img/screenshot_2018-02-14_20-36-03.png]]

** Trigger Word Detection
[[file:img/screenshot_2018-02-14_20-53-46.png]]

* Conclusion
** Conclusion and thank you
[[file:img/screenshot_2018-02-14_20-55-38.png]]

[[file:img/screenshot_2018-02-14_20-57-36.png]]

* Practice questions
** Quiz: Sequence models & Attention mechanism

[[file:img/screenshot_2018-02-14_21-16-26.png]]

* Programming assignments
** Neural Machine Translation
** Trigger word detection
