#+TITLE: Recurrent Neural Networks

* Table of Contents :TOC_3_gh:
- [[#recurrent-neural-networks][Recurrent Neural Networks]]
  - [[#why-sequence-models][Why sequence models]]
  - [[#notation][Notation]]
  - [[#recurrent-neural-network-model][Recurrent Neural Network Model]]
  - [[#backpropagation-through-time][Backpropagation through time]]
  - [[#different-types-of-rnns][Different types of RNNs]]
  - [[#language-model-and-sequence-generation][Language model and sequence generation]]
  - [[#sampling-novel-sequences][Sampling novel sequences]]
  - [[#vanishing-gradients-with-rnns][Vanishing gradients with RNNs]]
  - [[#gated-recurrent-unit-gru][Gated Recurrent Unit (GRU)]]
  - [[#long-short-term-memory-lstm][Long Short Term Memory (LSTM)]]
  - [[#bidirectional-rnn][Bidirectional RNN]]
  - [[#deep-rnns][Deep RNNs]]

* Recurrent Neural Networks
** Why sequence models
[[file:img/screenshot_2018-02-06_13-59-38.png]]

** Notation
[[file:img/screenshot_2018-02-06_14-14-07.png]]

[[file:img/screenshot_2018-02-06_14-13-41.png]]

** Recurrent Neural Network Model
[[file:img/screenshot_2018-02-06_15-52-01.png]]

[[file:img/screenshot_2018-02-06_16-45-24.png]]

[[file:img/screenshot_2018-02-06_16-49-19.png]]

[[file:img/screenshot_2018-02-06_16-53-06.png]]

** Backpropagation through time
[[file:img/screenshot_2018-02-06_17-00-58.png]]

** Different types of RNNs
[[file:img/screenshot_2018-02-06_17-06-16.png]]

[[file:img/screenshot_2018-02-06_17-10-05.png]]

[[file:img/screenshot_2018-02-06_17-11-20.png]]
** Language model and sequence generation
[[file:img/screenshot_2018-02-06_17-15-34.png]]

[[file:img/screenshot_2018-02-06_17-18-43.png]]

[[file:img/screenshot_2018-02-06_17-24-33.png]]
** Sampling novel sequences
[[file:img/screenshot_2018-02-06_17-37-57.png]]

[[file:img/screenshot_2018-02-06_17-41-05.png]]

- Character-level language model needs longer sequences,
  which means that it's computationally expensive.
- So it is used in more specialized cases.

[[file:img/screenshot_2018-02-06_17-43-12.png]]
** Vanishing gradients with RNNs
[[file:img/screenshot_2018-02-06_17-50-15.png]]
** Gated Recurrent Unit (GRU)
[[file:img/screenshot_2018-02-06_17-53-28.png]]

[[file:img/screenshot_2018-02-06_18-06-17.png]]

[[file:img/screenshot_2018-02-06_18-08-57.png]]
** Long Short Term Memory (LSTM)
[[file:img/screenshot_2018-02-06_18-26-55.png]]

- GRU is simpler, LSTM is more powerful
- GRU or LSTM: There is no generally better algorithm between these.
- But historically, LSTM has been more commonly used.

** Bidirectional RNN
[[file:img/screenshot_2018-02-06_18-38-33.png]]
- BRNN has a disadvantage that it need the entire sequence to make predictions.

** Deep RNNs
[[file:img/screenshot_2018-02-06_18-45-48.png]]
