#+TITLE: Natural Language Processing & Word Embeddings

* Table of Contents :TOC_3_gh:
- [[#introduction-to-word-embeddings][Introduction to Word Embeddings]]
  - [[#word-representation][Word Representation]]
  - [[#using-word-embeddings][Using word embeddings]]
  - [[#properties-of-word-embeddings][Properties of word embeddings]]
  - [[#embedding-matrix][Embedding matrix]]
- [[#learning-word-embeddings-word2vec--glove][Learning Word Embeddings: Word2vec & GloVe]]
  - [[#learning-word-embeddings][Learning word embeddings]]
  - [[#word2vec][Word2Vec]]
  - [[#negative-sampling][Negative Sampling]]
  - [[#glove-word-vectors][GloVe word vectors]]
- [[#applications-using-word-embeddings][Applications using Word Embeddings]]
  - [[#sentiment-classification][Sentiment Classification]]
  - [[#debiasing-word-embeddings][Debiasing word embeddings]]
- [[#practice-questions][Practice questions]]
  - [[#quiz-natural-language-porcessing--word-embeddings][Quiz: Natural Language Porcessing & Word Embeddings]]
- [[#programming-assignments][Programming assignments]]
  - [[#operations-on-word-vectors---debiasing][Operations on word vectors - Debiasing]]
  - [[#emojify][Emojify]]

* Introduction to Word Embeddings
** Word Representation
[[file:img/screenshot_2018-02-11_22-32-17.png]]

[[file:img/screenshot_2018-02-11_22-37-17.png]]

[[file:img/screenshot_2018-02-11_22-40-11.png]]

** Using word embeddings
[[file:img/screenshot_2018-02-11_22-44-37.png]]

[[file:img/screenshot_2018-02-11_22-49-04.png]]

[[file:img/screenshot_2018-02-11_22-51-45.png]]
- The word embeddings of nlp funciton in a similar way as face encoding of face recognition.
- While word embeddings have a fixed number of inputs(vocabulary),
  face encoding differs in that it can have an infinite number of inputs(face images).

** Properties of word embeddings
[[file:img/screenshot_2018-02-11_23-11-50.png]]

[[file:img/screenshot_2018-02-11_23-16-24.png]]

[[file:img/screenshot_2018-02-11_23-19-10.png]]

** Embedding matrix
[[file:img/screenshot_2018-02-11_23-25-37.png]]

* Learning Word Embeddings: Word2vec & GloVe
** Learning word embeddings
[[file:img/screenshot_2018-02-12_10-33-46.png]]

[[file:img/screenshot_2018-02-12_10-37-12.png]]

** Word2Vec
[[file:img/screenshot_2018-02-12_10-40-40.png]]

[[file:img/screenshot_2018-02-12_10-45-31.png]]

- ~theta t~ is the parameter associated with the output ~t~.

[[file:img/screenshot_2018-02-12_10-50-30.png]]

- If you sample the context uniformly at radom, words like ~the~, ~of~, ~a~ , ~and~ will dominate the context.
- So there is some other heuristic method to sample the context.

** Negative Sampling
[[file:img/screenshot_2018-02-12_11-06-52.png]]

[[file:img/screenshot_2018-02-12_11-13-43.png]]

- Each target word(10k) has their binary classification model.
- Each model takes word embeddings as inputs, in previous examples, 300 features.
- Each model predicts probability of the word in charge.

[[file:img/screenshot_2018-02-12_15-38-08.png]]

- ~f(wi)^3/4~ is heuristic value, which is in between ~frequency distribution~ and ~uniform distribution~.

** GloVe word vectors
[[file:img/screenshot_2018-02-12_15-44-28.png]]

[[file:img/screenshot_2018-02-12_15-53-17.png]]

[[file:img/screenshot_2018-02-12_15-56-58.png]]

- Each feature can't be completely orgthogonal.
- But it still makes sense when it comes to word similarity.

* Applications using Word Embeddings
** Sentiment Classification
[[file:img/screenshot_2018-02-12_16-04-45.png]]

[[file:img/screenshot_2018-02-12_16-08-03.png]]

[[file:img/screenshot_2018-02-12_16-10-09.png]]

** Debiasing word embeddings
- the term ~bias~ in this lecture is NOT the ~bias~ as a parameter, but the bias in learned feature.

[[file:img/screenshot_2018-02-12_16-16-08.png]]

[[file:img/screenshot_2018-02-12_16-23-06.png]]

* Practice questions
** Quiz: Natural Language Porcessing & Word Embeddings

[[file:img/screenshot_2018-02-12_16-26-15.png]]

[[file:img/screenshot_2018-02-12_16-42-34.png]]

[[file:img/screenshot_2018-02-12_16-43-09.png]]

* Programming assignments
** Operations on word vectors - Debiasing
[[file:img/screenshot_2018-02-12_23-23-31.png]]

[[file:img/screenshot_2018-02-12_23-24-13.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: cosine_similarity

  def cosine_similarity(u, v):
      """
      Cosine similarity reflects the degree of similariy between u and v

      Arguments:
          u -- a word vector of shape (n,)
          v -- a word vector of shape (n,)

      Returns:
          cosine_similarity -- the cosine similarity between u and v defined by the formula above.
      """

      distance = 0.0

      ### START CODE HERE ###
      # Compute the dot product between u and v (≈1 line)
      dot = None
      # Compute the L2 norm of u (≈1 line)
      norm_u = None

      # Compute the L2 norm of v (≈1 line)
      norm_v = None
      # Compute the cosine similarity defined by formula (1) (≈1 line)
      cosine_similarity = None
      ### END CODE HERE ###

      return cosine_similarity
#+END_SRC

[[file:img/screenshot_2018-02-12_23-28-58.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: complete_analogy

  def complete_analogy(word_a, word_b, word_c, word_to_vec_map):
      """
      Performs the word analogy task as explained above: a is to b as c is to ____.

      Arguments:
      word_a -- a word, string
      word_b -- a word, string
      word_c -- a word, string
      word_to_vec_map -- dictionary that maps words to their corresponding vectors.

      Returns:
      best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity
      """

      # convert words to lower case
      word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()

      ### START CODE HERE ###
      # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)
      e_a, e_b, e_c = None
      ### END CODE HERE ###

      words = word_to_vec_map.keys()
      max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number
      best_word = None                   # Initialize best_word with None, it will help keep track of the word to output

      # loop over the whole word vector set
      for w in words:
          # to avoid best_word being one of the input words, pass on them.
          if w in [word_a, word_b, word_c] :
              continue

          ### START CODE HERE ###
          # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)
          cosine_sim = None

          # If the cosine_sim is more than the max_cosine_sim seen so far,
              # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)
          if None > None:
              max_cosine_sim = None
              best_word = None
          ### END CODE HERE ###

      return best_word
#+END_SRC

[[file:img/screenshot_2018-02-12_23-33-08.png]]

** Emojify
