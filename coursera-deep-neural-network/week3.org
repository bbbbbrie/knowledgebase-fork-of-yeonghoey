#+TITLE: Hyperparameter tuning, Batch Normalization and Programming Frameworks

* Table of Contents :TOC_3_gh:
- [[#hyperparameter-tuning][Hyperparameter tuning]]
  - [[#tuning-process][Tuning process]]
  - [[#using-an-appropriate-scale-to-pick-hyperparameters][Using an appropriate scale to pick hyperparameters]]
  - [[#hyperparameters-tuning-in-practice-pandas-vs-caviar][Hyperparameters tuning in practice: Pandas vs. Caviar]]
- [[#batch-normalization][Batch Normalization]]
  - [[#normalizing-activations-in-a-network][Normalizing activations in a network]]
  - [[#fitting-batch-norm-into-a-neural-network][Fitting Batch Norm into a neural network]]
  - [[#why-does-batch-norm-work][Why does Batch Norm work?]]
  - [[#batch-norm-at-test-time][Batch Norm at test time]]
- [[#multi-class-classification][Multi-class classification]]
  - [[#softmax-regression][Softmax Regression]]
  - [[#training-a-softmax-classifier][Training a softmax classifier]]

* Hyperparameter tuning
** Tuning process
[[file:img/screenshot_2017-10-24_07-04-16.png]]

- Order of importance :: ~red~ > ~orange~ > ~purple~
- Hyperparameters for Adam :: Use default values for most cases

[[file:img/screenshot_2017-10-24_07-08-48.png]]

- *Prefer random values to grid values*
- It's hard to know which parameters are more important.
- Using grid, there would be lots of meaningless tests.
- These points go deep when there are more dimensions.

[[file:img/screenshot_2017-10-24_07-13-47.png]]

- At first, pick random values in wide range.
- When you find some points work better, narrow the random ranges.

** Using an appropriate scale to pick hyperparameters
[[file:img/screenshot_2017-10-24_07-17-49.png]]

- It's not always effective to pick hyperparameters by sampling uniformly at random.

[[file:img/screenshot_2017-10-24_07-22-27.png]]

[[file:img/screenshot_2017-10-24_07-26-26.png]]

** Hyperparameters tuning in practice: Pandas vs. Caviar
[[file:img/screenshot_2017-10-24_07-47-41.png]]

[[file:img/screenshot_2017-10-24_07-52-27.png]]

- Depends on available computation resources.

* Batch Normalization
** Normalizing activations in a network
[[file:img/screenshot_2017-10-26_02-01-03.png]]

- People generally normalize ~z~ instead of ~a~

[[file:img/screenshot_2017-10-26_02-06-30.png]]

- ~r~ (gamma) and ~b~ (beta) are to control the distribution of ~z~,
  because unlike ~x~, some ~z~ s will be inputs of the activation functions.
  If the activation function is sigmoid and ~z~ follows ~(0, 1)~,
  ~a~ s will be distributed just like a linear values.

** Fitting Batch Norm into a neural network
[[file:img/screenshot_2017-10-26_02-21-56.png]]

[[file:img/screenshot_2017-10-26_02-26-42.png]]

- Using Batch Norm, there is no point to have ~b~ (bias terms), becausue they are cleaned off when calculating ~z_norm~.

[[file:img/screenshot_2017-10-26_02-31-08.png]]

** Why does Batch Norm work?
[[file:img/screenshot_2017-10-27_23-56-49.png]]

[[file:img/screenshot_2017-10-27_23-59-05.png]]

- Batch Norm prevents changing the input values of hidde layers
- For the 3rd layer, for example, without Batch Norm, the input of the 3rd layer will be affected by the previous values

[[file:img/screenshot_2017-10-28_00-05-06.png]]

** Batch Norm at test time
[[file:img/screenshot_2017-10-28_00-46-33.png]]

#+BEGIN_QUOTE
But at test time, you might need to process a single example at a time.
So, the way to do that is to estimate \mu and \sigma squared from your training set and there are many ways to do that.
(...) what people usually do is (...) exponentially weighted averages (...)
#+END_QUOTE

* Multi-class classification
** Softmax Regression
[[file:img/screenshot_2017-10-28_07-07-24.png]]

[[file:img/screenshot_2017-10-28_07-13-10.png]]

[[file:img/screenshot_2017-10-28_07-16-06.png]]

** Training a softmax classifier
