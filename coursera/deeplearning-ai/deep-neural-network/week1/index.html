<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120656861-1"></script>
  <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());
   gtag('config', 'UA-120656861-1');
  </script>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="theme-color" content="#ffffff">

  <title>Practical aspects of Deep Learning</title>

  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>

  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <link rel="stylesheet" href="/_css/content.css" />


  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

  <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Condensed|Roboto+Mono" rel="stylesheet">
</head>
<body>
<nav class="menu">
  <ol class="breadcrumb">
    <li><a href="/" >Home</a></li>
    <li><a href="/coursera/" >coursera</a></li>
    <li><a href="/coursera/deeplearning-ai/" >deeplearning-ai</a></li>
    <li><a href="/coursera/deeplearning-ai/deep-neural-network/" >deep-neural-network</a></li>
    <li>week1</li>
  </ol>
</nav>

<article>
<header>
<h1 class="title">Practical aspects of Deep Learning</h1>
</header>

<nav class="toc">
<h2>Table of Contents</h2>
<ul>
<li><a href="#setting-up-your-machine-learning-application">Setting up your Machine Learning Application</a><ul>
<li><a href="#train-dev-test-sets">Train / Dev / Test sets</a></li>
<li><a href="#bias-variance">Bias / Variance</a></li>
<li><a href="#basic-recipe-for-machine-learning">Basic Recipe for Machine Learning</a></li>
</ul></li>
<li><a href="#regularizing-your-neural-network">Regularizing your neural network</a><ul>
<li><a href="#regularization">Regularization</a></li>
<li><a href="#why-regularization-reduces-overfitting">Why regularization reduces overfitting?</a></li>
<li><a href="#dropout-regularization">Dropout Regularization</a></li>
<li><a href="#understanding-dropout">Understanding Dropout</a></li>
<li><a href="#other-regularization-methods">Other regularization methods</a></li>
</ul></li>
<li><a href="#setting-up-your-optimization-problem">Setting up your optimization problem</a><ul>
<li><a href="#normalizing-inputs">Normalizing inputs</a></li>
<li><a href="#vanishing-exploding-gradients">Vanishing / Exploding gradients</a></li>
<li><a href="#weight-initialization-for-deep-networks">Weight Initialization for Deep Networks</a></li>
<li><a href="#numerical-approximation-of-gradients">Numerical approximation of gradients</a></li>
<li><a href="#gradient-checking">Gradient checking</a></li>
<li><a href="#gradient-checking-implementation-notes">Gradient Checking Implementation Notes</a></li>
</ul></li>
<li><a href="#programming-assignments">Programming assignments</a><ul>
<li><a href="#initialization">Initialization</a></li>
<li><a href="#regularization-1">Regularization</a></li>
<li><a href="#gradient-checking-1">Gradient Checking</a></li>
</ul></li>
</ul>
</nav>

<h1 id="setting-up-your-machine-learning-application">Setting up your Machine Learning Application</h1>
<h2 id="train-dev-test-sets">Train / Dev / Test sets</h2>
<ul>
<li><a href="https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo" class="uri">https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo</a></li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_09-22-34.png" /></p>
<p>Traditionally, 7:3 (train:dev) or 6:2:2 (train:dev:test), is a rule of thumb for data set. But, in these days, big data came out, and there are a lot more data than past. We can decrease dev and test data portion ratio, because the size of data set would just be enough in absolute perspective with a big data set.</p>
<dl>
<dt>Training Set</dt>
<dd><p>This data set is used to adjust the weights on the neural network.</p>
</dd>
<dt>Validation(Dev) Set</dt>
<dd><p>This data set is used to minimize overfitting. If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.</p>
</dd>
<dt>Testing Set</dt>
<dd><p>This data set is used <strong>only for testing the final solution in order to confirm the actual predictive power</strong> of the network.</p>
</dd>
</dl>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_09-13-51.png" /></p>
<blockquote>
<p>Make sure that the dev and test sets come from the same distribution.</p>
</blockquote>
<h2 id="bias-variance">Bias / Variance</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_09-33-46.png" /></p>
<ul>
<li><a href="http://slideplayer.com/slide/8744653/" class="uri">http://slideplayer.com/slide/8744653/</a></li>
<li><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_09-42-42.png" /></li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_09-44-20.png" /></p>
<h2 id="basic-recipe-for-machine-learning">Basic Recipe for Machine Learning</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_09-52-29.png" /></p>
<h1 id="regularizing-your-neural-network">Regularizing your neural network</h1>
<h2 id="regularization">Regularization</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-01-58.png" /></p>
<h2 id="why-regularization-reduces-overfitting">Why regularization reduces overfitting?</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-08-39.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-15-16.png" /></p>
<ul>
<li>When we increase <code>lambda</code> to a huge number, <code>W</code> is going close to zero.</li>
<li><code>W</code> close to zeon means that the nework is going to be simpler.</li>
<li>Which will alleviate the high variance problem.</li>
</ul>
<h2 id="dropout-regularization">Dropout Regularization</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-20-53.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-28-17.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-30-31.png" /></p>
<h2 id="understanding-dropout">Understanding Dropout</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-38-29.png" /></p>
<h2 id="other-regularization-methods">Other regularization methods</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-43-27.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-14_10-48-46.png" /></p>
<blockquote>
<p>The main downside of early stopping is that this couples these two tasks.</p>
</blockquote>
<h1 id="setting-up-your-optimization-problem">Setting up your optimization problem</h1>
<h2 id="normalizing-inputs">Normalizing inputs</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_07-19-10.png" /></p>
<blockquote>
<p>Use the <code>mu</code> and <code>sigma</code> of training set when normalizing test set.</p>
</blockquote>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_07-22-47.png" /></p>
<h2 id="vanishing-exploding-gradients">Vanishing / Exploding gradients</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_07-27-40.png" /></p>
<h2 id="weight-initialization-for-deep-networks">Weight Initialization for Deep Networks</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_07-33-01.png" /></p>
<h2 id="numerical-approximation-of-gradients">Numerical approximation of gradients</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_14-32-21.png" /></p>
<h2 id="gradient-checking">Gradient checking</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_14-42-30.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_14-47-13.png" /></p>
<h2 id="gradient-checking-implementation-notes">Gradient Checking Implementation Notes</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_14-53-42.png" /></p>
<h1 id="programming-assignments">Programming assignments</h1>
<h2 id="initialization">Initialization</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-06-59.png" /></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">def</span> model(X, Y, learning_rate <span class="op">=</span> <span class="fl">0.01</span>, num_iterations <span class="op">=</span> <span class="dv">15000</span>, print_cost <span class="op">=</span> <span class="va">True</span>, initialization <span class="op">=</span> <span class="st">&quot;he&quot;</span>):</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="co">    X -- input data, of shape (2, number of examples)</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co">    Y -- true &quot;label&quot; vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co">    learning_rate -- learning rate for gradient descent</span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="co">    num_iterations -- number of iterations to run gradient descent</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="co">    print_cost -- if True, print the cost every 1000 iterations</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11"><span class="co">    initialization -- flag to choose which initialization to use (&quot;zeros&quot;,&quot;random&quot; or &quot;he&quot;)</span></a>
<a class="sourceLine" id="cb1-12" data-line-number="12"></a>
<a class="sourceLine" id="cb1-13" data-line-number="13"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb1-14" data-line-number="14"><span class="co">    parameters -- parameters learnt by the model</span></a>
<a class="sourceLine" id="cb1-15" data-line-number="15"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-16" data-line-number="16"></a>
<a class="sourceLine" id="cb1-17" data-line-number="17">    grads <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb1-18" data-line-number="18">    costs <span class="op">=</span> [] <span class="co"># to keep track of the loss</span></a>
<a class="sourceLine" id="cb1-19" data-line-number="19">    m <span class="op">=</span> X.shape[<span class="dv">1</span>] <span class="co"># number of examples</span></a>
<a class="sourceLine" id="cb1-20" data-line-number="20">    layers_dims <span class="op">=</span> [X.shape[<span class="dv">0</span>], <span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1-21" data-line-number="21"></a>
<a class="sourceLine" id="cb1-22" data-line-number="22">    <span class="co"># Initialize parameters dictionary.</span></a>
<a class="sourceLine" id="cb1-23" data-line-number="23">    <span class="cf">if</span> initialization <span class="op">==</span> <span class="st">&quot;zeros&quot;</span>:</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">        parameters <span class="op">=</span> initialize_parameters_zeros(layers_dims)</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">    <span class="cf">elif</span> initialization <span class="op">==</span> <span class="st">&quot;random&quot;</span>:</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">        parameters <span class="op">=</span> initialize_parameters_random(layers_dims)</a>
<a class="sourceLine" id="cb1-27" data-line-number="27">    <span class="cf">elif</span> initialization <span class="op">==</span> <span class="st">&quot;he&quot;</span>:</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">        parameters <span class="op">=</span> initialize_parameters_he(layers_dims)</a>
<a class="sourceLine" id="cb1-29" data-line-number="29"></a>
<a class="sourceLine" id="cb1-30" data-line-number="30">    <span class="co"># Loop (gradient descent)</span></a>
<a class="sourceLine" id="cb1-31" data-line-number="31"></a>
<a class="sourceLine" id="cb1-32" data-line-number="32">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, num_iterations):</a>
<a class="sourceLine" id="cb1-33" data-line-number="33"></a>
<a class="sourceLine" id="cb1-34" data-line-number="34">        <span class="co"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></a>
<a class="sourceLine" id="cb1-35" data-line-number="35">        a3, cache <span class="op">=</span> forward_propagation(X, parameters)</a>
<a class="sourceLine" id="cb1-36" data-line-number="36"></a>
<a class="sourceLine" id="cb1-37" data-line-number="37">        <span class="co"># Loss</span></a>
<a class="sourceLine" id="cb1-38" data-line-number="38">        cost <span class="op">=</span> compute_loss(a3, Y)</a>
<a class="sourceLine" id="cb1-39" data-line-number="39"></a>
<a class="sourceLine" id="cb1-40" data-line-number="40">        <span class="co"># Backward propagation.</span></a>
<a class="sourceLine" id="cb1-41" data-line-number="41">        grads <span class="op">=</span> backward_propagation(X, Y, cache)</a>
<a class="sourceLine" id="cb1-42" data-line-number="42"></a>
<a class="sourceLine" id="cb1-43" data-line-number="43">        <span class="co"># Update parameters.</span></a>
<a class="sourceLine" id="cb1-44" data-line-number="44">        parameters <span class="op">=</span> update_parameters(parameters, grads, learning_rate)</a>
<a class="sourceLine" id="cb1-45" data-line-number="45"></a>
<a class="sourceLine" id="cb1-46" data-line-number="46">        <span class="co"># Print the loss every 1000 iterations</span></a>
<a class="sourceLine" id="cb1-47" data-line-number="47">        <span class="cf">if</span> print_cost <span class="kw">and</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">            <span class="bu">print</span>(<span class="st">&quot;Cost after iteration </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(i, cost))</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">            costs.append(cost)</a>
<a class="sourceLine" id="cb1-50" data-line-number="50"></a>
<a class="sourceLine" id="cb1-51" data-line-number="51">    <span class="co"># plot the loss</span></a>
<a class="sourceLine" id="cb1-52" data-line-number="52">    plt.plot(costs)</a>
<a class="sourceLine" id="cb1-53" data-line-number="53">    plt.ylabel(<span class="st">&#39;cost&#39;</span>)</a>
<a class="sourceLine" id="cb1-54" data-line-number="54">    plt.xlabel(<span class="st">&#39;iterations (per hundreds)&#39;</span>)</a>
<a class="sourceLine" id="cb1-55" data-line-number="55">    plt.title(<span class="st">&quot;Learning rate =&quot;</span> <span class="op">+</span> <span class="bu">str</span>(learning_rate))</a>
<a class="sourceLine" id="cb1-56" data-line-number="56">    plt.show()</a>
<a class="sourceLine" id="cb1-57" data-line-number="57"></a>
<a class="sourceLine" id="cb1-58" data-line-number="58">    <span class="cf">return</span> parameters</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-24-15.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-27-52.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-28-19.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-31-42.png" /></p>
<h2 id="regularization-1">Regularization</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-33-24.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-41-53.png" /></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> model(X, Y, learning_rate <span class="op">=</span> <span class="fl">0.3</span>, num_iterations <span class="op">=</span> <span class="dv">30000</span>, print_cost <span class="op">=</span> <span class="va">True</span>, lambd <span class="op">=</span> <span class="dv">0</span>, keep_prob <span class="op">=</span> <span class="dv">1</span>):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="co">    X -- input data, of shape (input size, number of examples)</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="co">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="co">    learning_rate -- learning rate of the optimization</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">    num_iterations -- number of iterations of the optimization loop</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="co">    print_cost -- If True, print the cost every 10000 iterations</span></a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="co">    lambd -- regularization hyperparameter, scalar</span></a>
<a class="sourceLine" id="cb2-12" data-line-number="12"><span class="co">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></a>
<a class="sourceLine" id="cb2-13" data-line-number="13"></a>
<a class="sourceLine" id="cb2-14" data-line-number="14"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb2-15" data-line-number="15"><span class="co">    parameters -- parameters learned by the model. They can then be used to predict.</span></a>
<a class="sourceLine" id="cb2-16" data-line-number="16"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-17" data-line-number="17"></a>
<a class="sourceLine" id="cb2-18" data-line-number="18">    grads <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb2-19" data-line-number="19">    costs <span class="op">=</span> []                            <span class="co"># to keep track of the cost</span></a>
<a class="sourceLine" id="cb2-20" data-line-number="20">    m <span class="op">=</span> X.shape[<span class="dv">1</span>]                        <span class="co"># number of examples</span></a>
<a class="sourceLine" id="cb2-21" data-line-number="21">    layers_dims <span class="op">=</span> [X.shape[<span class="dv">0</span>], <span class="dv">20</span>, <span class="dv">3</span>, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb2-22" data-line-number="22"></a>
<a class="sourceLine" id="cb2-23" data-line-number="23">    <span class="co"># Initialize parameters dictionary.</span></a>
<a class="sourceLine" id="cb2-24" data-line-number="24">    parameters <span class="op">=</span> initialize_parameters(layers_dims)</a>
<a class="sourceLine" id="cb2-25" data-line-number="25"></a>
<a class="sourceLine" id="cb2-26" data-line-number="26">    <span class="co"># Loop (gradient descent)</span></a>
<a class="sourceLine" id="cb2-27" data-line-number="27"></a>
<a class="sourceLine" id="cb2-28" data-line-number="28">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, num_iterations):</a>
<a class="sourceLine" id="cb2-29" data-line-number="29"></a>
<a class="sourceLine" id="cb2-30" data-line-number="30">        <span class="co"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></a>
<a class="sourceLine" id="cb2-31" data-line-number="31">        <span class="cf">if</span> keep_prob <span class="op">==</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb2-32" data-line-number="32">            a3, cache <span class="op">=</span> forward_propagation(X, parameters)</a>
<a class="sourceLine" id="cb2-33" data-line-number="33">        <span class="cf">elif</span> keep_prob <span class="op">&lt;</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb2-34" data-line-number="34">            a3, cache <span class="op">=</span> forward_propagation_with_dropout(X, parameters, keep_prob)</a>
<a class="sourceLine" id="cb2-35" data-line-number="35"></a>
<a class="sourceLine" id="cb2-36" data-line-number="36">        <span class="co"># Cost function</span></a>
<a class="sourceLine" id="cb2-37" data-line-number="37">        <span class="cf">if</span> lambd <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb2-38" data-line-number="38">            cost <span class="op">=</span> compute_cost(a3, Y)</a>
<a class="sourceLine" id="cb2-39" data-line-number="39">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb2-40" data-line-number="40">            cost <span class="op">=</span> compute_cost_with_regularization(a3, Y, parameters, lambd)</a>
<a class="sourceLine" id="cb2-41" data-line-number="41"></a>
<a class="sourceLine" id="cb2-42" data-line-number="42">        <span class="co"># Backward propagation.</span></a>
<a class="sourceLine" id="cb2-43" data-line-number="43">        <span class="cf">assert</span>(lambd<span class="op">==</span><span class="dv">0</span> <span class="kw">or</span> keep_prob<span class="op">==</span><span class="dv">1</span>)    <span class="co"># it is possible to use both L2 regularization and dropout,</span></a>
<a class="sourceLine" id="cb2-44" data-line-number="44">                                            <span class="co"># but this assignment will only explore one at a time</span></a>
<a class="sourceLine" id="cb2-45" data-line-number="45">        <span class="cf">if</span> lambd <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> keep_prob <span class="op">==</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb2-46" data-line-number="46">            grads <span class="op">=</span> backward_propagation(X, Y, cache)</a>
<a class="sourceLine" id="cb2-47" data-line-number="47">        <span class="cf">elif</span> lambd <span class="op">!=</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb2-48" data-line-number="48">            grads <span class="op">=</span> backward_propagation_with_regularization(X, Y, cache, lambd)</a>
<a class="sourceLine" id="cb2-49" data-line-number="49">        <span class="cf">elif</span> keep_prob <span class="op">&lt;</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb2-50" data-line-number="50">            grads <span class="op">=</span> backward_propagation_with_dropout(X, Y, cache, keep_prob)</a>
<a class="sourceLine" id="cb2-51" data-line-number="51"></a>
<a class="sourceLine" id="cb2-52" data-line-number="52">        <span class="co"># Update parameters.</span></a>
<a class="sourceLine" id="cb2-53" data-line-number="53">        parameters <span class="op">=</span> update_parameters(parameters, grads, learning_rate)</a>
<a class="sourceLine" id="cb2-54" data-line-number="54"></a>
<a class="sourceLine" id="cb2-55" data-line-number="55">        <span class="co"># Print the loss every 10000 iterations</span></a>
<a class="sourceLine" id="cb2-56" data-line-number="56">        <span class="cf">if</span> print_cost <span class="kw">and</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb2-57" data-line-number="57">            <span class="bu">print</span>(<span class="st">&quot;Cost after iteration </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(i, cost))</a>
<a class="sourceLine" id="cb2-58" data-line-number="58">        <span class="cf">if</span> print_cost <span class="kw">and</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb2-59" data-line-number="59">            costs.append(cost)</a>
<a class="sourceLine" id="cb2-60" data-line-number="60"></a>
<a class="sourceLine" id="cb2-61" data-line-number="61">    <span class="co"># plot the cost</span></a>
<a class="sourceLine" id="cb2-62" data-line-number="62">    plt.plot(costs)</a>
<a class="sourceLine" id="cb2-63" data-line-number="63">    plt.ylabel(<span class="st">&#39;cost&#39;</span>)</a>
<a class="sourceLine" id="cb2-64" data-line-number="64">    plt.xlabel(<span class="st">&#39;iterations (x1,000)&#39;</span>)</a>
<a class="sourceLine" id="cb2-65" data-line-number="65">    plt.title(<span class="st">&quot;Learning rate =&quot;</span> <span class="op">+</span> <span class="bu">str</span>(learning_rate))</a>
<a class="sourceLine" id="cb2-66" data-line-number="66">    plt.show()</a>
<a class="sourceLine" id="cb2-67" data-line-number="67"></a>
<a class="sourceLine" id="cb2-68" data-line-number="68">    <span class="cf">return</span> parameters</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-42-46.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-43-08.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-46-48.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-50-41.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-51-45.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_15-56-56.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-00-24.png" /></p>
<h2 id="gradient-checking-1">Gradient Checking</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-05-00.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-05-29.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-05-41.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-10-33.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-18-13.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-18-39.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/deep-neural-network/week1/_img/screenshot_2017-10-15_16-29-56.png" /></p>
</article>

</body>
</html>
