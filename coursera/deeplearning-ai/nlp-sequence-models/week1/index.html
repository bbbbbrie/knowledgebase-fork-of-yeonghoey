<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120656861-1"></script>
  <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());
   gtag('config', 'UA-120656861-1');
  </script>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="theme-color" content="#ffffff">

  <title>Recurrent Neural Networks</title>

  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>

  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <link rel="stylesheet" href="/_css/content.css" />


  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

  <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Condensed|Roboto+Mono" rel="stylesheet">
</head>
<body>
<nav class="menu">
  <ol class="breadcrumb">
    <li><a href="/" >Home</a></li>
    <li><a href="/coursera/" >coursera</a></li>
    <li><a href="/coursera/deeplearning-ai/" >deeplearning-ai</a></li>
    <li><a href="/coursera/deeplearning-ai/nlp-sequence-models/" >nlp-sequence-models</a></li>
    <li>week1</li>
  </ol>
</nav>

<article>
<header>
<h1 class="title">Recurrent Neural Networks</h1>
</header>

<nav class="toc">
<h2>Table of Contents</h2>
<ul>
<li><a href="#recurrent-neural-networks-1">Recurrent Neural Networks</a><ul>
<li><a href="#why-sequence-models">Why sequence models</a></li>
<li><a href="#notation">Notation</a></li>
<li><a href="#recurrent-neural-network-model">Recurrent Neural Network Model</a></li>
<li><a href="#backpropagation-through-time">Backpropagation through time</a></li>
<li><a href="#different-types-of-rnns">Different types of RNNs</a></li>
<li><a href="#language-model-and-sequence-generation">Language model and sequence generation</a></li>
<li><a href="#sampling-novel-sequences">Sampling novel sequences</a></li>
<li><a href="#vanishing-gradients-with-rnns">Vanishing gradients with RNNs</a></li>
<li><a href="#gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</a></li>
<li><a href="#long-short-term-memory-lstm">Long Short Term Memory (LSTM)</a></li>
<li><a href="#bidirectional-rnn">Bidirectional RNN</a></li>
<li><a href="#deep-rnns">Deep RNNs</a></li>
</ul></li>
<li><a href="#programming-assignments">Programming assignments</a><ul>
<li><a href="#building-a-recurrent-neural-network---step-by-step">Building a recurrent neural network - step by step</a></li>
<li><a href="#dinosaur-island---character-level-language-modeling">Dinosaur Island - Character-Level Language Modeling</a></li>
<li><a href="#jazz-improvisation-with-lstm">Jazz improvisation with LSTM</a></li>
</ul></li>
</ul>
</nav>

<h1 id="recurrent-neural-networks-1">Recurrent Neural Networks</h1>
<h2 id="why-sequence-models">Why sequence models</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_13-59-38.png" /></p>
<h2 id="notation">Notation</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_14-14-07.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_14-13-41.png" /></p>
<h2 id="recurrent-neural-network-model">Recurrent Neural Network Model</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_15-52-01.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_16-45-24.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_16-49-19.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_16-53-06.png" /></p>
<h2 id="backpropagation-through-time">Backpropagation through time</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-00-58.png" /></p>
<h2 id="different-types-of-rnns">Different types of RNNs</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-06-16.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-10-05.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-11-20.png" /></p>
<h2 id="language-model-and-sequence-generation">Language model and sequence generation</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-15-34.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-18-43.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-24-33.png" /></p>
<h2 id="sampling-novel-sequences">Sampling novel sequences</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-37-57.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-41-05.png" /></p>
<ul>
<li>Character-level language model needs longer sequences, which means that it's computationally expensive.</li>
<li>So it is used in more specialized cases.</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-43-12.png" /></p>
<h2 id="vanishing-gradients-with-rnns">Vanishing gradients with RNNs</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-50-15.png" /></p>
<h2 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_17-53-28.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_18-06-17.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_18-08-57.png" /></p>
<h2 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_18-26-55.png" /></p>
<ul>
<li>GRU is simpler, LSTM is more powerful</li>
<li>GRU or LSTM: There is no generally better algorithm between these.</li>
<li>But historically, LSTM has been more commonly used.</li>
</ul>
<h2 id="bidirectional-rnn">Bidirectional RNN</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_18-38-33.png" /></p>
<ul>
<li>BRNN has a disadvantage that it need the entire sequence to make predictions.</li>
</ul>
<h2 id="deep-rnns">Deep RNNs</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-06_18-45-48.png" /></p>
<h1 id="programming-assignments">Programming assignments</h1>
<h2 id="building-a-recurrent-neural-network---step-by-step">Building a recurrent neural network - step by step</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_06-40-18.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_06-42-11.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_06-42-56.png" /></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">def</span> rnn_cell_forward(xt, a_prev, parameters):</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="co">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co">    parameters -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="co">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="co">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11"><span class="co">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="co">                        ba --  Bias, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb1-13" data-line-number="13"><span class="co">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></a>
<a class="sourceLine" id="cb1-14" data-line-number="14"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb1-15" data-line-number="15"><span class="co">    a_next -- next hidden state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb1-16" data-line-number="16"><span class="co">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span></a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="co">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-19" data-line-number="19">    <span class="cf">return</span> a_next, yt_pred, cache</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_06-53-24.png" /></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> rnn_forward(x, a0, parameters):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="co">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="co">    a0 -- Initial hidden state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="co">    parameters -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="co">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="co">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></a>
<a class="sourceLine" id="cb2-12" data-line-number="12"><span class="co">                        ba --  Bias numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="co">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></a>
<a class="sourceLine" id="cb2-14" data-line-number="14"></a>
<a class="sourceLine" id="cb2-15" data-line-number="15"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb2-16" data-line-number="16"><span class="co">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></a>
<a class="sourceLine" id="cb2-17" data-line-number="17"><span class="co">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></a>
<a class="sourceLine" id="cb2-18" data-line-number="18"><span class="co">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-20" data-line-number="20">    <span class="cf">return</span> a, y_pred, caches</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-01-53.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-02-08.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-02-24.png" /></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">def</span> lstm_cell_forward(xt, a_prev, c_prev, parameters):</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="co">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"></a>
<a class="sourceLine" id="cb3-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6"><span class="co">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7"><span class="co">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8"><span class="co">    c_prev -- Memory state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9"><span class="co">    parameters -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb3-10" data-line-number="10"><span class="co">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb3-11" data-line-number="11"><span class="co">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb3-12" data-line-number="12"><span class="co">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb3-13" data-line-number="13"><span class="co">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14"><span class="co">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb3-15" data-line-number="15"><span class="co">                        bc --  Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb3-16" data-line-number="16"><span class="co">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb3-17" data-line-number="17"><span class="co">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb3-18" data-line-number="18"><span class="co">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></a>
<a class="sourceLine" id="cb3-19" data-line-number="19"><span class="co">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></a>
<a class="sourceLine" id="cb3-20" data-line-number="20"></a>
<a class="sourceLine" id="cb3-21" data-line-number="21"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb3-22" data-line-number="22"><span class="co">    a_next -- next hidden state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb3-23" data-line-number="23"><span class="co">    c_next -- next memory state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb3-24" data-line-number="24"><span class="co">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span></a>
<a class="sourceLine" id="cb3-25" data-line-number="25"><span class="co">    cache -- tuple of values needed for the Backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></a>
<a class="sourceLine" id="cb3-26" data-line-number="26"></a>
<a class="sourceLine" id="cb3-27" data-line-number="27"><span class="co">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></a>
<a class="sourceLine" id="cb3-28" data-line-number="28"><span class="co">          c stands for the memory value</span></a>
<a class="sourceLine" id="cb3-29" data-line-number="29"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb3-30" data-line-number="30">    <span class="cf">return</span> a_next, c_next, yt_pred, cache</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-21-24.png" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">def</span> lstm_forward(x, a0, parameters):</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="co">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6"><span class="co">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7"><span class="co">    a0 -- Initial hidden state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8"><span class="co">    parameters -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"><span class="co">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb4-10" data-line-number="10"><span class="co">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11"><span class="co">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb4-12" data-line-number="12"><span class="co">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13"><span class="co">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14"><span class="co">                        bc -- Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb4-15" data-line-number="15"><span class="co">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb4-16" data-line-number="16"><span class="co">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb4-17" data-line-number="17"><span class="co">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></a>
<a class="sourceLine" id="cb4-18" data-line-number="18"><span class="co">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></a>
<a class="sourceLine" id="cb4-19" data-line-number="19"></a>
<a class="sourceLine" id="cb4-20" data-line-number="20"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb4-21" data-line-number="21"><span class="co">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></a>
<a class="sourceLine" id="cb4-22" data-line-number="22"><span class="co">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></a>
<a class="sourceLine" id="cb4-23" data-line-number="23"><span class="co">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></a>
<a class="sourceLine" id="cb4-24" data-line-number="24"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-25" data-line-number="25">    <span class="cf">return</span> a, y, c, caches</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-39-25.png" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">def</span> rnn_cell_backward(da_next, cache):</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"><span class="co">    Implements the backward pass for the RNN-cell (single time-step).</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"></a>
<a class="sourceLine" id="cb5-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6"><span class="co">    da_next -- Gradient of loss with respect to next hidden state</span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7"><span class="co">    cache -- python dictionary containing useful values (output of rnn_cell_forward())</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8"></a>
<a class="sourceLine" id="cb5-9" data-line-number="9"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10"><span class="co">    gradients -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb5-11" data-line-number="11"><span class="co">                        dx -- Gradients of input data, of shape (n_x, m)</span></a>
<a class="sourceLine" id="cb5-12" data-line-number="12"><span class="co">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb5-13" data-line-number="13"><span class="co">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></a>
<a class="sourceLine" id="cb5-14" data-line-number="14"><span class="co">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></a>
<a class="sourceLine" id="cb5-15" data-line-number="15"><span class="co">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb5-16" data-line-number="16"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb5-17" data-line-number="17">    <span class="cf">return</span> gradients</a></code></pre></div>
<ul>
<li><code>da_next</code> is <code>dJ/da</code>, and partial derivatives like <code>da/dx</code> must be multiplied to this.</li>
<li>However, this derivatives may have different dimensions.</li>
<li>So, <code>dJ/da</code> must be multiplied to <code>da</code> part, before completely calculate <code>da/d&lt;input&gt;</code>.</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_15-26-25.png" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">def</span> rnn_backward(da, caches):</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="co">    Implement the backward pass for a RNN over an entire sequence of input data.</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4"></a>
<a class="sourceLine" id="cb6-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb6-6" data-line-number="6"><span class="co">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></a>
<a class="sourceLine" id="cb6-7" data-line-number="7"><span class="co">    caches -- tuple containing information from the forward pass (rnn_forward)</span></a>
<a class="sourceLine" id="cb6-8" data-line-number="8"></a>
<a class="sourceLine" id="cb6-9" data-line-number="9"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb6-10" data-line-number="10"><span class="co">    gradients -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb6-11" data-line-number="11"><span class="co">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></a>
<a class="sourceLine" id="cb6-12" data-line-number="12"><span class="co">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb6-13" data-line-number="13"><span class="co">                        dWax -- Gradient w.r.t the input&#39;s weight matrix, numpy-array of shape (n_a, n_x)</span></a>
<a class="sourceLine" id="cb6-14" data-line-number="14"><span class="co">                        dWaa -- Gradient w.r.t the hidden state&#39;s weight matrix, numpy-arrayof shape (n_a, n_a)</span></a>
<a class="sourceLine" id="cb6-15" data-line-number="15"><span class="co">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb6-16" data-line-number="16"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb6-17" data-line-number="17">    <span class="cf">return</span> gradients</a></code></pre></div>
<ul>
<li><code>da</code> provides upstream gradients, which stem from <code>y_t</code></li>
<li>So <code>da</code> is added by <code>da_prev</code> gradients from <code>da_next</code></li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_16-04-14.png" /></p>
<ul>
<li>The equations above are incorrect; Following equations are correct:</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_17-40-23.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_16-04-32.png" /></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="kw">def</span> lstm_cell_backward(da_next, dc_next, cache):</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="co">    Implement the backward pass for the LSTM-cell (single time-step).</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb7-6" data-line-number="6"><span class="co">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb7-7" data-line-number="7"><span class="co">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb7-8" data-line-number="8"><span class="co">    cache -- cache storing information from the forward pass</span></a>
<a class="sourceLine" id="cb7-9" data-line-number="9"></a>
<a class="sourceLine" id="cb7-10" data-line-number="10"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb7-11" data-line-number="11"><span class="co">    gradients -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb7-12" data-line-number="12"><span class="co">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span></a>
<a class="sourceLine" id="cb7-13" data-line-number="13"><span class="co">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb7-14" data-line-number="14"><span class="co">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span></a>
<a class="sourceLine" id="cb7-15" data-line-number="15"><span class="co">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb7-16" data-line-number="16"><span class="co">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb7-17" data-line-number="17"><span class="co">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb7-18" data-line-number="18"><span class="co">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb7-19" data-line-number="19"><span class="co">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb7-20" data-line-number="20"><span class="co">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb7-21" data-line-number="21"><span class="co">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb7-22" data-line-number="22"><span class="co">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb7-23" data-line-number="23"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb7-24" data-line-number="24">    <span class="cf">return</span> gradients</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_17-45-59.png" /></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">def</span> lstm_backward(da, caches):</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="co">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"></a>
<a class="sourceLine" id="cb8-6" data-line-number="6"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb8-7" data-line-number="7"><span class="co">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="co">    caches -- cache storing information from the forward pass (lstm_forward)</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9"></a>
<a class="sourceLine" id="cb8-10" data-line-number="10"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb8-11" data-line-number="11"><span class="co">    gradients -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb8-12" data-line-number="12"><span class="co">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span></a>
<a class="sourceLine" id="cb8-13" data-line-number="13"><span class="co">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></a>
<a class="sourceLine" id="cb8-14" data-line-number="14"><span class="co">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb8-15" data-line-number="15"><span class="co">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb8-16" data-line-number="16"><span class="co">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb8-17" data-line-number="17"><span class="co">                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></a>
<a class="sourceLine" id="cb8-18" data-line-number="18"><span class="co">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb8-19" data-line-number="19"><span class="co">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb8-20" data-line-number="20"><span class="co">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb8-21" data-line-number="21"><span class="co">                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb8-22" data-line-number="22"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-23" data-line-number="23">    <span class="cf">return</span> gradients</a></code></pre></div>
<ul>
<li>Failed to produce the expected output.</li>
<li>According There must be some errors in this problem. So I just passed this.</li>
</ul>
<h2 id="dinosaur-island---character-level-language-modeling">Dinosaur Island - Character-Level Language Modeling</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-34-36.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-37-26.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-37-49.png" /></p>
<pre class="example"><code>Use numpy.clip()
</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">def</span> clip(gradients, maxValue):</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">    <span class="co">&#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb10-3" data-line-number="3"><span class="co">    Clips the gradients&#39; values between minimum and maximum.</span></a>
<a class="sourceLine" id="cb10-4" data-line-number="4"></a>
<a class="sourceLine" id="cb10-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb10-6" data-line-number="6"><span class="co">    gradients -- a dictionary containing the gradients &quot;dWaa&quot;, &quot;dWax&quot;, &quot;dWya&quot;, &quot;db&quot;, &quot;dby&quot;</span></a>
<a class="sourceLine" id="cb10-7" data-line-number="7"><span class="co">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span></a>
<a class="sourceLine" id="cb10-8" data-line-number="8"></a>
<a class="sourceLine" id="cb10-9" data-line-number="9"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb10-10" data-line-number="10"><span class="co">    gradients -- a dictionary with the clipped gradients.</span></a>
<a class="sourceLine" id="cb10-11" data-line-number="11"><span class="co">    &#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb10-12" data-line-number="12">    <span class="cf">return</span> gardients</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-45-33.png" /></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">def</span> sample(parameters, char_to_ix, seed):</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3"><span class="co">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span></a>
<a class="sourceLine" id="cb11-4" data-line-number="4"></a>
<a class="sourceLine" id="cb11-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb11-6" data-line-number="6"><span class="co">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span></a>
<a class="sourceLine" id="cb11-7" data-line-number="7"><span class="co">    char_to_ix -- python dictionary mapping each character to an index.</span></a>
<a class="sourceLine" id="cb11-8" data-line-number="8"><span class="co">    seed -- used for grading purposes. Do not worry about it.</span></a>
<a class="sourceLine" id="cb11-9" data-line-number="9"></a>
<a class="sourceLine" id="cb11-10" data-line-number="10"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb11-11" data-line-number="11"><span class="co">    indices -- a list of length n containing the indices of the sampled characters.</span></a>
<a class="sourceLine" id="cb11-12" data-line-number="12"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb11-13" data-line-number="13">    <span class="cf">return</span> indices</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_07-59-48.png" /></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">def</span> optimize(X, Y, a_prev, parameters, learning_rate <span class="op">=</span> <span class="fl">0.01</span>):</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3"><span class="co">    Execute one step of the optimization to train the model.</span></a>
<a class="sourceLine" id="cb12-4" data-line-number="4"></a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb12-6" data-line-number="6"><span class="co">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span></a>
<a class="sourceLine" id="cb12-7" data-line-number="7"><span class="co">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span></a>
<a class="sourceLine" id="cb12-8" data-line-number="8"><span class="co">    a_prev -- previous hidden state.</span></a>
<a class="sourceLine" id="cb12-9" data-line-number="9"><span class="co">    parameters -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb12-10" data-line-number="10"><span class="co">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></a>
<a class="sourceLine" id="cb12-11" data-line-number="11"><span class="co">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></a>
<a class="sourceLine" id="cb12-12" data-line-number="12"><span class="co">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></a>
<a class="sourceLine" id="cb12-13" data-line-number="13"><span class="co">                        b --  Bias, numpy array of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb12-14" data-line-number="14"><span class="co">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></a>
<a class="sourceLine" id="cb12-15" data-line-number="15"><span class="co">    learning_rate -- learning rate for the model.</span></a>
<a class="sourceLine" id="cb12-16" data-line-number="16"></a>
<a class="sourceLine" id="cb12-17" data-line-number="17"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb12-18" data-line-number="18"><span class="co">    loss -- value of the loss function (cross-entropy)</span></a>
<a class="sourceLine" id="cb12-19" data-line-number="19"><span class="co">    gradients -- python dictionary containing:</span></a>
<a class="sourceLine" id="cb12-20" data-line-number="20"><span class="co">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></a>
<a class="sourceLine" id="cb12-21" data-line-number="21"><span class="co">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></a>
<a class="sourceLine" id="cb12-22" data-line-number="22"><span class="co">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span></a>
<a class="sourceLine" id="cb12-23" data-line-number="23"><span class="co">                        db -- Gradients of bias vector, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb12-24" data-line-number="24"><span class="co">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span></a>
<a class="sourceLine" id="cb12-25" data-line-number="25"><span class="co">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></a>
<a class="sourceLine" id="cb12-26" data-line-number="26"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb12-27" data-line-number="27">    <span class="cf">return</span> loss, gradients, a[<span class="bu">len</span>(X)<span class="op">-</span><span class="dv">1</span>]</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-14-43.png" /></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">def</span> model(data, ix_to_char, char_to_ix, num_iterations <span class="op">=</span> <span class="dv">35000</span>, n_a <span class="op">=</span> <span class="dv">50</span>, dino_names <span class="op">=</span> <span class="dv">7</span>, vocab_size <span class="op">=</span> <span class="dv">27</span>):</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3"><span class="co">    Trains the model and generates dinosaur names.</span></a>
<a class="sourceLine" id="cb13-4" data-line-number="4"></a>
<a class="sourceLine" id="cb13-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb13-6" data-line-number="6"><span class="co">    data -- text corpus</span></a>
<a class="sourceLine" id="cb13-7" data-line-number="7"><span class="co">    ix_to_char -- dictionary that maps the index to a character</span></a>
<a class="sourceLine" id="cb13-8" data-line-number="8"><span class="co">    char_to_ix -- dictionary that maps a character to an index</span></a>
<a class="sourceLine" id="cb13-9" data-line-number="9"><span class="co">    num_iterations -- number of iterations to train the model for</span></a>
<a class="sourceLine" id="cb13-10" data-line-number="10"><span class="co">    n_a -- number of units of the RNN cell</span></a>
<a class="sourceLine" id="cb13-11" data-line-number="11"><span class="co">    dino_names -- number of dinosaur names you want to sample at each iteration.</span></a>
<a class="sourceLine" id="cb13-12" data-line-number="12"><span class="co">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span></a>
<a class="sourceLine" id="cb13-13" data-line-number="13"></a>
<a class="sourceLine" id="cb13-14" data-line-number="14"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb13-15" data-line-number="15"><span class="co">    parameters -- learned parameters</span></a>
<a class="sourceLine" id="cb13-16" data-line-number="16"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb13-17" data-line-number="17">    <span class="cf">return</span> parameters</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-20-14.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-21-18.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-22-35.png" /></p>
<h2 id="jazz-improvisation-with-lstm">Jazz improvisation with LSTM</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-31-17.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-32-05.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-32-29.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-35-18.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-36-37.png" /></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="kw">def</span> djmodel(Tx, n_a, n_values):</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3"><span class="co">    Implement the model</span></a>
<a class="sourceLine" id="cb14-4" data-line-number="4"></a>
<a class="sourceLine" id="cb14-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb14-6" data-line-number="6"><span class="co">    Tx -- length of the sequence in a corpus</span></a>
<a class="sourceLine" id="cb14-7" data-line-number="7"><span class="co">    n_a -- the number of activations used in our model</span></a>
<a class="sourceLine" id="cb14-8" data-line-number="8"><span class="co">    n_values -- number of unique values in the music data</span></a>
<a class="sourceLine" id="cb14-9" data-line-number="9"></a>
<a class="sourceLine" id="cb14-10" data-line-number="10"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb14-11" data-line-number="11"><span class="co">    model -- a keras model with the</span></a>
<a class="sourceLine" id="cb14-12" data-line-number="12"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb14-13" data-line-number="13">    <span class="cf">return</span> model</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-50-45.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-51-43.png" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">def</span> music_inference_model(LSTM_cell, densor, n_values <span class="op">=</span> <span class="dv">78</span>, n_a <span class="op">=</span> <span class="dv">64</span>, Ty <span class="op">=</span> <span class="dv">100</span>):</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-3" data-line-number="3"><span class="co">    Uses the trained &quot;LSTM_cell&quot; and &quot;densor&quot; from model() to generate a sequence of values.</span></a>
<a class="sourceLine" id="cb15-4" data-line-number="4"></a>
<a class="sourceLine" id="cb15-5" data-line-number="5"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb15-6" data-line-number="6"><span class="co">    LSTM_cell -- the trained &quot;LSTM_cell&quot; from model(), Keras layer object</span></a>
<a class="sourceLine" id="cb15-7" data-line-number="7"><span class="co">    densor -- the trained &quot;densor&quot; from model(), Keras layer object</span></a>
<a class="sourceLine" id="cb15-8" data-line-number="8"><span class="co">    n_values -- integer, umber of unique values</span></a>
<a class="sourceLine" id="cb15-9" data-line-number="9"><span class="co">    n_a -- number of units in the LSTM_cell</span></a>
<a class="sourceLine" id="cb15-10" data-line-number="10"><span class="co">    Ty -- integer, number of time steps to generate</span></a>
<a class="sourceLine" id="cb15-11" data-line-number="11"></a>
<a class="sourceLine" id="cb15-12" data-line-number="12"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb15-13" data-line-number="13"><span class="co">    inference_model -- Keras model instance</span></a>
<a class="sourceLine" id="cb15-14" data-line-number="14"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-15" data-line-number="15">    <span class="cf">return</span> inference_model</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_08-56-01.png" /></p>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html" class="uri">https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html</a></li>
<li><a href="https://keras.io/utils/#to_categorical" class="uri">https://keras.io/utils/#to_categorical</a></li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># GRADED FUNCTION: predict_and_sample</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"></a>
<a class="sourceLine" id="cb16-3" data-line-number="3"><span class="kw">def</span> predict_and_sample(inference_model, x_initializer <span class="op">=</span> x_initializer, a_initializer <span class="op">=</span> a_initializer,</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">                       c_initializer <span class="op">=</span> c_initializer):</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb16-6" data-line-number="6"><span class="co">    Predicts the next value of values using the inference model.</span></a>
<a class="sourceLine" id="cb16-7" data-line-number="7"></a>
<a class="sourceLine" id="cb16-8" data-line-number="8"><span class="co">    Arguments:</span></a>
<a class="sourceLine" id="cb16-9" data-line-number="9"><span class="co">    inference_model -- Keras model instance for inference time</span></a>
<a class="sourceLine" id="cb16-10" data-line-number="10"><span class="co">    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation</span></a>
<a class="sourceLine" id="cb16-11" data-line-number="11"><span class="co">    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell</span></a>
<a class="sourceLine" id="cb16-12" data-line-number="12"><span class="co">    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel</span></a>
<a class="sourceLine" id="cb16-13" data-line-number="13"></a>
<a class="sourceLine" id="cb16-14" data-line-number="14"><span class="co">    Returns:</span></a>
<a class="sourceLine" id="cb16-15" data-line-number="15"><span class="co">    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated</span></a>
<a class="sourceLine" id="cb16-16" data-line-number="16"><span class="co">    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated</span></a>
<a class="sourceLine" id="cb16-17" data-line-number="17"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb16-18" data-line-number="18"></a>
<a class="sourceLine" id="cb16-19" data-line-number="19">    <span class="co">### START CODE HERE </span><span class="al">###</span></a>
<a class="sourceLine" id="cb16-20" data-line-number="20">    <span class="co"># Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.</span></a>
<a class="sourceLine" id="cb16-21" data-line-number="21">    pred <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb16-22" data-line-number="22">    <span class="co"># Step 2: Convert &quot;pred&quot; into an np.array() of indices with the maximum probabilities</span></a>
<a class="sourceLine" id="cb16-23" data-line-number="23">    indices <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb16-24" data-line-number="24">    <span class="co"># Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )</span></a>
<a class="sourceLine" id="cb16-25" data-line-number="25">    results <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb16-26" data-line-number="26">    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></a>
<a class="sourceLine" id="cb16-27" data-line-number="27"></a>
<a class="sourceLine" id="cb16-28" data-line-number="28">    <span class="cf">return</span> results, indices</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/nlp-sequence-models/week1/_img/screenshot_2018-02-07_09-20-49.png" /></p>
</article>

</body>
</html>
