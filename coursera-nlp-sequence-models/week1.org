#+TITLE: Recurrent Neural Networks

* Table of Contents :TOC_3_gh:
- [[#recurrent-neural-networks][Recurrent Neural Networks]]
  - [[#why-sequence-models][Why sequence models]]
  - [[#notation][Notation]]
  - [[#recurrent-neural-network-model][Recurrent Neural Network Model]]
  - [[#backpropagation-through-time][Backpropagation through time]]
  - [[#different-types-of-rnns][Different types of RNNs]]
  - [[#language-model-and-sequence-generation][Language model and sequence generation]]
  - [[#sampling-novel-sequences][Sampling novel sequences]]
  - [[#vanishing-gradients-with-rnns][Vanishing gradients with RNNs]]
  - [[#gated-recurrent-unit-gru][Gated Recurrent Unit (GRU)]]
  - [[#long-short-term-memory-lstm][Long Short Term Memory (LSTM)]]
  - [[#bidirectional-rnn][Bidirectional RNN]]
  - [[#deep-rnns][Deep RNNs]]
- [[#programming-assignments][Programming assignments]]
  - [[#building-a-recurrent-neural-network---step-by-step][Building a recurrent neural network - step by step]]
  - [[#dinosaur-island---character-level-language-modeling][Dinosaur Island - Character-Level Language Modeling]]
  - [[#jazz-improvisation-with-lstm][Jazz improvisation with LSTM]]

* Recurrent Neural Networks
** Why sequence models
[[file:img/screenshot_2018-02-06_13-59-38.png]]

** Notation
[[file:img/screenshot_2018-02-06_14-14-07.png]]

[[file:img/screenshot_2018-02-06_14-13-41.png]]

** Recurrent Neural Network Model
[[file:img/screenshot_2018-02-06_15-52-01.png]]

[[file:img/screenshot_2018-02-06_16-45-24.png]]

[[file:img/screenshot_2018-02-06_16-49-19.png]]

[[file:img/screenshot_2018-02-06_16-53-06.png]]

** Backpropagation through time
[[file:img/screenshot_2018-02-06_17-00-58.png]]

** Different types of RNNs
[[file:img/screenshot_2018-02-06_17-06-16.png]]

[[file:img/screenshot_2018-02-06_17-10-05.png]]

[[file:img/screenshot_2018-02-06_17-11-20.png]]
** Language model and sequence generation
[[file:img/screenshot_2018-02-06_17-15-34.png]]

[[file:img/screenshot_2018-02-06_17-18-43.png]]

[[file:img/screenshot_2018-02-06_17-24-33.png]]
** Sampling novel sequences
[[file:img/screenshot_2018-02-06_17-37-57.png]]

[[file:img/screenshot_2018-02-06_17-41-05.png]]

- Character-level language model needs longer sequences,
  which means that it's computationally expensive.
- So it is used in more specialized cases.

[[file:img/screenshot_2018-02-06_17-43-12.png]]
** Vanishing gradients with RNNs
[[file:img/screenshot_2018-02-06_17-50-15.png]]
** Gated Recurrent Unit (GRU)
[[file:img/screenshot_2018-02-06_17-53-28.png]]

[[file:img/screenshot_2018-02-06_18-06-17.png]]

[[file:img/screenshot_2018-02-06_18-08-57.png]]
** Long Short Term Memory (LSTM)
[[file:img/screenshot_2018-02-06_18-26-55.png]]

- GRU is simpler, LSTM is more powerful
- GRU or LSTM: There is no generally better algorithm between these.
- But historically, LSTM has been more commonly used.

** Bidirectional RNN
[[file:img/screenshot_2018-02-06_18-38-33.png]]
- BRNN has a disadvantage that it need the entire sequence to make predictions.

** Deep RNNs
[[file:img/screenshot_2018-02-06_18-45-48.png]]
* Programming assignments
** Building a recurrent neural network - step by step
[[file:img/screenshot_2018-02-07_06-40-18.png]]

[[file:img/screenshot_2018-02-07_06-42-11.png]]


[[file:img/screenshot_2018-02-07_06-42-56.png]]

#+BEGIN_SRC python
  def rnn_cell_forward(xt, a_prev, parameters):
      """
      Implements a single forward step of the RNN-cell as described in Figure (2)

      Arguments:
      xt -- your input data at timestep "t", numpy array of shape (n_x, m).
      a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)
      parameters -- python dictionary containing:
                          Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                          Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                          Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          ba --  Bias, numpy array of shape (n_a, 1)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
      Returns:
      a_next -- next hidden state, of shape (n_a, m)
      yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)
      cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)
      """
      return a_next, yt_pred, cache
#+END_SRC

[[file:img/screenshot_2018-02-07_06-53-24.png]]

#+BEGIN_SRC python
  def rnn_forward(x, a0, parameters):
      """
      Implement the forward propagation of the recurrent neural network described in Figure (3).

      Arguments:
      x -- Input data for every time-step, of shape (n_x, m, T_x).
      a0 -- Initial hidden state, of shape (n_a, m)
      parameters -- python dictionary containing:
                          Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                          Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                          Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          ba --  Bias numpy array of shape (n_a, 1)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

      Returns:
      a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)
      y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
      caches -- tuple of values needed for the backward pass, contains (list of caches, x)
      """
      return a, y_pred, caches
#+END_SRC

[[file:img/screenshot_2018-02-07_07-01-53.png]]

[[file:img/screenshot_2018-02-07_07-02-08.png]]

[[file:img/screenshot_2018-02-07_07-02-24.png]]

#+BEGIN_SRC python
  def lstm_cell_forward(xt, a_prev, c_prev, parameters):
      """
      Implement a single forward step of the LSTM-cell as described in Figure (4)

      Arguments:
      xt -- your input data at timestep "t", numpy array of shape (n_x, m).
      a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)
      c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)
      parameters -- python dictionary containing:
                          Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
                          bf -- Bias of the forget gate, numpy array of shape (n_a, 1)
                          Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)
                          bi -- Bias of the update gate, numpy array of shape (n_a, 1)
                          Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)
                          bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)
                          Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)
                          bo --  Bias of the output gate, numpy array of shape (n_a, 1)
                          Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

      Returns:
      a_next -- next hidden state, of shape (n_a, m)
      c_next -- next memory state, of shape (n_a, m)
      yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)
      cache -- tuple of values needed for the Backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)

      Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),
            c stands for the memory value
      """
      return a_next, c_next, yt_pred, cache
#+END_SRC

[[file:img/screenshot_2018-02-07_07-21-24.png]]

#+BEGIN_SRC python
  def lstm_forward(x, a0, parameters):
      """
      Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).

      Arguments:
      x -- Input data for every time-step, of shape (n_x, m, T_x).
      a0 -- Initial hidden state, of shape (n_a, m)
      parameters -- python dictionary containing:
                          Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
                          bf -- Bias of the forget gate, numpy array of shape (n_a, 1)
                          Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)
                          bi -- Bias of the update gate, numpy array of shape (n_a, 1)
                          Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)
                          bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)
                          Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)
                          bo -- Bias of the output gate, numpy array of shape (n_a, 1)
                          Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

      Returns:
      a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)
      y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
      caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)
      """
      return a, y, c, caches
#+END_SRC

[[file:img/screenshot_2018-02-07_07-39-25.png]]



** Dinosaur Island - Character-Level Language Modeling
[[file:img/screenshot_2018-02-07_07-34-36.png]]

[[file:img/screenshot_2018-02-07_07-37-26.png]]

[[file:img/screenshot_2018-02-07_07-37-49.png]]

: Use numpy.clip()

#+BEGIN_SRC python
  def clip(gradients, maxValue):
      '''
      Clips the gradients' values between minimum and maximum.

      Arguments:
      gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby"
      maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue

      Returns:
      gradients -- a dictionary with the clipped gradients.
      '''
      return gardients
#+END_SRC

[[file:img/screenshot_2018-02-07_07-45-33.png]]

#+BEGIN_SRC python
  def sample(parameters, char_to_ix, seed):
      """
      Sample a sequence of characters according to a sequence of probability distributions output of the RNN

      Arguments:
      parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. 
      char_to_ix -- python dictionary mapping each character to an index.
      seed -- used for grading purposes. Do not worry about it.

      Returns:
      indices -- a list of length n containing the indices of the sampled characters.
      """
      return indices
#+END_SRC

[[file:img/screenshot_2018-02-07_07-59-48.png]]

#+BEGIN_SRC python
  def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):
      """
      Execute one step of the optimization to train the model.

      Arguments:
      X -- list of integers, where each integer is a number that maps to a character in the vocabulary.
      Y -- list of integers, exactly the same as X but shifted one index to the left.
      a_prev -- previous hidden state.
      parameters -- python dictionary containing:
                          Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                          Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                          Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          b --  Bias, numpy array of shape (n_a, 1)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
      learning_rate -- learning rate for the model.

      Returns:
      loss -- value of the loss function (cross-entropy)
      gradients -- python dictionary containing:
                          dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)
                          dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)
                          dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)
                          db -- Gradients of bias vector, of shape (n_a, 1)
                          dby -- Gradients of output bias vector, of shape (n_y, 1)
      a[len(X)-1] -- the last hidden state, of shape (n_a, 1)
      """
      return loss, gradients, a[len(X)-1]
#+END_SRC

[[file:img/screenshot_2018-02-07_08-14-43.png]]

#+BEGIN_SRC python
  def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):
      """
      Trains the model and generates dinosaur names.

      Arguments:
      data -- text corpus
      ix_to_char -- dictionary that maps the index to a character
      char_to_ix -- dictionary that maps a character to an index
      num_iterations -- number of iterations to train the model for
      n_a -- number of units of the RNN cell
      dino_names -- number of dinosaur names you want to sample at each iteration.
      vocab_size -- number of unique characters found in the text, size of the vocabulary

      Returns:
      parameters -- learned parameters
      """
      return parameters
#+END_SRC

[[file:img/screenshot_2018-02-07_08-20-14.png]]

[[file:img/screenshot_2018-02-07_08-21-18.png]]

[[file:img/screenshot_2018-02-07_08-22-35.png]]
** Jazz improvisation with LSTM
