#+TITLE: Sequence models & Attention mechanism

* Table of Contents :TOC_3_gh:
- [[#various-sequence-to-sequence-architectures][Various sequence to sequence architectures]]
  - [[#basic-models][Basic Models]]
  - [[#picking-the-most-likely-sentence][Picking the most likely sentence]]
  - [[#beam-search][Beam Search]]
  - [[#refinements-to-beam-search][Refinements to Beam Search]]
  - [[#error-analysis-in-beam-search][Error analysis in beam search]]
  - [[#bleu-score-optional][Bleu Score (optional)]]
  - [[#attention-model-intuition][Attention Model Intuition]]
  - [[#attention-model][Attention Model]]
- [[#speech-recognition---audio-data][Speech recognition - Audio data]]
  - [[#speech-recognition][Speech recognition]]
  - [[#trigger-word-detection][Trigger Word Detection]]
- [[#conclusion][Conclusion]]
  - [[#conclusion-and-thank-you][Conclusion and thank you]]
- [[#practice-questions][Practice questions]]
  - [[#quiz-sequence-models--attention-mechanism][Quiz: Sequence models & Attention mechanism]]
- [[#programming-assignments][Programming assignments]]
  - [[#neural-machine-translation][Neural Machine Translation]]
  - [[#trigger-word-detection-1][Trigger word detection]]

* Various sequence to sequence architectures
** Basic Models
[[file:img/screenshot_2018-02-13_10-12-12.png]]

[[file:img/screenshot_2018-02-13_10-14-44.png]]

** Picking the most likely sentence
[[file:img/screenshot_2018-02-13_10-18-37.png]]

[[file:img/screenshot_2018-02-13_10-20-21.png]]

Unlike the language model previously introduced, in machine translation,
pick most likely ~y~ instead of sampling for consistent results.

[[file:img/screenshot_2018-02-13_10-26-51.png]]

Don't use greedy approach, otherwise ~Jane is going~, which is less succinct in the above example, will always be chosen.

** Beam Search
[[file:img/screenshot_2018-02-13_10-33-49.png]]

[[file:img/screenshot_2018-02-13_10-41-03.png]]

In the second step, pick the top 3 again among 30,000 possibilities.
Because Beam width is 3, every step needs 3 models.

[[file:img/screenshot_2018-02-13_10-44-05.png]]

** Refinements to Beam Search
[[file:img/screenshot_2018-02-14_08-47-01.png]]

The actual probability is too small to practically calculate.
So take log to the probabilities

[[file:img/screenshot_2018-02-14_08-50-10.png]]

** Error analysis in beam search
[[file:img/screenshot_2018-02-14_08-56-15.png]]

[[file:img/screenshot_2018-02-14_08-59-11.png]]

[[file:img/screenshot_2018-02-14_09-01-28.png]]

** Bleu Score (optional)
[[file:img/screenshot_2018-02-14_09-08-08.png]]

[[file:img/screenshot_2018-02-14_09-11-13.png]]

[[file:img/screenshot_2018-02-14_09-15-12.png]]

[[file:img/screenshot_2018-02-14_09-17-49.png]]
** Attention Model Intuition
[[file:img/screenshot_2018-02-14_09-25-35.png]]

[[file:img/screenshot_2018-02-14_09-37-13.png]]

** Attention Model
[[file:img/screenshot_2018-02-14_09-43-51.png]]

[[file:img/screenshot_2018-02-14_09-50-54.png]]

- Essentially, ~a^<t, t'>~ is a softmax function

[[file:img/screenshot_2018-02-14_09-52-26.png]]

* Speech recognition - Audio data
** Speech recognition
[[file:img/screenshot_2018-02-14_19-59-57.png]]

[[file:img/screenshot_2018-02-14_20-32-30.png]]

[[file:img/screenshot_2018-02-14_20-36-03.png]]

** Trigger Word Detection
[[file:img/screenshot_2018-02-14_20-53-46.png]]

* Conclusion
** Conclusion and thank you
[[file:img/screenshot_2018-02-14_20-55-38.png]]

[[file:img/screenshot_2018-02-14_20-57-36.png]]

* Practice questions
** Quiz: Sequence models & Attention mechanism

[[file:img/screenshot_2018-02-14_21-16-26.png]]

* Programming assignments
** Neural Machine Translation
[[file:img/screenshot_2018-02-14_21-26-34.png]]

[[file:img/screenshot_2018-02-14_21-27-10.png]]

[[file:img/screenshot_2018-02-14_21-29-44.png]]

[[file:img/screenshot_2018-02-14_21-30-24.png]]

[[file:img/screenshot_2018-02-14_21-31-05.png]]

[[file:img/screenshot_2018-02-14_21-31-39.png]]

[[file:img/screenshot_2018-02-14_21-37-20.png]]

- https://keras.io/layers/core/#repeatvector
- https://keras.io/layers/merge/#concatenate

#+BEGIN_SRC python
  # GRADED FUNCTION: one_step_attention

  def one_step_attention(a, s_prev):
      """
      Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights
      "alphas" and the hidden states "a" of the Bi-LSTM.

      Arguments:
      a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)
      s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)

      Returns:
      context -- context vector, input of the next (post-attetion) LSTM cell
      """

      ### START CODE HERE ###
      # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line)
      s_prev = None
      # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)
      concat = None
      # Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines)
      e = None
      # Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines)
      energies = None
      # Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line)
      alphas = None
      # Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)
      context = None
      ### END CODE HERE ###

      return context
#+END_SRC

[[file:img/screenshot_2018-02-14_21-45-46.png]]

- https://keras.io/layers/wrappers/#bidirectional
- https://keras.io/layers/recurrent/#lstm
- The first parameter of ~LSTM~ is the output size

To produce expected output, modify the cell above as follows:

[[file:img/screenshot_2018-02-14_22-19-03.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: model

  def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):
      """
      Arguments:
      Tx -- length of the input sequence
      Ty -- length of the output sequence
      n_a -- hidden state size of the Bi-LSTM
      n_s -- hidden state size of the post-attention LSTM
      human_vocab_size -- size of the python dictionary "human_vocab"
      machine_vocab_size -- size of the python dictionary "machine_vocab"

      Returns:
      model -- Keras model instance
      """

      # Define the inputs of your model with a shape (Tx,)
      # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)
      X = Input(shape=(Tx, human_vocab_size))
      s0 = Input(shape=(n_s,), name='s0')
      c0 = Input(shape=(n_s,), name='c0')
      s = s0
      c = c0

      # Initialize empty list of outputs
      outputs = []

      ### START CODE HERE ###

      # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)
      a = None

      # Step 2: Iterate for Ty steps
      for t in range(None):

          # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)
          context = None

          # Step 2.B: Apply the post-attention LSTM cell to the "context" vector.
          # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)
          s, _, c = None

          # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)
          out = None

          # Step 2.D: Append "out" to the "outputs" list (≈ 1 line)
          None

      # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)
      model = None

      ### END CODE HERE ###

      return model
#+END_SRC

[[file:img/screenshot_2018-02-14_22-32-59.png]]

- https://keras.io/optimizers/#adam
- https://keras.io/optimizers/#usage-of-optimizers
- https://keras.io/models/model/#compile

#+BEGIN_SRC python
  ### START CODE HERE ### (≈2 lines)
  opt = None
  None
  ### END CODE HERE ###
#+END_SRC

[[file:img/screenshot_2018-02-14_22-37-55.png]]

[[file:img/screenshot_2018-02-14_22-40-58.png]]

[[file:img/screenshot_2018-02-14_22-42-56.png]]

[[file:img/screenshot_2018-02-14_22-48-19.png]]

[[file:img/screenshot_2018-02-14_22-49-22.png]]

** Trigger word detection
[[file:img/screenshot_2018-02-14_22-51-28.png]]

[[file:img/screenshot_2018-02-14_22-53-44.png]]

[[file:img/screenshot_2018-02-14_22-55-14.png]]

[[file:img/screenshot_2018-02-14_22-56-08.png]]

[[file:img/screenshot_2018-02-14_22-58-16.png]]

[[file:img/screenshot_2018-02-14_23-00-56.png]]

[[file:img/screenshot_2018-02-14_23-01-12.png]]

[[file:img/screenshot_2018-02-14_23-04-42.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: is_overlapping

  def is_overlapping(segment_time, previous_segments):
      """
      Checks if the time of a segment overlaps with the times of existing segments.

      Arguments:
      segment_time -- a tuple of (segment_start, segment_end) for the new segment
      previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments

      Returns:
      True if the time segment overlaps with any of the existing segments, False otherwise
      """

      segment_start, segment_end = segment_time

      ### START CODE HERE ### (≈ 4 line)
      # Step 1: Initialize overlap as a "False" flag. (≈ 1 line)
      overlap = None

      # Step 2: loop over the previous_segments start and end times.
      # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)
      for previous_start, previous_end in previous_segments:
          if None:
              overlap = None
      ### END CODE HERE ###

      return overlap
#+END_SRC

[[file:img/screenshot_2018-02-14_23-07-34.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: insert_audio_clip

  def insert_audio_clip(background, audio_clip, previous_segments):
      """
      Insert a new audio segment over the background noise at a random time step, ensuring that the
      audio segment does not overlap with existing segments.

      Arguments:
      background -- a 10 second background audio recording.
      audio_clip -- the audio clip to be inserted/overlaid.
      previous_segments -- times where audio segments have already been placed

      Returns:
      new_background -- the updated background audio
      """

      # Get the duration of the audio clip in ms
      segment_ms = len(audio_clip)

      ### START CODE HERE ###
      # Step 1: Use one of the helper functions to pick a random time segment onto which to insert
      # the new audio clip. (≈ 1 line)
      segment_time = None

      # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep
      # picking new segment_time at random until it doesn't overlap. (≈ 2 lines)
      while None:
          segment_time = None

      # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)
      None
      ### END CODE HERE ###

      # Step 4: Superpose audio segment and background
      new_background = background.overlay(audio_clip, position = segment_time[0])

      return new_background, segment_time
#+END_SRC

[[file:img/screenshot_2018-02-14_23-10-37.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: insert_ones

  def insert_ones(y, segment_end_ms):
      """
      Update the label vector y. The labels of the 50 output steps strictly after the end of the segment
      should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the
      50 followinf labels should be ones.


      Arguments:
      y -- numpy array of shape (1, Ty), the labels of the training example
      segment_end_ms -- the end time of the segment in ms

      Returns:
      y -- updated labels
      """

      # duration of the background (in terms of spectrogram time-steps)
      segment_end_y = int(segment_end_ms * Ty / 10000.0)

      # Add 1 to the correct index in the background label (y)
      ### START CODE HERE ### (≈ 3 lines)
      for i in range(None, None):
          if None < None:
              y[0, i] = None
      ### END CODE HERE ###

      return y
#+END_SRC

[[file:img/screenshot_2018-02-14_23-15-35.png]]

[[file:img/screenshot_2018-02-14_23-37-15.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: create_training_example

  def create_training_example(background, activates, negatives):
      """
      Creates a training example with a given background, activates, and negatives.

      Arguments:
      background -- a 10 second background audio recording
      activates -- a list of audio segments of the word "activate"
      negatives -- a list of audio segments of random words that are not "activate"

      Returns:
      x -- the spectrogram of the training example
      y -- the label at each time step of the spectrogram
      """

      # Set the random seed
      np.random.seed(18)

      # Make background quieter
      background = background - 20

      ### START CODE HERE ###
      # Step 1: Initialize y (label vector) of zeros (≈ 1 line)
      y = None

      # Step 2: Initialize segment times as empty list (≈ 1 line)
      previous_segments = None
      ### END CODE HERE ###

      # Select 0-4 random "activate" audio clips from the entire list of "activates" recordings
      number_of_activates = np.random.randint(0, 5)
      random_indices = np.random.randint(len(activates), size=number_of_activates)
      random_activates = [activates[i] for i in random_indices]

      ### START CODE HERE ### (≈ 3 lines)
      # Step 3: Loop over randomly selected "activate" clips and insert in background
      for random_activate in random_activates:
          # Insert the audio clip on the background
          background, segment_time = None
          # Retrieve segment_start and segment_end from segment_time
          segment_start, segment_end = None
          # Insert labels in "y"
          y = None
      ### END CODE HERE ###

      # Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings
      number_of_negatives = np.random.randint(0, 3)
      random_indices = np.random.randint(len(negatives), size=number_of_negatives)
      random_negatives = [negatives[i] for i in random_indices]

      ### START CODE HERE ### (≈ 2 lines)
      # Step 4: Loop over randomly selected negative clips and insert in background
      for random_negative in random_negatives:
          # Insert the audio clip on the background
          background, _ = None
      ### END CODE HERE ###

      # Standardize the volume of the audio clip
      background = match_target_amplitude(background, -20.0)

      # Export new training example
      file_handle = background.export("train" + ".wav", format="wav")
      print("File (train.wav) was saved in your directory.")

      # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)
      x = graph_spectrogram("train.wav")

      return x, y
#+END_SRC

[[file:img/screenshot_2018-02-14_23-43-37.png]]

[[file:img/screenshot_2018-02-14_23-45-29.png]]

[[file:img/screenshot_2018-02-14_23-45-44.png]]

[[file:img/screenshot_2018-02-14_23-46-43.png]]

- https://keras.io/layers/convolutional/#conv1d
- https://keras.io/layers/normalization/
- https://keras.io/activations/
- https://keras.io/layers/core/#dropout
- https://keras.io/layers/recurrent/#gru

#+BEGIN_SRC python
  # GRADED FUNCTION: model

  def model(input_shape):
      """
      Function creating the model's graph in Keras.

      Argument:
      input_shape -- shape of the model's input data (using Keras conventions)

      Returns:
      model -- Keras model instance
      """

      X_input = Input(shape = input_shape)

      ### START CODE HERE ###

      # Step 1: CONV layer (≈4 lines)
      X = None                                 # CONV1D
      X = None                                 # Batch normalization
      X = None                                 # ReLu activation
      X = None                                 # dropout (use 0.8)

      # Step 2: First GRU Layer (≈4 lines)
      X = None                                 # GRU (use 128 units and return the sequences)
      X = None                                 # dropout (use 0.8)
      X = None                                 # Batch normalization

      # Step 3: Second GRU Layer (≈4 lines)
      X = None                                 # GRU (use 128 units and return the sequences)
      X = None                                 # dropout (use 0.8)
      X = None                                 # Batch normalization
      X = None                                 # dropout (use 0.8)

      # Step 4: Time-distributed dense layer (≈1 line)
      X = TimeDistributed(Dense(1, activation = "sigmoid"))(X) # time distributed  (sigmoid)

      ### END CODE HERE ###

      model = Model(inputs = X_input, outputs = X)

      return model
#+END_SRC

[[file:img/screenshot_2018-02-14_23-54-28.png]]

[[file:img/screenshot_2018-02-14_23-55-08.png]]

[[file:img/screenshot_2018-02-14_23-56-15.png]]

[[file:img/screenshot_2018-02-14_23-59-05.png]]

[[file:img/screenshot_2018-02-14_23-59-41.png]]
