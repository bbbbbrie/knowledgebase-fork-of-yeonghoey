#+TITLE: Practical aspects of Deep Learning

* Table of Contents :TOC_3_gh:
- [[#setting-up-your-machine-learning-application][Setting up your Machine Learning Application]]
  - [[#train--dev--test-sets][Train / Dev / Test sets]]
  - [[#bias--variance][Bias / Variance]]
  - [[#basic-recipe-for-machine-learning][Basic Recipe for Machine Learning]]
- [[#regularizing-your-neural-network][Regularizing your neural network]]
  - [[#regularization][Regularization]]
  - [[#why-regularization-reduces-overfitting][Why regularization reduces overfitting?]]

* Setting up your Machine Learning Application
** Train / Dev / Test sets

- https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo

[[file:img/screenshot_2017-10-14_09-22-34.png]]

Traditionally, 7:3 (train:dev) or 6:2:2 (train:dev:test), is a rule of thumb for data set.
But, in these days, big data came out, and there are a lot more data than past.
We can decrease dev and test data portion ratio, because the size of data set would just be enough in absolute perspective
with a big data set.

- Training Set :: This data set is used to adjust the weights on the neural network.

- Validation(Dev) Set ::
  This data set is used to minimize overfitting.
  If the accuracy over the training data set increases,
  but the accuracy over then validation data set stays the same or decreases,
  then you're overfitting your neural network and you should stop training.

- Testing Set :: This data set is used *only for testing the final solution in order to confirm the actual predictive power* of the network.

[[file:img/screenshot_2017-10-14_09-13-51.png]]

#+BEGIN_QUOTE
Make sure that the dev and test sets come from the same distribution. 
#+END_QUOTE
** Bias / Variance
[[file:img/screenshot_2017-10-14_09-33-46.png]]

- http://slideplayer.com/slide/8744653/
- [[file:img/screenshot_2017-10-14_09-42-42.png]]

[[file:img/screenshot_2017-10-14_09-44-20.png]]

** Basic Recipe for Machine Learning
[[file:img/screenshot_2017-10-14_09-52-29.png]]

* Regularizing your neural network
** Regularization
[[file:img/screenshot_2017-10-14_10-01-58.png]]

** Why regularization reduces overfitting?
[[file:img/screenshot_2017-10-14_10-08-39.png]]

[[file:img/screenshot_2017-10-14_10-15-16.png]]

- When we increase ~lambda~ to a huge number, ~W~ is going close to zero.
- ~W~ close to zeon means that the nework is going to be simpler.
- Which will alleviate the high variance problem.
