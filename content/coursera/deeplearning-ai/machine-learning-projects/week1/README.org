#+TITLE: ML Strategy (1)

* Introduction to ML Strategy
** Why ML Strategy
[[file:_img/screenshot_2017-10-30_08-45-11.png]]

** Orthogonalization
[[file:_img/screenshot_2017-10-30_08-50-48.png]]

[[file:_img/screenshot_2017-10-30_08-56-11.png]]
* Setting up your goal
** Single number evaluation metric
[[file:_img/screenshot_2017-10-31_23-27-35.png]]

- Precision :: When the classifier says that something is a cat, the percentage that it's correct.
- Recall    :: When there is a cat, the percentage that the classifier says it's a cat.
- F1 Score  :: can be thought as the average of *precision* and *recall*.



[[file:_img/screenshot_2017-10-31_23-29-11.png]]

- Average :: Make multiple metrics into single one.
** Satisficing and Optimizing metric
[[file:_img/screenshot_2017-11-02_08-32-21.png]]

** Train/dev/test distributions
[[file:_img/screenshot_2017-11-02_08-37-21.png]]

- ~dev~ / ~test~ data set should come from the same distribution

[[file:_img/screenshot_2017-11-02_08-38-46.png]]

[[file:_img/screenshot_2017-11-02_08-40-03.png]]
** Size of the dev and test sets
[[file:_img/screenshot_2017-11-08_08-35-33.png]]

- The actual amount is more important thatn the ratio of data set
- If you have large enough data set, having ~data~ / ~test~ set even 1% is okay because its actual amount is enough.

[[file:_img/screenshot_2017-11-08_08-35-57.png]]

- ~dev~ set vs ~test~ set :: Whether evaluating during iteration or not
- Not having ~test~ set may okay
** When to change dev/test sets and metrics
[[file:_img/screenshot_2017-11-08_08-42-50.png]]

[[file:_img/screenshot_2017-11-08_08-43-16.png]]

[[file:_img/screenshot_2017-11-08_08-43-34.png]]

* Comparing to human-level performance
** Why human-level performance
[[file:_img/screenshot_2017-11-08_08-44-24.png]]

- Bayes error :: Theoretical minimum error
- Human error :: An approximation of *Bayes error*

[[file:_img/screenshot_2017-11-08_08-45-44.png]]

** Avoidable bias
[[file:_img/screenshot_2017-11-08_08-46-19.png]]

** Understanding human-level performance
[[file:_img/screenshot_2017-11-08_08-47-01.png]]

- When considering human level error as an approximation of bayes error, the best performance which human can acquire would be relavent.
- In other views, you can consider human level performance differently.

[[file:_img/screenshot_2017-11-08_08-48-46.png]]

** Surpassing human-level performance
[[file:_img/screenshot_2017-11-08_08-50-06.png]]

** Improving your model performance
[[file:_img/screenshot_2017-11-08_08-50-37.png]]

[[file:_img/screenshot_2017-11-08_08-51-09.png]]

* Machine Learning flight simulator
** Quiz: Bird recognition in the city of Peacetopia
[[file:_img/screenshot_2017-11-08_11-55-02.png]]

[[file:_img/screenshot_2017-11-08_11-57-44.png]]

It would be hard to measure the running time and memory usage during iteration.
So it will slow down the learning process.

[[file:_img/screenshot_2017-11-08_12-28-17.png]]

It seems that the variety of training data set would be okay.

[[file:_img/screenshot_2017-11-08_12-36-02.png]]

It seems that with only 1,000 images of 100,000,000 images, changing data set won't make notable changes.
