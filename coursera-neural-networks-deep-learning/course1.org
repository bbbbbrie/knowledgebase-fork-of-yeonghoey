#+TITLE: Neural Networks and Deep Learning

* Table of Contents :TOC_3_gh:
- [[#week-1][Week 1]]
  - [[#introduction-to-deep-learning][Introduction to Deep Learning]]
    - [[#what-is-a-neural-network][What is a neural network?]]
    - [[#supervised-learning-with-neural-networks][Supervised Learning with Neural Networks]]
    - [[#why-is-deep-learning-taking-off][Why is Deep Learning taking off?]]
    - [[#about-this-course][About this Course]]
- [[#week-2][Week 2]]
  - [[#logistic-regression-as-a-neural-network][Logistic Regression as a Neural Network]]
    - [[#binary-classification][Binary Classification]]
    - [[#logistic-regression][Logistic Regression]]
    - [[#logistic-regression-cost-function][Logistic Regression Cost Function]]
    - [[#gradient-descent][Gradient Descent]]
    - [[#derivatives][Derivatives]]
    - [[#more-derivative-examples][More Derivative Examples]]
    - [[#computation-graph][Computation Graph]]
    - [[#derivatives-with-a-computation-graph][Derivatives with a Computation Graph]]
    - [[#logistic-regression-gradient-descent][Logistic Regression Gradient Descent]]
    - [[#gradient-descent-on-m-examples][Gradient Descent on m Examples]]
  - [[#python-and-vectorization][Python and Vectorization]]
    - [[#vectorization][Vectorization]]
    - [[#more-vectorization-examples][More Vectorization Examples]]
    - [[#vectorizing-logistic-regression][Vectorizing Logistic Regression]]
    - [[#vectorizing-logistic-regressions-gradient-output][Vectorizing Logistic Regression's Gradient Output]]
    - [[#broadcasting-in-python][Broadcasting in Python]]
    - [[#a-note-on-pythonnumpy-vectors][A note on python/numpy vectors]]
    - [[#explanation-of-logistic-regression-cost-function-optional][Explanation of logistic regression cost function (optional)]]
  - [[#programming-assignments][Programming Assignments]]
    - [[#python-basics-with-numpy][Python Basics with numpy]]
    - [[#logistic-regression-with-a-neural-network-mindset][Logistic Regression with a Neural Network mindset]]
- [[#week-3][Week 3]]
  - [[#shallow-neural-network][Shallow Neural Network]]
    - [[#neural-networks-overview][Neural Networks Overview]]
    - [[#neural-network-representation][Neural Network Representation]]
    - [[#computing-a-neural-networks-output][Computing a Neural Network's Output]]
    - [[#vectorizing-across-multiple-examples][Vectorizing across multiple examples]]
    - [[#explanation-for-vectorized-implementation][Explanation for Vectorized Implementation]]
    - [[#activation-functions][Activation functions]]
    - [[#why-do-you-need-non-linear-activation-functions][Why do you need non-linear activation functions?]]
    - [[#derivatives-of-activation-functions][Derivatives of activation functions]]
    - [[#gradient-descent-for-neural-networks][Gradient descent for Neural Networks]]
    - [[#backpropagation-intuition-optional][Backpropagation intuition (optional)]]
    - [[#random-initialization][Random Initialization]]
  - [[#programming-assignment][Programming Assignment]]
    - [[#planar-data-classification][Planar data classification]]

* Week 1
** Introduction to Deep Learning
*** What is a neural network?
[[file:img/screenshot_2017-09-12_08-01-22.png]]

[[file:img/screenshot_2017-09-12_08-01-40.png]]

[[file:img/screenshot_2017-09-12_08-01-54.png]]
*** Supervised Learning with Neural Networks
[[file:img/screenshot_2017-09-13_00-50-59.png]]

[[file:img/screenshot_2017-09-13_00-51-21.png]]

[[file:img/screenshot_2017-09-13_00-51-43.png]]
*** Why is Deep Learning taking off?
[[file:img/screenshot_2017-09-13_01-04-45.png]]

- ~m~ stands for the number of training examples

[[file:img/screenshot_2017-09-13_01-05-22.png]]
*** About this Course
[[file:img/screenshot_2017-09-13_08-26-24.png]]

* Week 2
** Logistic Regression as a Neural Network
*** Binary Classification
[[file:img/screenshot_2017-09-14_07-24-18.png]]

[[file:img/screenshot_2017-09-14_07-24-44.png]]
*** Logistic Regression
[[file:img/screenshot_2017-09-14_07-31-55.png]]

*** Logistic Regression Cost Function
[[file:img/screenshot_2017-09-15_07-34-40.png]]

*** Gradient Descent
[[file:img/screenshot_2017-09-15_08-47-22.png]]

[[file:img/screenshot_2017-09-15_08-46-52.png]]
*** Derivatives
[[file:img/screenshot_2017-09-16_14-41-04.png]]

*** More Derivative Examples
[[file:img/screenshot_2017-09-16_15-30-37.png]]



[[file:img/screenshot_2017-09-16_15-31-29.png]]

*** Computation Graph
[[file:img/screenshot_2017-09-16_15-32-09.png]]

*** Derivatives with a Computation Graph
- Calculus :: Chain rule


[[file:img/screenshot_2017-09-16_15-46-34.png]]

[[file:img/screenshot_2017-09-16_15-47-24.png]]
*** Logistic Regression Gradient Descent
[[file:img/screenshot_2017-09-17_13-19-16.png]]

*** Gradient Descent on m Examples
[[file:img/screenshot_2017-09-17_13-29-07.png]]

[[file:img/screenshot_2017-09-17_13-27-08.png]]
** Python and Vectorization
*** Vectorization
[[file:img/screenshot_2017-09-17_13-38-39.png]]

[[file:img/screenshot_2017-09-17_13-38-54.png]]

[[file:img/screenshot_2017-09-17_13-39-21.png]]

*** More Vectorization Examples
[[file:img/screenshot_2017-09-18_08-32-09.png]]

[[file:img/screenshot_2017-09-18_08-32-39.png]]

*** Vectorizing Logistic Regression
[[file:img/screenshot_2017-09-18_08-41-30.png]]
*** Vectorizing Logistic Regression's Gradient Output 
[[file:img/screenshot_2017-09-20_08-42-00.png]]

[[file:img/screenshot_2017-09-20_08-42-26.png]]

*** Broadcasting in Python
[[file:img/screenshot_2017-09-20_08-47-36.png]]

[[file:img/screenshot_2017-09-20_08-45-33.png]]

[[file:img/screenshot_2017-09-20_08-45-51.png]]
*** A note on python/numpy vectors
[[file:img/screenshot_2017-09-20_08-48-48.png]]

*** Explanation of logistic regression cost function (optional)
[[file:img/screenshot_2017-09-20_08-53-24.png]]

Generally, most algorithms get a loss function and try to minimize it.
For ~P(y|x)~, the bigger the better. So, the loss function ~L~ is the negative of ~P(y|x)~.

[[file:img/screenshot_2017-09-20_09-00-19.png]]
** Programming Assignments
Because I'm not allowed to post my code for the assignments,
I'll just put the instructions and summary notes instead.

*** Python Basics with numpy
[[file:img/screenshot_2017-09-21_18-16-54.png]]

[[file:img/screenshot_2017-09-21_18-22-10.png]]

[[file:img/screenshot_2017-09-21_18-23-44.png]]

[[file:img/screenshot_2017-09-21_18-31-13.png]]

[[file:img/screenshot_2017-09-21_22-48-12.png]]

[[file:img/screenshot_2017-09-21_22-55-01.png]]
*** Logistic Regression with a Neural Network mindset
[[file:img/screenshot_2017-09-23_07-34-05.png]]

[[file:img/screenshot_2017-09-23_07-36-34.png]]

[[file:img/screenshot_2017-09-23_07-35-34.png]]

[[file:img/screenshot_2017-09-23_07-37-21.png]]

[[file:img/screenshot_2017-09-23_07-46-22.png]]

- The formula of ~J~ must be calculated by ~elementwise multiplication~, not ~dot product~.

[[file:img/screenshot_2017-09-23_08-14-32.png]]

[[file:img/screenshot_2017-09-23_08-20-04.png]]

[[file:img/screenshot_2017-09-23_08-23-02.png]]

[[file:img/screenshot_2017-09-23_08-23-41.png]]

[[file:img/screenshot_2017-09-23_08-24-26.png]]

[[file:img/screenshot_2017-09-23_08-25-59.png]]
* Week 3
** Shallow Neural Network
*** Neural Networks Overview
[[file:img/screenshot_2017-09-23_09-52-42.png]]

*** Neural Network Representation
[[file:img/screenshot_2017-09-23_09-58-03.png]]

*** Computing a Neural Network's Output
[[file:img/screenshot_2017-09-24_15-51-31.png]]

[[file:img/screenshot_2017-09-24_15-52-25.png]]

[[file:img/screenshot_2017-09-24_15-53-08.png]]

*** Vectorizing across multiple examples
[[file:img/screenshot_2017-09-24_15-53-58.png]]

[[file:img/screenshot_2017-09-24_15-54-36.png]]

*** Explanation for Vectorized Implementation
[[file:img/screenshot_2017-09-24_15-55-39.png]]

[[file:img/screenshot_2017-09-24_15-56-39.png]]

*** Activation functions
- For hidden units, ~tanh~ is almost alway superior to ~sigmoid~
- Because ~[-1, 1]~ and ~0~ mean, rather than ~[0, 1]~ and ~0.5~ mean, actually make the learning for the next layer easier.
- ~sigmoid~ is preferred mostly for the output layer which expects values of ~[0, 1]~
- There days, ~ReLU~ is the default and generally most preferred.

k[[file:img/screenshot_2017-09-24_15-58-26.png]]

[[file:img/screenshot_2017-09-24_15-58-53.png]]

*** Why do you need non-linear activation functions?
- If all activation functions are linear, *the calculation of hidden layers can be boiled down to a single linear layer.*
- When *the output value can have all the real number*, then the activation function for the output layer can be a linear one.

[[file:img/screenshot_2017-09-24_16-09-21.png]]

*** Derivatives of activation functions
[[file:img/screenshot_2017-09-24_18-05-32.png]]

[[file:img/screenshot_2017-09-24_18-06-05.png]]

- Theoretically the derivative of ~z=0~ is undefined, but it doesn't matter technically.

[[file:img/screenshot_2017-09-24_18-06-30.png]]

*** Gradient descent for Neural Networks
[[file:img/screenshot_2017-09-24_18-08-44.png]]

*** Backpropagation intuition (optional)
[[file:img/screenshot_2017-09-24_18-09-23.png]]

[[file:img/screenshot_2017-09-24_18-09-52.png]]

[[file:img/screenshot_2017-09-24_18-10-13.png]]

*** Random Initialization
if initial ~W~ values are all zeros, all hidden units become completly identical, zero, which make the hidden layer meaningless.

[[file:img/screenshot_2017-09-24_18-10-55.png]]

By multipling ~0.01~, it can be avoided to have very large values of ~a~ which have very small derivatives slowing down the learning.

[[file:img/screenshot_2017-09-24_18-11-26.png]]
** Programming Assignment
*** Planar data classification
[[file:img/screenshot_2017-09-28_06-28-11.png]]

[[file:img/screenshot_2017-09-28_06-30-57.png]]

[[file:img/screenshot_2017-09-28_06-31-43.png]]

[[file:img/screenshot_2017-09-28_06-32-28.png]]

[[file:img/screenshot_2017-09-28_06-47-36.png]]

[[file:img/screenshot_2017-09-28_07-09-33.png]]

[[file:img/screenshot_2017-09-28_07-23-53.png]]

                  [[file:img/screenshot_2017-09-28_07-25-41.png]]

[[file:img/screenshot_2017-09-28_07-26-05.png]]

[[file:img/screenshot_2017-09-28_07-26-36.png]]
