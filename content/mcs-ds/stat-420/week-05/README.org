#+TITLE: STAT 420: Week 5
#+SUBTITLE: Summary of Week 1-4, as for Exam Week

* Week 1
:REFERENCES:
- http://daviddalpiaz.github.io/appliedstats/data-and-programming.html#data-frames
- http://www.statisticshowto.com/probability-and-statistics/skewed-distribution/
:END:

** Combining
#+BEGIN_SRC R :session :results output :exports both
  all(c(1, c(2, 3)) == c(1, 2, 3))
#+END_SRC

#+RESULTS:
: [1] TRUE

#+BEGIN_SRC R :session :results output :exports both
  x = data.frame(x0 = c(1, 2, 3), x1 = c(4, 5, 6))
  all(data.matrix(x) == matrix(1:6, nrow = 3))
  all(data.matrix(cbind(x, c(7, 8, 9))) == matrix(1:9, nrow = 3))
  all(data.matrix(cbind(x, c(7, 8, 9))) == matrix(1:9, nrow = 3))
#+END_SRC

#+RESULTS:
: [1] TRUE
: [1] TRUE
: [1] TRUE

** Subsetting
#+BEGIN_SRC R :session :results output :exports both
  x = (1:5) ^ 2
  x[c(1, 3)]
  x[-c(1, 3)]
#+END_SRC

#+RESULTS:
: [1] 1 9
: [1]  4 16 25

#+BEGIN_SRC R :session :results output :exports both
  mean(subset(airquality, Month == 5)$Wind)
  mean(subset(airquality, Month == 5, c("Wind"))[[1]])
  mean(airquality[airquality$Month == 5, ]$Wind)
  mean(airquality[which(airquality$Month == 5), ]$Wind)
#+END_SRC

#+RESULTS:
: [1] 11.62258
: [1] 11.62258
: [1] 11.62258
: [1] 11.62258

#+BEGIN_SRC R :session :results output :exports both
  diag(5)
#+END_SRC

#+RESULTS:
:      [,1] [,2] [,3] [,4] [,5]
: [1,]    1    0    0    0    0
: [2,]    0    1    0    0    0
: [3,]    0    0    1    0    0
: [4,]    0    0    0    1    0
: [5,]    0    0    0    0    1

#+BEGIN_SRC R :session :results output :exports both
  diag(matrix(1:9, nrow = 3))
#+END_SRC

#+RESULTS:
: [1] 1 5 9

** Plotting
#+BEGIN_SRC R :session :file _img/boxplot.png :results graphics :exports both
  mpg = ggplot2::mpg
  boxplot(hwy ~ drv, data = mpg,
          xlab   = "Drivetrain (f = FWD, r = RWD, 4 = 4WD)",
          ylab   = "Miles Per Gallon (Highway)",
          main   = "MPG (Highway) vs Drivetrain",
          pch    = 20,
          cex    = 2,
          col    = "darkorange",
          border = "dodgerblue")
#+END_SRC

#+RESULTS:
[[file:_img/boxplot.png]]

- ~pch~ :: character to be used for plotting points.
- ~cex~ :: character expansion (size)
- ~col~ :: color

[[file:_img/b2f5c18533d6f9870fb5dc2fc25111ea0994c6e1.png]]

* Week 2
:REFERENCES:
- http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html
:END:

** SLR
\begin{equation}
Y = f(X) + \epsilon
\end{equation}

- Response = Prediction + Error
- Response = Signal + Noise
- Response = Model + Unexplained
- Response = Deterministic + Random
- Response = Explainable + Unexplainable

[[file:_img/4d5af9db62c340d109b1bf20047759ac68419459.png]]

Assumptions of SLR: *LINE*
- Linear :: The relationship between $Y$ and $x$ is linear, of the form $\beta_0 + \beta_1x$.
- Indepedent :: The errors $\epsilon$ are independent.
- Normal :: The errors, $\epsilon$ are normally distributed.
- Equal Variance :: At each value of $x$ , the variance of $Y$ is the same, $\sigma^2$.

Meaning of *SLR*:
- Simple :: a single predictor variable
- Linear :: $Y$ is a linear combination of the predictors $X$.
- Regression  ::  we are attempting to measure the relationship between a response variable and predictor variables.

** Least Squares Approach
\begin{aligned}
S_{xy} &= \sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})\\
S_{xx} &= \sum_{i = 1}^{n}(x_i - \bar{x})^2\\
S_{yy} &= \sum_{i = 1}^{n}(y_i - \bar{y})^2
\end{aligned}

Based on above, the estimates are:
\begin{aligned}
\hat{\beta}_1 &= \frac{S_{xy}}{S_{xx}}\\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}
\end{aligned}

** Variance Estimation
\begin{aligned}
e_i   &= y_i - \hat{y}_i \\
s_e^2 &= \frac{1}{n - 2} \sum_{i = 1}^{n} e_i^2
\end{aligned}

- $s_e$ is also called *RSE*, /Residual Standard Error/

** Maximum Likelihood Estimation Approach
Eventually, estimates other than variance are the same as Least Squares Approach.

\begin{aligned}
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i = 1}^{n}e_i^2
\end{aligned}

** Making Predictions
#+BEGIN_SRC R :session :results output :exports both
  x = cars$speed
  y = cars$dist

  x0 = 8
  # Test whether x0 is one of sample predictor values
  x0 %in% unique(x)
  # Test whether x0 is within sample predictor value range
  min(x) < x0 & x0 < max(x)
#+END_SRC

#+RESULTS:
: [1] TRUE
: [1] TRUE

** Decomposition of Variation
\begin{aligned}
R^2 &= \frac{\text{SSReg}}{\text{SST}} = \frac{\sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} \\[2.5ex]
    &= \frac{\text{SST} - \text{SSE}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}} \\[2.5ex]
    &= 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} = 
1 - \frac{\sum_{i = 1}^{n}e_i^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
\end{aligned}

- SSReg :: Sum of Squares Regression
- SST :: Sum of Squares Total
- SSE :: Sum of Squares Error

** lm
#+BEGIN_SRC R :session :results output :exports code
  m = lm(dist ~ speed, data = cars)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :session :results output :exports both
  coef(m)
#+END_SRC

#+RESULTS:
: (Intercept)       speed 
:  -17.579095    3.932409

#+BEGIN_SRC R :session :results output :exports both
  resid(m)
#+END_SRC

#+RESULTS:
#+begin_example
         1          2          3          4          5          6          7 
  3.849460  11.849460  -5.947766  12.052234   2.119825  -7.812584  -3.744993 
         8          9         10         11         12         13         14 
  4.255007  12.255007  -8.677401   2.322599 -15.609810  -9.609810  -5.609810 
        15         16         17         18         19         20         21 
 -1.609810  -7.542219   0.457781   0.457781  12.457781 -11.474628  -1.474628 
        22         23         24         25         26         27         28 
 22.525372  42.525372 -21.407036 -15.407036  12.592964 -13.339445  -5.339445 
        29         30         31         32         33         34         35 
-17.271854  -9.271854   0.728146 -11.204263   2.795737  22.795737  30.795737 
        36         37         38         39         40         41         42 
-21.136672 -11.136672  10.863328 -29.069080 -13.069080  -9.069080  -5.069080 
        43         44         45         46         47         48         49 
  2.930920  -2.933898 -18.866307  -6.798715  15.201285  16.201285  43.201285 
        50 
  4.268876
#+end_example

#+BEGIN_SRC R :session :results output :exports both
  fitted(m)
#+END_SRC

#+RESULTS:
#+begin_example
        1         2         3         4         5         6         7         8 
-1.849460 -1.849460  9.947766  9.947766 13.880175 17.812584 21.744993 21.744993 
        9        10        11        12        13        14        15        16 
21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 
       17        18        19        20        21        22        23        24 
33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 
       25        26        27        28        29        30        31        32 
41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 
       33        34        35        36        37        38        39        40 
53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 
       41        42        43        44        45        46        47        48 
61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 
       49        50 
76.798715 80.731124
#+end_example

* Week 3
:REFERENCES:
- http://daviddalpiaz.github.io/appliedstats/inference-for-simple-linear-regression.html
:END:

** Gauss-Markov Theorem
From this chapter, we consider $\hat{\beta_0}$ and $\hat{\beta_1}$ as random variables:

\begin{aligned}
\hat{\beta}_1 &= \frac{\sum_{i = 1}^{n}(x_i - \bar{x}) Y_i}{\sum_{i = 1}^{n}(x_i - \bar{x})^2} \\
\hat{\beta}_0 &= \bar{Y} - \hat{\beta}_1 \bar{x}
\end{aligned}

Gauss-Markov theorem tells us that the estimates we derived above are *BLUE*, /best linear unbiased estimates/:
- Best :: the estimates are with the minimum variance.
- Linear :: the estimates are linear combinations of $Y_i$
- Unbiased :: $\text{E}[\hat{\beta}_0] = \beta_0$ and $\text{E}[\hat{\beta}_1] = \beta_1$.

** Sampling Distributions
\begin{aligned}
\hat{\beta}_1 &\sim N\left( \beta_1, \frac{\sigma^2}{S_{xx}} \right) \\
\hat{\beta}_0 &\sim N\left( \beta_0, \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right) \right)
\end{aligned}

** Standard Errors
\begin{aligned}
\text{SD}[\hat{\beta}_0] &= \sigma\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}} \\
\text{SD}[\hat{\beta}_1] &= \frac{\sigma}{\sqrt{S_{xx}}}
\end{aligned}

Since we donâ€™t know $\sigma$ in practice, we will have to estimates it using $s_e$.
\begin{aligned}
\text{SE}[\hat{\beta}_0] &= s_e\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}} \\
\text{SE}[\hat{\beta}_1] &= \frac{s_e}{\sqrt{S_{xx}}}
\end{aligned}

Now if we divide by the standard error, instead of the standard deviation,
we obtain the following results which will allow us to make confidence intervals and perform hypothesis testing.

\begin{aligned}
\frac{\hat{\beta}_0 - \beta_0}{\text{SE}[\hat{\beta}_0]} &\sim t_{n-2} \\
\frac{\hat{\beta}_1 - \beta_1}{\text{SE}[\hat{\beta}_1]} &\sim t_{n-2}
\end{aligned}

[[file:_img/e52d73ee9850e48f7a78151298851e7150ebb17b.png]]

** Confidence Intervals
\begin{aligned}
\text{EST} &\pm \text{CRIT} \cdot \text{SE} \\
\text{EST} &\pm \text{MARGIN}
\end{aligned}

And,

\begin{aligned}
\hat{\beta}_0 &\pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}} \\
\hat{\beta}_1 &\pm t_{\alpha/2, n - 2} \cdot \frac{s_e}{\sqrt{S_{xx}}} \\
\hat{y}(x) &\pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}} \\
\hat{y}(x) &\pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}} \text{(Prediction)}
\end{aligned}
where,
$$
P(t_{n-2} > t_{\alpha/2, n - 2}) = \alpha/2
$$

The proof is [[https://www.biostat.wisc.edu/~kbroman/teaching/labstat/fourth/notes16.pdf][here]], about [[https://faculty.math.illinois.edu/~hildebr/461/variance.pdf][covariance]]

** Hypothesis Tests
\begin{aligned}
t &= \frac{\hat{\beta}_1-\beta_{10}}{s_e / \sqrt{S_{xx}}} \\
\text{p-value} &= P(>|t|)
\end{aligned}

- When $\text{p-value} < \alpha$, we reject $H_0$.

* Week 4
:REFERENCES:
- http://daviddalpiaz.github.io/appliedstats/multiple-linear-regression.html
:END:

** Matrix Approach to Regression
$$
Y = X \beta + \epsilon \\

Y = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots\\ Y_n \end{bmatrix}, \quad
X = \begin{bmatrix}
1      & x_{11}    & x_{12}    & \cdots & x_{1(p-1)} \\
1      & x_{21}    & x_{22}    & \cdots & x_{2(p-1)} \\
\vdots & \vdots    & \vdots    &  & \vdots \\
1      & x_{n1}    & x_{n2}    & \cdots & x_{n(p-1)} \\
\end{bmatrix}, \quad
\epsilon = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots\\ \epsilon_n \end{bmatrix}
$$

$$
\beta = \begin{bmatrix}
\beta_0 & \beta_1 & \beta_2 & \dots & \beta_{p-1} \\
\end{bmatrix}
$$

(I tweaked the notation from the book as rows for $n$, cols for $p$)

So, we can derive:
\begin{aligned}
\hat{\beta} &= \left(  X^\top X  \right)^{-1}X^\top y \\
\hat{y} &= X \hat{\beta} \\
s_e^2 &= \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n - p} = \frac{e^\top e}{n-p} \\
\end{aligned}

** Sampling Distribution
$$
\text{Var}[\hat{\beta}_j] = \sigma^2 C_{jj}
$$

where

$$
C = \left(X^\top X\right)^{-1}
$$

$$
C = \begin{bmatrix}
C_{00}     & C_{01}     & C_{02}     & \cdots & C_{0(p-1)}     \\
C_{10}     & C_{11}     & C_{12}     & \cdots & C_{1(p-1)}     \\
C_{20}     & C_{21}     & C_{22}     & \cdots & C_{2(p-1)}     \\
\vdots     & \vdots     & \vdots     &        & \vdots         \\
C_{(p-1)0} & C_{(p-1)1} & C_{(p-1)2} & \cdots & C_{(p-1)(p-1)} \\
\end{bmatrix}
$$

So,

\begin{aligned}
\text{SD}[\hat{\beta}] &= \sigma \sqrt{\left(  X^\top X  \right)^{-1}} \\
\text{SE}[\hat{\beta}] &= s_e \sqrt{\left(  X^\top X  \right)^{-1}}
\end{aligned}

And,

\begin{aligned}
\hat{\beta}_j &\sim N\left(\beta_j, \sigma^2 C_{jj}  \right). \\
\frac{\hat{\beta}_j - \beta_j}{s_e \sqrt{C_{jj}}} &\sim t_{n-p}
\end{aligned}

** Confidence Intervals
\begin{aligned}
\hat{\beta}_j &\pm t_{\alpha/2, n - p} \cdot s_e\sqrt{C_{jj}} \\
\hat{y}(x_0) &\pm t_{\alpha/2, n - p} \cdot s_e \sqrt{x_{0}^\top C x_{0}} \\
\hat{y}(x_0) &\pm t_{\alpha/2, n - p} \cdot s_e \sqrt{1 + x_{0}^\top C x_{0}} \quad \text{(Prediction)}
\end{aligned}

#+BEGIN_SRC R :session :exports none
  autompg = read.table(
    "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data",
    quote = "\"",
    comment.char = "",
    stringsAsFactors = FALSE)
                                          # give the dataframe headers
  colnames(autompg) = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin", "name")
                                          # remove missing data, which is stored as "?"
  autompg = subset(autompg, autompg$hp != "?")
                                          # remove the plymouth reliant, as it causes some issues
  autompg = subset(autompg, autompg$name != "plymouth reliant")
                                          # give the dataset row names, based on the engine, year and name
  rownames(autompg) = paste(autompg$cyl, "cylinder", autompg$year, autompg$name)
                                          # remove the variable for name, as well as origin
  autompg = subset(autompg, select = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year"))
                                          # change horsepower from character to numeric
  autompg$hp = as.numeric(autompg$hp)
                                          # check final structure of data
  str(autompg)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :session :results output :exports both
  mpg_model = lm(mpg ~ wt + year, data = autompg)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :session :results output :exports both
  confint(mpg_model, level = 0.99)
#+END_SRC

#+RESULTS:
:                     0.5 %       99.5 %
: (Intercept) -25.052563681 -4.222720208
: wt           -0.007191036 -0.006078716
: year          0.632680051  0.890123859

** Significance of Regression
[[file:_img/fa7b501ce96470993b2057d3c66438357f0d571c.png]]

** Nested Models
[[file:_img/70d75ebd2751069b69ebd0093f32595bce3a421e.png]]
