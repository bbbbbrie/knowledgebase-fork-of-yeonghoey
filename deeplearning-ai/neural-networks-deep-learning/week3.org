#+TITLE: Neural Networks and Deep Learning: Week 3

* Table of Contents :TOC_3_gh:
- [[#shallow-neural-network][Shallow Neural Network]]
  - [[#neural-networks-overview][Neural Networks Overview]]
  - [[#neural-network-representation][Neural Network Representation]]
  - [[#computing-a-neural-networks-output][Computing a Neural Network's Output]]
  - [[#vectorizing-across-multiple-examples][Vectorizing across multiple examples]]
  - [[#explanation-for-vectorized-implementation][Explanation for Vectorized Implementation]]
  - [[#activation-functions][Activation functions]]
  - [[#why-do-you-need-non-linear-activation-functions][Why do you need non-linear activation functions?]]
  - [[#derivatives-of-activation-functions][Derivatives of activation functions]]
  - [[#gradient-descent-for-neural-networks][Gradient descent for Neural Networks]]
  - [[#backpropagation-intuition-optional][Backpropagation intuition (optional)]]
  - [[#random-initialization][Random Initialization]]
- [[#programming-assignment][Programming Assignment]]
  - [[#planar-data-classification][Planar data classification]]

* Shallow Neural Network
** Neural Networks Overview
[[file:_img/screenshot_2017-09-23_09-52-42.png]]

** Neural Network Representation
[[file:_img/screenshot_2017-09-23_09-58-03.png]]

** Computing a Neural Network's Output
[[file:_img/screenshot_2017-09-24_15-51-31.png]]

[[file:_img/screenshot_2017-09-24_15-52-25.png]]

[[file:_img/screenshot_2017-09-24_15-53-08.png]]

** Vectorizing across multiple examples
[[file:_img/screenshot_2017-09-24_15-53-58.png]]

[[file:_img/screenshot_2017-09-24_15-54-36.png]]

** Explanation for Vectorized Implementation
[[file:_img/screenshot_2017-09-24_15-55-39.png]]

[[file:_img/screenshot_2017-09-24_15-56-39.png]]

** Activation functions
- For hidden units, ~tanh~ is almost alway superior to ~sigmoid~
- Because ~[-1, 1]~ and ~0~ mean, rather than ~[0, 1]~ and ~0.5~ mean, actually make the learning for the next layer easier.
- ~sigmoid~ is preferred mostly for the output layer which expects values of ~[0, 1]~
- There days, ~ReLU~ is the default and generally most preferred.

[[file:_img/screenshot_2017-09-24_15-58-26.png]]

[[file:_img/screenshot_2017-09-24_15-58-53.png]]

** Why do you need non-linear activation functions?
- If all activation functions are linear, *the calculation of hidden layers can be boiled down to a single linear layer.*
- When *the output value can have all the real number*, then the activation function for the output layer can be a linear one.

[[file:_img/screenshot_2017-09-24_16-09-21.png]]

** Derivatives of activation functions
[[file:_img/screenshot_2017-09-24_18-05-32.png]]

[[file:_img/screenshot_2017-09-24_18-06-05.png]]

- Theoretically the derivative of ~z=0~ is undefined, but it doesn't matter technically.

[[file:_img/screenshot_2017-09-24_18-06-30.png]]

** Gradient descent for Neural Networks
[[file:_img/screenshot_2017-09-24_18-08-44.png]]

** Backpropagation intuition (optional)
[[file:_img/screenshot_2017-09-24_18-09-23.png]]

[[file:_img/screenshot_2017-09-24_18-09-52.png]]

[[file:_img/screenshot_2017-09-24_18-10-13.png]]

** Random Initialization
if initial ~W~ values are all zeros, all hidden units become completly identical, zero, which make the hidden layer meaningless.

[[file:_img/screenshot_2017-09-24_18-10-55.png]]

By multipling ~0.01~, it can be avoided to have very large values of ~a~ which have very small derivatives slowing down the learning.

[[file:_img/screenshot_2017-09-24_18-11-26.png]]

* Programming Assignment
** Planar data classification
#+BEGIN_SRC python
  def initialize_parameters(n_x, n_h, n_y):
      parameters = {"W1": W1,
                    "b1": b1,
                    "W2": W2,
                    "b2": b2}
      return parameters

  def forward_propagation(X, parameters):
      cache = {"Z1": Z1,
               "A1": A1,
               "Z2": Z2,
               "A2": A2}
      return A2, cache

  def compute_cost(A2, Y, parameters):
      return cost

  def backward_propagation(parameters, cache, X, Y):
      grads = {"dW1": dW1,
               "db1": db1,
               "dW2": dW2,
               "db2": db2}
      return grads

  def update_parameters(parameters, grads, learning_rate = 1.2):
      parameters = {"W1": W1,
                    "b1": b1,
                    "W2": W2,
                    "b2": b2}
      return parameters

  def predict(parameters, X):
      return predictions
#+END_SRC

[[file:_img/screenshot_2017-09-28_06-28-11.png]]

[[file:_img/screenshot_2017-09-28_06-30-57.png]]

[[file:_img/screenshot_2017-09-28_06-31-43.png]]

[[file:_img/screenshot_2017-09-28_06-32-28.png]]

[[file:_img/screenshot_2017-09-28_06-47-36.png]]

[[file:_img/screenshot_2017-09-28_07-09-33.png]]

[[file:_img/screenshot_2017-09-28_07-23-53.png]]

[[file:_img/screenshot_2017-09-28_07-25-41.png]]

[[file:_img/screenshot_2017-09-28_07-26-05.png]]

[[file:_img/screenshot_2017-09-28_07-26-36.png]]
