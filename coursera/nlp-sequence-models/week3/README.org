#+TITLE: Sequence models & Attention mechanism

* Table of Contents :TOC_3_gh:
- [[#various-sequence-to-sequence-architectures][Various sequence to sequence architectures]]
  - [[#basic-models][Basic Models]]
  - [[#picking-the-most-likely-sentence][Picking the most likely sentence]]
  - [[#beam-search][Beam Search]]
  - [[#refinements-to-beam-search][Refinements to Beam Search]]
  - [[#error-analysis-in-beam-search][Error analysis in beam search]]
  - [[#bleu-score-optional][Bleu Score (optional)]]
  - [[#attention-model-intuition][Attention Model Intuition]]
  - [[#attention-model][Attention Model]]
- [[#speech-recognition---audio-data][Speech recognition - Audio data]]
  - [[#speech-recognition][Speech recognition]]
  - [[#trigger-word-detection][Trigger Word Detection]]
- [[#conclusion][Conclusion]]
  - [[#conclusion-and-thank-you][Conclusion and thank you]]
- [[#practice-questions][Practice questions]]
  - [[#quiz-sequence-models--attention-mechanism][Quiz: Sequence models & Attention mechanism]]
- [[#programming-assignments][Programming assignments]]
  - [[#neural-machine-translation][Neural Machine Translation]]
  - [[#trigger-word-detection-1][Trigger word detection]]

* Various sequence to sequence architectures
** Basic Models
[[file:img/screenshot_2018-02-13_10-12-12.png]]

[[file:img/screenshot_2018-02-13_10-14-44.png]]

** Picking the most likely sentence
[[file:img/screenshot_2018-02-13_10-18-37.png]]

[[file:img/screenshot_2018-02-13_10-20-21.png]]

Unlike the language model previously introduced, in machine translation,
pick most likely ~y~ instead of sampling for consistent results.

[[file:img/screenshot_2018-02-13_10-26-51.png]]

Don't use greedy approach, otherwise ~Jane is going~, which is less succinct in the above example, will always be chosen.

** Beam Search
[[file:img/screenshot_2018-02-13_10-33-49.png]]

[[file:img/screenshot_2018-02-13_10-41-03.png]]

In the second step, pick the top 3 again among 30,000 possibilities.
Because Beam width is 3, every step needs 3 models.

[[file:img/screenshot_2018-02-13_10-44-05.png]]

** Refinements to Beam Search
[[file:img/screenshot_2018-02-14_08-47-01.png]]

The actual probability is too small to practically calculate.
So take log to the probabilities

[[file:img/screenshot_2018-02-14_08-50-10.png]]

** Error analysis in beam search
[[file:img/screenshot_2018-02-14_08-56-15.png]]

[[file:img/screenshot_2018-02-14_08-59-11.png]]

[[file:img/screenshot_2018-02-14_09-01-28.png]]

** Bleu Score (optional)
[[file:img/screenshot_2018-02-14_09-08-08.png]]

[[file:img/screenshot_2018-02-14_09-11-13.png]]

[[file:img/screenshot_2018-02-14_09-15-12.png]]

[[file:img/screenshot_2018-02-14_09-17-49.png]]
** Attention Model Intuition
[[file:img/screenshot_2018-02-14_09-25-35.png]]

[[file:img/screenshot_2018-02-14_09-37-13.png]]

** Attention Model
[[file:img/screenshot_2018-02-14_09-43-51.png]]

[[file:img/screenshot_2018-02-14_09-50-54.png]]

- Essentially, ~a^<t, t'>~ is a softmax function

[[file:img/screenshot_2018-02-14_09-52-26.png]]

* Speech recognition - Audio data
** Speech recognition
[[file:img/screenshot_2018-02-14_19-59-57.png]]

[[file:img/screenshot_2018-02-14_20-32-30.png]]

[[file:img/screenshot_2018-02-14_20-36-03.png]]

** Trigger Word Detection
[[file:img/screenshot_2018-02-14_20-53-46.png]]

* Conclusion
** Conclusion and thank you
[[file:img/screenshot_2018-02-14_20-55-38.png]]

[[file:img/screenshot_2018-02-14_20-57-36.png]]

* Practice questions
** Quiz: Sequence models & Attention mechanism

[[file:img/screenshot_2018-02-14_21-16-26.png]]

* Programming assignments
** Neural Machine Translation
[[file:img/screenshot_2018-02-14_21-26-34.png]]

[[file:img/screenshot_2018-02-14_21-27-10.png]]

[[file:img/screenshot_2018-02-14_21-29-44.png]]

[[file:img/screenshot_2018-02-14_21-30-24.png]]

[[file:img/screenshot_2018-02-14_21-31-05.png]]

[[file:img/screenshot_2018-02-14_21-31-39.png]]

[[file:img/screenshot_2018-02-14_21-37-20.png]]

- https://keras.io/layers/core/#repeatvector
- https://keras.io/layers/merge/#concatenate

#+BEGIN_SRC python
  # GRADED FUNCTION: one_step_attention

  def one_step_attention(a, s_prev):
      """
      Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights
      "alphas" and the hidden states "a" of the Bi-LSTM.

      Arguments:
      a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)
      s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)

      Returns:
      context -- context vector, input of the next (post-attetion) LSTM cell
      """

      ### START CODE HERE ###
      # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line)
      s_prev = None
      # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)
      concat = None
      # Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines)
      e = None
      # Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines)
      energies = None
      # Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line)
      alphas = None
      # Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)
      context = None
      ### END CODE HERE ###

      return context
#+END_SRC

[[file:img/screenshot_2018-02-14_21-45-46.png]]

- https://keras.io/layers/wrappers/#bidirectional
- https://keras.io/layers/recurrent/#lstm
- The first parameter of ~LSTM~ is the output size

To produce expected output, modify the cell above as follows:

[[file:img/screenshot_2018-02-14_22-19-03.png]]

#+BEGIN_SRC python
  # GRADED FUNCTION: model

  def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):
      """
      Arguments:
      Tx -- length of the input sequence
      Ty -- length of the output sequence
      n_a -- hidden state size of the Bi-LSTM
      n_s -- hidden state size of the post-attention LSTM
      human_vocab_size -- size of the python dictionary "human_vocab"
      machine_vocab_size -- size of the python dictionary "machine_vocab"

      Returns:
      model -- Keras model instance
      """

      # Define the inputs of your model with a shape (Tx,)
      # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)
      X = Input(shape=(Tx, human_vocab_size))
      s0 = Input(shape=(n_s,), name='s0')
      c0 = Input(shape=(n_s,), name='c0')
      s = s0
      c = c0

      # Initialize empty list of outputs
      outputs = []

      ### START CODE HERE ###

      # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)
      a = None

      # Step 2: Iterate for Ty steps
      for t in range(None):

          # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)
          context = None

          # Step 2.B: Apply the post-attention LSTM cell to the "context" vector.
          # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)
          s, _, c = None

          # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)
          out = None

          # Step 2.D: Append "out" to the "outputs" list (≈ 1 line)
          None

      # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)
      model = None

      ### END CODE HERE ###

      return model
#+END_SRC

[[file:img/screenshot_2018-02-14_22-32-59.png]]

- https://keras.io/optimizers/#adam
- https://keras.io/optimizers/#usage-of-optimizers
- https://keras.io/models/model/#compile

#+BEGIN_SRC python
  ### START CODE HERE ### (≈2 lines)
  opt = None
  None
  ### END CODE HERE ###
#+END_SRC

[[file:img/screenshot_2018-02-14_22-37-55.png]]

[[file:img/screenshot_2018-02-14_22-40-58.png]]

[[file:img/screenshot_2018-02-14_22-42-56.png]]

[[file:img/screenshot_2018-02-14_22-48-19.png]]

[[file:img/screenshot_2018-02-14_22-49-22.png]]

** Trigger word detection
