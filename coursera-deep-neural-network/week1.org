#+TITLE: Practical aspects of Deep Learning

* Table of Contents :TOC_3_gh:
- [[#setting-up-your-machine-learning-application][Setting up your Machine Learning Application]]
  - [[#train--dev--test-sets][Train / Dev / Test sets]]
  - [[#bias--variance][Bias / Variance]]
  - [[#basic-recipe-for-machine-learning][Basic Recipe for Machine Learning]]
- [[#regularizing-your-neural-network][Regularizing your neural network]]
  - [[#regularization][Regularization]]
  - [[#why-regularization-reduces-overfitting][Why regularization reduces overfitting?]]
  - [[#dropout-regularization][Dropout Regularization]]
  - [[#understanding-dropout][Understanding Dropout]]
  - [[#other-regularization-methods][Other regularization methods]]
- [[#setting-up-your-optimization-problem][Setting up your optimization problem]]
  - [[#normalizing-inputs][Normalizing inputs]]
  - [[#vanishing--exploding-gradients][Vanishing / Exploding gradients]]
  - [[#weight-initialization-for-deep-networks][Weight Initialization for Deep Networks]]
  - [[#numerical-approximation-of-gradients][Numerical approximation of gradients]]
  - [[#gradient-checking][Gradient checking]]
  - [[#gradient-checking-implementation-notes][Gradient Checking Implementation Notes]]
- [[#programming-assignments][Programming assignments]]
  - [[#initialization][Initialization]]
  - [[#regularization-1][Regularization]]
  - [[#gradient-checking-1][Gradient Checking]]

* Setting up your Machine Learning Application
** Train / Dev / Test sets

- https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo

[[file:img/screenshot_2017-10-14_09-22-34.png]]

Traditionally, 7:3 (train:dev) or 6:2:2 (train:dev:test), is a rule of thumb for data set.
But, in these days, big data came out, and there are a lot more data than past.
We can decrease dev and test data portion ratio, because the size of data set would just be enough in absolute perspective
with a big data set.

- Training Set :: This data set is used to adjust the weights on the neural network.

- Validation(Dev) Set ::
  This data set is used to minimize overfitting.
  If the accuracy over the training data set increases,
  but the accuracy over then validation data set stays the same or decreases,
  then you're overfitting your neural network and you should stop training.

- Testing Set :: This data set is used *only for testing the final solution in order to confirm the actual predictive power* of the network.

[[file:img/screenshot_2017-10-14_09-13-51.png]]

#+BEGIN_QUOTE
Make sure that the dev and test sets come from the same distribution. 
#+END_QUOTE
** Bias / Variance
[[file:img/screenshot_2017-10-14_09-33-46.png]]

- http://slideplayer.com/slide/8744653/
- [[file:img/screenshot_2017-10-14_09-42-42.png]]

[[file:img/screenshot_2017-10-14_09-44-20.png]]

** Basic Recipe for Machine Learning
[[file:img/screenshot_2017-10-14_09-52-29.png]]

* Regularizing your neural network
** Regularization
[[file:img/screenshot_2017-10-14_10-01-58.png]]

** Why regularization reduces overfitting?
[[file:img/screenshot_2017-10-14_10-08-39.png]]

[[file:img/screenshot_2017-10-14_10-15-16.png]]

- When we increase ~lambda~ to a huge number, ~W~ is going close to zero.
- ~W~ close to zeon means that the nework is going to be simpler.
- Which will alleviate the high variance problem.

** Dropout Regularization
[[file:img/screenshot_2017-10-14_10-20-53.png]]

[[file:img/screenshot_2017-10-14_10-28-17.png]]

[[file:img/screenshot_2017-10-14_10-30-31.png]]

** Understanding Dropout
[[file:img/screenshot_2017-10-14_10-38-29.png]]

** Other regularization methods
[[file:img/screenshot_2017-10-14_10-43-27.png]]

[[file:img/screenshot_2017-10-14_10-48-46.png]]

#+BEGIN_QUOTE
The main downside of early stopping is that this couples these two tasks.
#+END_QUOTE
* Setting up your optimization problem
** Normalizing inputs
[[file:img/screenshot_2017-10-15_07-19-10.png]]

#+BEGIN_QUOTE
Use the ~mu~ and ~sigma~ of training set when normalizing test set.
#+END_QUOTE

[[file:img/screenshot_2017-10-15_07-22-47.png]]
** Vanishing / Exploding gradients
[[file:img/screenshot_2017-10-15_07-27-40.png]]

** Weight Initialization for Deep Networks
[[file:img/screenshot_2017-10-15_07-33-01.png]]
** Numerical approximation of gradients
[[file:img/screenshot_2017-10-15_14-32-21.png]]

** Gradient checking
[[file:img/screenshot_2017-10-15_14-42-30.png]]

[[file:img/screenshot_2017-10-15_14-47-13.png]]

** Gradient Checking Implementation Notes
[[file:img/screenshot_2017-10-15_14-53-42.png]]
* Programming assignments
** Initialization
[[file:img/screenshot_2017-10-15_15-06-59.png]]

#+BEGIN_SRC python
  def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = "he"):
      """
      Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.

      Arguments:
      X -- input data, of shape (2, number of examples)
      Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
      learning_rate -- learning rate for gradient descent
      num_iterations -- number of iterations to run gradient descent
      print_cost -- if True, print the cost every 1000 iterations
      initialization -- flag to choose which initialization to use ("zeros","random" or "he")

      Returns:
      parameters -- parameters learnt by the model
      """

      grads = {}
      costs = [] # to keep track of the loss
      m = X.shape[1] # number of examples
      layers_dims = [X.shape[0], 10, 5, 1]

      # Initialize parameters dictionary.
      if initialization == "zeros":
          parameters = initialize_parameters_zeros(layers_dims)
      elif initialization == "random":
          parameters = initialize_parameters_random(layers_dims)
      elif initialization == "he":
          parameters = initialize_parameters_he(layers_dims)

      # Loop (gradient descent)

      for i in range(0, num_iterations):

          # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
          a3, cache = forward_propagation(X, parameters)

          # Loss
          cost = compute_loss(a3, Y)

          # Backward propagation.
          grads = backward_propagation(X, Y, cache)

          # Update parameters.
          parameters = update_parameters(parameters, grads, learning_rate)

          # Print the loss every 1000 iterations
          if print_cost and i % 1000 == 0:
              print("Cost after iteration {}: {}".format(i, cost))
              costs.append(cost)

      # plot the loss
      plt.plot(costs)
      plt.ylabel('cost')
      plt.xlabel('iterations (per hundreds)')
      plt.title("Learning rate =" + str(learning_rate))
      plt.show()

      return parameters
#+END_SRC

[[file:img/screenshot_2017-10-15_15-24-15.png]]

[[file:img/screenshot_2017-10-15_15-27-52.png]]

[[file:img/screenshot_2017-10-15_15-28-19.png]]

[[file:img/screenshot_2017-10-15_15-31-42.png]]
** Regularization
[[file:img/screenshot_2017-10-15_15-33-24.png]]

[[file:img/screenshot_2017-10-15_15-41-53.png]]

#+BEGIN_SRC python
  def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):
      """
      Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.

      Arguments:
      X -- input data, of shape (input size, number of examples)
      Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
      learning_rate -- learning rate of the optimization
      num_iterations -- number of iterations of the optimization loop
      print_cost -- If True, print the cost every 10000 iterations
      lambd -- regularization hyperparameter, scalar
      keep_prob - probability of keeping a neuron active during drop-out, scalar.

      Returns:
      parameters -- parameters learned by the model. They can then be used to predict.
      """

      grads = {}
      costs = []                            # to keep track of the cost
      m = X.shape[1]                        # number of examples
      layers_dims = [X.shape[0], 20, 3, 1]

      # Initialize parameters dictionary.
      parameters = initialize_parameters(layers_dims)

      # Loop (gradient descent)

      for i in range(0, num_iterations):

          # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
          if keep_prob == 1:
              a3, cache = forward_propagation(X, parameters)
          elif keep_prob < 1:
              a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)

          # Cost function
          if lambd == 0:
              cost = compute_cost(a3, Y)
          else:
              cost = compute_cost_with_regularization(a3, Y, parameters, lambd)

          # Backward propagation.
          assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout,
                                              # but this assignment will only explore one at a time
          if lambd == 0 and keep_prob == 1:
              grads = backward_propagation(X, Y, cache)
          elif lambd != 0:
              grads = backward_propagation_with_regularization(X, Y, cache, lambd)
          elif keep_prob < 1:
              grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)

          # Update parameters.
          parameters = update_parameters(parameters, grads, learning_rate)

          # Print the loss every 10000 iterations
          if print_cost and i % 10000 == 0:
              print("Cost after iteration {}: {}".format(i, cost))
          if print_cost and i % 1000 == 0:
              costs.append(cost)

      # plot the cost
      plt.plot(costs)
      plt.ylabel('cost')
      plt.xlabel('iterations (x1,000)')
      plt.title("Learning rate =" + str(learning_rate))
      plt.show()

      return parameters
#+END_SRC

[[file:img/screenshot_2017-10-15_15-42-46.png]]

[[file:img/screenshot_2017-10-15_15-43-08.png]]

[[file:img/screenshot_2017-10-15_15-46-48.png]]

[[file:img/screenshot_2017-10-15_15-50-41.png]]

[[file:img/screenshot_2017-10-15_15-51-45.png]]

[[file:img/screenshot_2017-10-15_15-56-56.png]]

[[file:img/screenshot_2017-10-15_16-00-24.png]]

** Gradient Checking
[[file:img/screenshot_2017-10-15_16-05-00.png]]

[[file:img/screenshot_2017-10-15_16-05-29.png]]

[[file:img/screenshot_2017-10-15_16-05-41.png]]

[[file:img/screenshot_2017-10-15_16-10-33.png]]

[[file:img/screenshot_2017-10-15_16-18-13.png]]

[[file:img/screenshot_2017-10-15_16-18-39.png]]

[[file:img/screenshot_2017-10-15_16-29-56.png]]
