#+TITLE: Neural Networks and Deep Learning: Week 4

* Table of Contents :TOC_3_gh:
- [[#deep-neural-network][Deep Neural Network]]
  - [[#deep-l-layer-neural-network][Deep L-layer neural network]]
  - [[#forward-propagation-in-a-deep-network][Forward Propagation in a Deep Network]]
  - [[#getting-your-matrix-dimensions-right][Getting your matrix dimensions right]]
  - [[#why-deep-representations][Why deep representations?]]
  - [[#building-blocks-of-deep-neural-networks][Building blocks of deep neural networks]]
  - [[#forward-and-backward-propagation][Forward and Backward Propagation]]
  - [[#parameters-vs-hyperparameters][Parameters vs Hyperparameters]]
  - [[#what-does-this-have-to-do-with-the-brain][What does this have to do with the brain?]]
- [[#programming-assignments][Programming Assignments]]
  - [[#building-your-deep-neural-network-step-by-step][Building your Deep Neural Network: Step by Step]]
    - [[#signatures-of-subroutines][Signatures of subroutines]]
  - [[#deep-neural-network---application][Deep Neural Network - Application]]

* Deep Neural Network
** Deep L-layer neural network
[[file:_img/screenshot_2017-09-28_07-50-15.png]]

[[file:_img/screenshot_2017-09-28_07-49-33.png]]

** Forward Propagation in a Deep Network
[[file:_img/screenshot_2017-09-29_07-11-09.png]]

** Getting your matrix dimensions right
[[file:_img/screenshot_2017-10-02_22-02-34.png]]

[[file:_img/screenshot_2017-10-02_22-06-47.png]]

** Why deep representations?
[[file:_img/screenshot_2017-10-02_22-13-12.png]]

[[file:_img/screenshot_2017-10-02_22-25-13.png]]

** Building blocks of deep neural networks
[[file:_img/screenshot_2017-10-04_07-16-05.png]]

[[file:_img/screenshot_2017-10-04_07-21-20.png]]

** Forward and Backward Propagation
[[file:_img/screenshot_2017-10-04_07-24-12.png]]

[[file:_img/screenshot_2017-10-04_07-37-12.png]]

[[file:_img/screenshot_2017-10-04_07-40-19.png]]

** Parameters vs Hyperparameters
[[file:_img/screenshot_2017-10-04_08-14-19.png]]

[[file:_img/screenshot_2017-10-04_08-18-29.png]]

#+BEGIN_QUOTE
So maybe one rule of thumb is (..)
just try a few values for the hyper parameters and double check if there's a better value for the hyper parameters
and as you do so you slowly gain intuition as well about the hyper parameters that work best for your problems (...)
#+END_QUOTE

** What does this have to do with the brain?
[[file:_img/screenshot_2017-10-04_08-24-10.png]]
* Programming Assignments
** Building your Deep Neural Network: Step by Step
[[file:_img/screenshot_2017-10-06_05-46-31.png]]

[[file:_img/screenshot_2017-10-06_05-47-48.png]]

[[file:_img/screenshot_2017-10-06_05-54-53.png]]

[[file:_img/screenshot_2017-10-06_06-15-46.png]]

[[file:_img/screenshot_2017-10-06_06-17-01.png]]

[[file:_img/screenshot_2017-10-06_06-17-26.png]]

[[file:_img/screenshot_2017-10-06_06-25-09.png]]

[[file:_img/screenshot_2017-10-06_06-29-59.png]]

*** Signatures of subroutines
#+BEGIN_SRC python
  def initialize_parameters(n_x, n_h, n_y):
      parameters = {"W1": W1,
                    "b1": b1,
                    "W2": W2,
                    "b2": b2}
      return parameters

  def initialize_parameters_deep(layer_dims):
    # parameters is like {"W1":, "b1":, "W2":, ...}
    return parameters

  def linear_forward(A, W, b):
      assert(Z.shape == (W.shape[0], A.shape[1]))
      cache = (A, W, b)
      return Z, cache

  def linear_activation_forward(A_prev, W, b, activation):
      return A, cache

  def L_model_forward(X, parameters):
      return AL, caches

  def compute_cost(AL, Y):
      # cross-entropy cost
      return cost

  def linear_backward(dZ, cache):
      return dA_prev, dW, db

  def linear_activation_backward(dA, cache, activation):
      return dA_prev, dW, db

  def L_model_backward(AL, Y, caches):
      return grads

  def update_parameters(parameters, grads, learning_rate):
      return parameters
#+END_SRC

** Deep Neural Network - Application
[[file:_img/screenshot_2017-10-07_07-48-52.png]]

[[file:_img/screenshot_2017-10-07_07-49-58.png]]
