#+TITLE: Recurrent Neural Networks

* Table of Contents :TOC_3_gh:
- [[#recurrent-neural-networks][Recurrent Neural Networks]]
  - [[#why-sequence-models][Why sequence models]]
  - [[#notation][Notation]]
  - [[#recurrent-neural-network-model][Recurrent Neural Network Model]]
  - [[#backpropagation-through-time][Backpropagation through time]]
  - [[#different-types-of-rnns][Different types of RNNs]]
  - [[#language-model-and-sequence-generation][Language model and sequence generation]]
  - [[#sampling-novel-sequences][Sampling novel sequences]]
  - [[#vanishing-gradients-with-rnns][Vanishing gradients with RNNs]]
  - [[#gated-recurrent-unit-gru][Gated Recurrent Unit (GRU)]]
  - [[#long-short-term-memory-lstm][Long Short Term Memory (LSTM)]]
  - [[#bidirectional-rnn][Bidirectional RNN]]
  - [[#deep-rnns][Deep RNNs]]
- [[#programming-assignments][Programming assignments]]
  - [[#building-a-recurrent-neural-network---step-by-step][Building a recurrent neural network - step by step]]
  - [[#dinosaur-island---character-level-language-modeling][Dinosaur Island - Character-Level Language Modeling]]
  - [[#jazz-improvisation-with-lstm][Jazz improvisation with LSTM]]

* Recurrent Neural Networks
** Why sequence models
[[file:img/screenshot_2018-02-06_13-59-38.png]]

** Notation
[[file:img/screenshot_2018-02-06_14-14-07.png]]

[[file:img/screenshot_2018-02-06_14-13-41.png]]

** Recurrent Neural Network Model
[[file:img/screenshot_2018-02-06_15-52-01.png]]

[[file:img/screenshot_2018-02-06_16-45-24.png]]

[[file:img/screenshot_2018-02-06_16-49-19.png]]

[[file:img/screenshot_2018-02-06_16-53-06.png]]

** Backpropagation through time
[[file:img/screenshot_2018-02-06_17-00-58.png]]

** Different types of RNNs
[[file:img/screenshot_2018-02-06_17-06-16.png]]

[[file:img/screenshot_2018-02-06_17-10-05.png]]

[[file:img/screenshot_2018-02-06_17-11-20.png]]
** Language model and sequence generation
[[file:img/screenshot_2018-02-06_17-15-34.png]]

[[file:img/screenshot_2018-02-06_17-18-43.png]]

[[file:img/screenshot_2018-02-06_17-24-33.png]]
** Sampling novel sequences
[[file:img/screenshot_2018-02-06_17-37-57.png]]

[[file:img/screenshot_2018-02-06_17-41-05.png]]

- Character-level language model needs longer sequences,
  which means that it's computationally expensive.
- So it is used in more specialized cases.

[[file:img/screenshot_2018-02-06_17-43-12.png]]
** Vanishing gradients with RNNs
[[file:img/screenshot_2018-02-06_17-50-15.png]]
** Gated Recurrent Unit (GRU)
[[file:img/screenshot_2018-02-06_17-53-28.png]]

[[file:img/screenshot_2018-02-06_18-06-17.png]]

[[file:img/screenshot_2018-02-06_18-08-57.png]]
** Long Short Term Memory (LSTM)
[[file:img/screenshot_2018-02-06_18-26-55.png]]

- GRU is simpler, LSTM is more powerful
- GRU or LSTM: There is no generally better algorithm between these.
- But historically, LSTM has been more commonly used.

** Bidirectional RNN
[[file:img/screenshot_2018-02-06_18-38-33.png]]
- BRNN has a disadvantage that it need the entire sequence to make predictions.

** Deep RNNs
[[file:img/screenshot_2018-02-06_18-45-48.png]]
* Programming assignments
** Building a recurrent neural network - step by step
[[file:img/screenshot_2018-02-07_06-40-18.png]]

[[file:img/screenshot_2018-02-07_06-42-11.png]]


[[file:img/screenshot_2018-02-07_06-42-56.png]]

#+BEGIN_SRC python
  def rnn_cell_forward(xt, a_prev, parameters):
      """
      Implements a single forward step of the RNN-cell as described in Figure (2)

      Arguments:
      xt -- your input data at timestep "t", numpy array of shape (n_x, m).
      a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)
      parameters -- python dictionary containing:
                          Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                          Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                          Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          ba --  Bias, numpy array of shape (n_a, 1)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
      Returns:
      a_next -- next hidden state, of shape (n_a, m)
      yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)
      cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)
      """
      return a_next, yt_pred, cache
#+END_SRC

[[file:img/screenshot_2018-02-07_06-53-24.png]]

#+BEGIN_SRC python
  def rnn_forward(x, a0, parameters):
      """
      Implement the forward propagation of the recurrent neural network described in Figure (3).

      Arguments:
      x -- Input data for every time-step, of shape (n_x, m, T_x).
      a0 -- Initial hidden state, of shape (n_a, m)
      parameters -- python dictionary containing:
                          Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
                          Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
                          Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          ba --  Bias numpy array of shape (n_a, 1)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

      Returns:
      a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)
      y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
      caches -- tuple of values needed for the backward pass, contains (list of caches, x)
      """
      return a, y_pred, caches
#+END_SRC

[[file:img/screenshot_2018-02-07_07-01-53.png]]

[[file:img/screenshot_2018-02-07_07-02-08.png]]

[[file:img/screenshot_2018-02-07_07-02-24.png]]

#+BEGIN_SRC python
  def lstm_cell_forward(xt, a_prev, c_prev, parameters):
      """
      Implement a single forward step of the LSTM-cell as described in Figure (4)

      Arguments:
      xt -- your input data at timestep "t", numpy array of shape (n_x, m).
      a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)
      c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)
      parameters -- python dictionary containing:
                          Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
                          bf -- Bias of the forget gate, numpy array of shape (n_a, 1)
                          Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)
                          bi -- Bias of the update gate, numpy array of shape (n_a, 1)
                          Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)
                          bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)
                          Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)
                          bo --  Bias of the output gate, numpy array of shape (n_a, 1)
                          Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

      Returns:
      a_next -- next hidden state, of shape (n_a, m)
      c_next -- next memory state, of shape (n_a, m)
      yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)
      cache -- tuple of values needed for the Backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)

      Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),
            c stands for the memory value
      """
      return a_next, c_next, yt_pred, cache
#+END_SRC

[[file:img/screenshot_2018-02-07_07-21-24.png]]

#+BEGIN_SRC python
  def lstm_forward(x, a0, parameters):
      """
      Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).

      Arguments:
      x -- Input data for every time-step, of shape (n_x, m, T_x).
      a0 -- Initial hidden state, of shape (n_a, m)
      parameters -- python dictionary containing:
                          Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
                          bf -- Bias of the forget gate, numpy array of shape (n_a, 1)
                          Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)
                          bi -- Bias of the update gate, numpy array of shape (n_a, 1)
                          Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)
                          bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)
                          Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)
                          bo -- Bias of the output gate, numpy array of shape (n_a, 1)
                          Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
                          by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)

      Returns:
      a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)
      y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
      caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)
      """
      return a, y, c, caches
#+END_SRC

** Dinosaur Island - Character-Level Language Modeling
** Jazz improvisation with LSTM
