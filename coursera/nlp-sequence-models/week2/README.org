#+TITLE: Natural Language Processing & Word Embeddings

* Table of Contents :TOC_3_gh:
- [[#introduction-to-word-embeddings][Introduction to Word Embeddings]]
  - [[#word-representation][Word Representation]]
  - [[#using-word-embeddings][Using word embeddings]]
  - [[#properties-of-word-embeddings][Properties of word embeddings]]
  - [[#embedding-matrix][Embedding matrix]]
- [[#learning-word-embeddings-word2vec--glove][Learning Word Embeddings: Word2vec & GloVe]]
  - [[#learning-word-embeddings][Learning word embeddings]]
  - [[#word2vec][Word2Vec]]
  - [[#negative-sampling][Negative Sampling]]

* Introduction to Word Embeddings
** Word Representation
[[file:img/screenshot_2018-02-11_22-32-17.png]]

[[file:img/screenshot_2018-02-11_22-37-17.png]]

[[file:img/screenshot_2018-02-11_22-40-11.png]]

** Using word embeddings
[[file:img/screenshot_2018-02-11_22-44-37.png]]

[[file:img/screenshot_2018-02-11_22-49-04.png]]

[[file:img/screenshot_2018-02-11_22-51-45.png]]
- The word embeddings of nlp funciton in a similar way as face encoding of face recognition.
- While word embeddings have a fixed number of inputs(vocabulary),
  face encoding differs in that it can have an infinite number of inputs(face images).

** Properties of word embeddings
[[file:img/screenshot_2018-02-11_23-11-50.png]]

[[file:img/screenshot_2018-02-11_23-16-24.png]]

[[file:img/screenshot_2018-02-11_23-19-10.png]]

** Embedding matrix
[[file:img/screenshot_2018-02-11_23-25-37.png]]


* Learning Word Embeddings: Word2vec & GloVe
** Learning word embeddings
[[file:img/screenshot_2018-02-12_10-33-46.png]]

[[file:img/screenshot_2018-02-12_10-37-12.png]]

** Word2Vec
[[file:img/screenshot_2018-02-12_10-40-40.png]]

[[file:img/screenshot_2018-02-12_10-45-31.png]]

[[file:img/screenshot_2018-02-12_10-50-30.png]]

- If you sample the context uniformly at radom, words like ~the~, ~of~, ~a~ , ~and~ will dominate the context.
- So there is some other heuristic method to sample the context.

** Negative Sampling
[[file:img/screenshot_2018-02-12_11-06-52.png]]

[[file:img/screenshot_2018-02-12_11-13-43.png]]

- Each target word(10k) has their binary classification model.
- Each model takes word embeddings as inputs, in previous examples, 300 features.
- Each model predicts probability of the word in charge.
