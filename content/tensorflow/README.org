#+TITLE: TensorFlow

* Table of Contents :TOC_3_gh:
- [[#overview][Overview]]
- [[#reference][Reference]]
  - [[#placeholder][placeholder]]
  - [[#get_variable][get_variable]]
  - [[#trainadamoptimizer][train.AdamOptimizer]]
  - [[#xavier_initializer][xavier_initializer]]
  - [[#sessrun][sess.run]]
  - [[#cast][cast]]
- [[#terminology][Terminology]]
- [[#topics][Topics]]
- [[#how-to][How-to]]
- [[#links][Links]]

* Overview
* Reference
** placeholder
- https://www.tensorflow.org/api_docs/python/tf/placeholder

#+BEGIN_SRC python
  x = tf.placeholder(tf.float32, shape=(1024, 1024))
  y = tf.matmul(x, x)

  with tf.Session() as sess:
    print(sess.run(y))  # ERROR: will fail because x was not fed.

    rand_array = np.random.rand(1024, 1024)
    print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed.
#+END_SRC

** get_variable
- https://www.tensorflow.org/api_docs/python/tf/get_variable

#+BEGIN_SRC python
  def foo():
    with tf.variable_scope("foo", reuse=tf.AUTO_REUSE):
      v = tf.get_variable("v", [1])
    return v

  v1 = foo()  # Creates v.
  v2 = foo()  # Gets the same, existing v.
  assert v1 == v2
#+END_SRC

** train.AdamOptimizer
- https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer
- https://stackoverflow.com/questions/33788989/tensorflow-using-adam-optimizer

#+BEGIN_SRC python
  train_op = tf.train.AdamOptimizer(learning_rate=kjjkk1e-4).minimize(cross_entropy)
#+END_SRC

** xavier_initializer
- https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer
- An initializer for a weight matrix.

#+BEGIN_SRC python
  tf.get_variable("W", [3, 3, 3, 8], initializer=tf.contrib.layers.xavier_initializer())
#+END_SRC

** sess.run
#+BEGIN_SRC python
  _, c = sess.run([optimizer cost], feed_dict={X: minibatch_X, Y: minibatch_Y})
#+END_SRC

** cast
#+BEGIN_SRC python
  tf.cast(loss, tf.float32)
#+END_SRC

* Terminology
* Topics
* How-to
* Links
