<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120656861-1"></script>
  <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());
   gtag('config', 'UA-120656861-1');
  </script>

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="theme-color" content="#ffffff">

  <title>Neural Networks and Deep Learning: Week 3</title>

  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>

  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <link rel="stylesheet" href="/_css/content.css" />


  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

  <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Condensed|Roboto+Mono" rel="stylesheet">
</head>
<body>
<nav class="menu">
  <ol class="breadcrumb">
    <li><a href="/" >Home</a></li>
    <li><a href="/coursera/" >coursera</a></li>
    <li><a href="/coursera/deeplearning-ai/" >deeplearning-ai</a></li>
    <li><a href="/coursera/deeplearning-ai/neural-networks-deep-learning/" >neural-networks-deep-learning</a></li>
    <li>week3</li>
  </ol>
</nav>

<article>
<header>
<h1 class="title">Neural Networks and Deep Learning: Week 3</h1>
</header>

<nav class="toc">
<h2>Table of Contents</h2>
<ul>
<li><a href="#shallow-neural-network">Shallow Neural Network</a><ul>
<li><a href="#neural-networks-overview">Neural Networks Overview</a></li>
<li><a href="#neural-network-representation">Neural Network Representation</a></li>
<li><a href="#computing-a-neural-networks-output">Computing a Neural Network's Output</a></li>
<li><a href="#vectorizing-across-multiple-examples">Vectorizing across multiple examples</a></li>
<li><a href="#explanation-for-vectorized-implementation">Explanation for Vectorized Implementation</a></li>
<li><a href="#activation-functions">Activation functions</a></li>
<li><a href="#why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</a></li>
<li><a href="#derivatives-of-activation-functions">Derivatives of activation functions</a></li>
<li><a href="#gradient-descent-for-neural-networks">Gradient descent for Neural Networks</a></li>
<li><a href="#backpropagation-intuition-optional">Backpropagation intuition (optional)</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
</ul></li>
<li><a href="#programming-assignment">Programming Assignment</a><ul>
<li><a href="#planar-data-classification">Planar data classification</a></li>
</ul></li>
</ul>
</nav>

<h1 id="shallow-neural-network">Shallow Neural Network</h1>
<h2 id="neural-networks-overview">Neural Networks Overview</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-23_09-52-42.png" /></p>
<h2 id="neural-network-representation">Neural Network Representation</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-23_09-58-03.png" /></p>
<h2 id="computing-a-neural-networks-output">Computing a Neural Network's Output</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-51-31.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-52-25.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-53-08.png" /></p>
<h2 id="vectorizing-across-multiple-examples">Vectorizing across multiple examples</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-53-58.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-54-36.png" /></p>
<h2 id="explanation-for-vectorized-implementation">Explanation for Vectorized Implementation</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-55-39.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-56-39.png" /></p>
<h2 id="activation-functions">Activation functions</h2>
<ul>
<li>For hidden units, <code>tanh</code> is almost alway superior to <code>sigmoid</code></li>
<li>Because <code>[-1, 1]</code> and <code>0</code> mean, rather than <code>[0, 1]</code> and <code>0.5</code> mean, actually make the learning for the next layer easier.</li>
<li><code>sigmoid</code> is preferred mostly for the output layer which expects values of <code>[0, 1]</code></li>
<li>There days, <code>ReLU</code> is the default and generally most preferred.</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-58-26.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_15-58-53.png" /></p>
<h2 id="why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</h2>
<ul>
<li>If all activation functions are linear, <strong>the calculation of hidden layers can be boiled down to a single linear layer.</strong></li>
<li>When <strong>the output value can have all the real number</strong>, then the activation function for the output layer can be a linear one.</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_16-09-21.png" /></p>
<h2 id="derivatives-of-activation-functions">Derivatives of activation functions</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-05-32.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-06-05.png" /></p>
<ul>
<li>Theoretically the derivative of <code>z=0</code> is undefined, but it doesn't matter technically.</li>
</ul>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-06-30.png" /></p>
<h2 id="gradient-descent-for-neural-networks">Gradient descent for Neural Networks</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-08-44.png" /></p>
<h2 id="backpropagation-intuition-optional">Backpropagation intuition (optional)</h2>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-09-23.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-09-52.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-10-13.png" /></p>
<h2 id="random-initialization">Random Initialization</h2>
<p>if initial <code>W</code> values are all zeros, all hidden units become completly identical, zero, which make the hidden layer meaningless.</p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-10-55.png" /></p>
<p>By multipling <code>0.01</code>, it can be avoided to have very large values of <code>a</code> which have very small derivatives slowing down the learning.</p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-24_18-11-26.png" /></p>
<h1 id="programming-assignment">Programming Assignment</h1>
<h2 id="planar-data-classification">Planar data classification</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">def</span> initialize_parameters(n_x, n_h, n_y):</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">    parameters <span class="op">=</span> {<span class="st">&quot;W1&quot;</span>: W1,</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">                  <span class="st">&quot;b1&quot;</span>: b1,</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">                  <span class="st">&quot;W2&quot;</span>: W2,</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">                  <span class="st">&quot;b2&quot;</span>: b2}</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">    <span class="cf">return</span> parameters</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="kw">def</span> forward_propagation(X, parameters):</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    cache <span class="op">=</span> {<span class="st">&quot;Z1&quot;</span>: Z1,</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">             <span class="st">&quot;A1&quot;</span>: A1,</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">             <span class="st">&quot;Z2&quot;</span>: Z2,</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">             <span class="st">&quot;A2&quot;</span>: A2}</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">    <span class="cf">return</span> A2, cache</a>
<a class="sourceLine" id="cb1-14" data-line-number="14"></a>
<a class="sourceLine" id="cb1-15" data-line-number="15"><span class="kw">def</span> compute_cost(A2, Y, parameters):</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">    <span class="cf">return</span> cost</a>
<a class="sourceLine" id="cb1-17" data-line-number="17"></a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="kw">def</span> backward_propagation(parameters, cache, X, Y):</a>
<a class="sourceLine" id="cb1-19" data-line-number="19">    grads <span class="op">=</span> {<span class="st">&quot;dW1&quot;</span>: dW1,</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">             <span class="st">&quot;db1&quot;</span>: db1,</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">             <span class="st">&quot;dW2&quot;</span>: dW2,</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">             <span class="st">&quot;db2&quot;</span>: db2}</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">    <span class="cf">return</span> grads</a>
<a class="sourceLine" id="cb1-24" data-line-number="24"></a>
<a class="sourceLine" id="cb1-25" data-line-number="25"><span class="kw">def</span> update_parameters(parameters, grads, learning_rate <span class="op">=</span> <span class="fl">1.2</span>):</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">    parameters <span class="op">=</span> {<span class="st">&quot;W1&quot;</span>: W1,</a>
<a class="sourceLine" id="cb1-27" data-line-number="27">                  <span class="st">&quot;b1&quot;</span>: b1,</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">                  <span class="st">&quot;W2&quot;</span>: W2,</a>
<a class="sourceLine" id="cb1-29" data-line-number="29">                  <span class="st">&quot;b2&quot;</span>: b2}</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">    <span class="cf">return</span> parameters</a>
<a class="sourceLine" id="cb1-31" data-line-number="31"></a>
<a class="sourceLine" id="cb1-32" data-line-number="32"><span class="kw">def</span> predict(parameters, X):</a>
<a class="sourceLine" id="cb1-33" data-line-number="33">    <span class="cf">return</span> predictions</a></code></pre></div>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_06-28-11.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_06-30-57.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_06-31-43.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_06-32-28.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_06-47-36.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_07-09-33.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_07-23-53.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_07-25-41.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_07-26-05.png" /></p>
<p><img src="https://media.yeonghoey.com/coursera/deeplearning-ai/neural-networks-deep-learning/week3/_img/screenshot_2017-09-28_07-26-36.png" /></p>
</article>

</body>
</html>
